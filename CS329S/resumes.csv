job_title,resume
,
Property Preservation Data Scientist,"
DEVI BALASUNDARAM
410-***-****
adpvy4@r.postjobfree.com
Profile Summary
Senior Business Intelligence Engineer with around 12+ years of experience in IT industry, proficient in providing Data Modeling, Data and Information Architecture solutions. A highly qualified Business Intelligence Engineer/Data Engineer combining a passionate commitment to delivering service excellence in Information Technology. Possess extensive knowledge of implementing business intelligence applications, utilizing the most contemporary data analysis tools within multiple industries, including financial, insurance, and healthcare. Leverages outstanding communication, interpersonal, and negotiation skills to assist in the achievement of business goals and objectives. Meticulous and detail-oriented with proven ability to accomplish tasks under minimal supervision and quickly grasp new technologies and best practices.
PROFESSIONAL HIGHLIGHTS

Able to deliver data management vision, goals, priorities, design principles, and operating policies in support of the business goals of the organization.
Proficiency in creating various types of Parameterized Reports such as Crosstab, Drill-down, Drill-Through, Summary, Graph, Chart, and formatting those using Tableau and Power BI.
Develop batch processing solutions by using Data Factory, Data Lake, Spark, Azure Synapse Pipelines, PolyBase, and Azure Databricks
Develop a stream processing solution by using Stream Analytics, Azure Databricks, and Azure Event Hubs
Implement logging used by Azure Monitor
Design metastores in Azure Synapse Analytics and Azure Databricks
Implement different table geometries with Azure Synapse Analytics pools
Design a folder structure that represents the levels of data transformation
Created complex stored procedures used for generating reports in SQL Server Reporting Services (SSRS)
Developed interactive dashboards using Tableau and SQL tools to provide on-demand reporting, powerful visualizations, and insights to business users
Experienced in designing and developing Tableau Reports, Documents, and Dashboards as per specified requirements and timelines and presenting them in a variety of formats to users
Extensively used Joins and sub-Queries to simplify complex queries involving multiple tables and optimized the procedures and triggers to be used in production
Defined constraints, indexes, views, and triggers to enforce data and referential integrity based on architect requirements. Used Execution Plan, SQL Profiler, and database engine tuning advisor to optimize queries and enhance the performance of databases
Strong experience in gathering and analysis of requirements
Work with Business partners to recommend data management best practices
Good knowledge in Performance tuning, SQL Tuning, Data optimization, query optimization, job scheduling
Created/Updated database objects like tables, views, stored procedures, function.

Education
Bachelors Degree in Electronics and Communication Engineering Amrita Institute of Technology, India.

Certification Awards / Recognitions
Microsoft Certified Azure Data Engineer Associate
Microsoft Certified Azure Data Scientist Associate
Microsoft Certified Azure Fundamentals
Tableau Desktop Specialist.
SAS Base Programmer for SAS 9.

Technical Skills
Azure Tools: DataBricks, Data Factory, Apache Spark, Azure Synapse, Data Lake Storage, Azure Stream Analytics,Azure Machine Learning
Reporting Tools: Tableau,Power BI,Cognos
Cloud : Azure
Database & Tools: Oracle 11g,PL/SQL, MS SQL Server 2008,SAS,Informix, DB2,SQL Server, Oracle 11g.
Languages: Python,SQL,C,C++,Perl, Informix 4GL,ESQL C, Keil C,TAT, Unix Shell scripts
Other Tool : VB 6.0, Powerbuilder 5.0, Lombardi Teamworks 6.0,Power System, JTag, CVS, Clearcase, WorkShare, Sharepoint 2010,TOAD.
Operating System: Unix, BREW, Linux,Windows.
Management Tools : Azure DevOps.
Version control Tools: Clearcase

TRAININGS
Azure Data Engineering
Azure Data Scientist
Databricks and Apache Spark
Python
NoSQL Database
Pursuing Azure Solution Architect

Work Experience
Augur IT Inc., Coppell TX May 2021 – Till Date
Sr. Business Intelligence Engineer /Data Engineer
Responsibilities
Instrumental in gathering, analyzing, and designing business requirements and needs
Creating new database objects like Procedures, Functions, Triggers, Indexes and Views using T-SQL in Development and Production
Creating various ad-hoc SQL queries for customer reports
Designing and developing ETL workflows and datasets in Azure to be used in-reporting.
Proficiency in creating pipeline jobs, scheduling triggers, Mapping data flows using Azure Data Factory and using Key Vaults to store credentials
Worked on migration of data from on-prem SQL server to Cloud databases using Azure Synapse Analytics and Azure SQL DB.
Worked with Azure Blob and Data Lake storage and loading data into Azure SQL Synapse Analytics.
Proficiency in creating various types of Parameterized Reports such as Crosstab, Drill-down, Drill-Through, Summary, Graph, Chart, and formatting those using SSRS
Designed SSIS ETL packages to load data into the target database after performing data validation and error handling using execute SQL task, derived column, data conversion and conditional split transformations.
Develop an automated pipeline for the Data Scientist to be able to train, test and deploy their models.
Knowledge in creating Power BI Dashboards using maps, line charts, cards, ribbon charts, scatter plots and different reporting layouts from the Power BI Marketplace
Requirement gathering from Clients. Translated clients’ needs into useable mapping and coding instructions and created functional requirements documents and mockups for reporting.
Manage, Create, and assign work items to team members for Sprint using Azure DevOps.
Tableau dashboard creation and implementation using filters, parameters, and calculated datasets.
Validation and testing of Tableau dashboards created by team members, ensuring the dashboards are in line with the client requirement.
Dashboard migration into Tableau Server, user level access maintenance and Job schedule for refresh extracts.

Mr Cooper., Irving TX July 2013 – May 2018
Senior Reporting Analyst
Responsibilities
Instrumental in gathering, analyzing, and designing business requirements and needs
Highly involved in the platform migration of the servicing system, thus analyzing the system change and the corresponding data migration.
Worked in an Agile Environment and involved in daily Scrum calls.
Involved in cash flow model, Securitization, Reporting layer implementation, develop, implement, and automate Reports requested by business user and modify existing reports to meet user’s changing needs.
Involved in REO and Property Preservation business process, created a cash inflow outflow model, and also involved in State wise performance thus reducing losses and improving business.
Interact with the business users on a day-to- day basis and assist the client during the user acceptance testing phase.
Implementation of existing reports in COGNOS environment
Involved in the conversion of existing reports in SAS programming to SQL Server 2008.
Created Dashboard across the various departments of Reverse Mortgage.
Implementation of reports in Power BI
Providing support to the customers in accessing the reports, and in meeting the business requirement on using the tools Microsoft Excel and Microsoft Access 2010.
Working on various stages from project development to project release and simultaneously testing the project on Development Server, Staging Server, and Production Server.
Risk management – Identify risks and communicate proactively.
Trained and acted as a mentor for new developers.

7-Eleven, Dallas, Texas Dec 2012 – July 2013
Sr. Associate
Responsibilities
Handled the staff augmented project individually and have been working closely with Stakeholders, WorkShare and Sharepoint teams, Testing team and 7-11 SEIs.
Organized and have led Project management meetings with Stakeholder, 7-11 Managers, QA Managers and Infrastructure team to ensure the project milestones are met.
Have led regular meetings with WorkShare and Sharepoint team regarding the ticket updates, migration of documents from Tsunami to Sharepoint. Legal AD Group creation, Permission and Access requests.
Actively involved in WorkShare testing, technical and understanding document, defects registration and creation of training plan for the legal document. Involved in the upgradation of the systems and coordinating with the Legal team regarding the performance of the Legal SharePoint Stabilization.
Understand the Client’s existing security policy & governance and provide the users with appropriate access to the SharePoint Sites.
Coordinating with offshore team daily and delegating work that can be carried out at their business hours and explaining the user requirement clearly and precisely to the team and get the deliverables timely and with Quality.
Prepare the documentation related to Software Requirements Specifications, Design & Technical Specifications, and Test Plans and get it reviewed from Project Manager and Client and help getting Sign-Off.
Involved in executing Proof of Concepts, Research & Analysis for any new development initiatives in the engagement.

Walmart Inc., Bentonville Arkansas Sept 2012 – Dec 2012
Programmer Analyst
Responsibilities
Involved in the analysis of requirement of the project, preparations of the technical design documents, coding, and implementation of program units in Linux.
Involved in the review of codes developed to ensure that the QA standards are met, and review of the unit test cases for the particular delivery.
Involved in the creation of Request for Proposal, various deliverable documents, and project plan tasks.
Involved in identifying the areas that are prone to risk and performed the programming process in a manner to restrict code and functionality break.
Did regressive testing and all the code versions were maintained using the Team forge tool.

Walmart Inc., Chennai, India Nov 2009 – Sept 2012
SQL Developer
Responsibilities
Worked as Technical Offshore Team Lead and was responsible for the support and maintenance of 17 financial standalone applications of Wal-Mart.
Actively involved in the resolution of production issues related to Payment, Application Down/Slowness issues, Job abend, Data transaction.
Involved in the automation of several reports which saved 40 hours of manual efforts per month.
Business documentation of issues were created to educate the customers on critical issues that can be handled at their end was done to reduce manual effort.
Involved in the automation of data rollout when the data transfer to the financial system has failed.
Involved in the training of offshore and onsite teams in the areas of business, programming languages and communication.

Motorola, Chennai, India March 2009 – Nov 2009
Programmer Analyst
Responsibilities
Involved in Requirement analysis, High level design development, Unit Testing, Environment Setup, Build and Release.
Involved in the messaging application screen development, image processing, and transmission of messages in packets. Was responsible for the technical design documentation for all the modules involved.
Involved in the Coordination with onsite and handled the offshore in Messaging/UI team for the five different mobile models.
Involved in screen generation using BREW, Feature Implementation, Debugging, Code Optimization, Unit Testing and Bug fixing.
Involved in Change Control Meeting for any changes. Experience in Incident Management, Change Management, and Problem management using Service Now Tool/Remedy.
Experience in using Microsoft Project Plan 2007 in designing the project plan, budget the effort in person hours and identify and allocate resources.

Toshiba, Chennai, India August 2006 – Feb 2009
Programmer Analyst
Responsibilities
Involved in the technical translation of the requirements in Japanese to English.
Involved in Requirement analysis, High level design development, Unit Testing, Environment Setup, Build and Release.
Involved in rectification of errors generated due to the translation tools by coordinating with the business and technical teams.
Responsible for maintenance of code using CVS repository.
Involved in the development of macros using VB for generation of reports. Involved in the regression testing on the products under development.
Involved in the creation of Blueprint on the architecture of the DGT/HIS module

Immigration Status
US Permanent Resident (Green Card)

Contact Details
Name : Devi Venkatalakshmi Podanur Balasundaram
Cell: 410-***-****
E-Mail * : adpvy4@r.postjobfree.com
Linkedin : HTTPS://WWW.LINKEDIN.COM/IN/DEVI-BALASUNDARAM-78A8A3114/

Gold award – 2018
Silver award– 2017

"
,
Data Sales,"
Professional Summary

Data scientist with * years’ experience processing and analyzing data across a variety of industries. Leverages various mathematical, statistical, and machine learning tools to collaboratively synthesize business insights and drive innovative solutions for productivity, efficiency, and revenue.

●Experienced in the application of Bayesian Techniques, Advanced Analytics, Neural Networks and Deep Neural Networks, Support Vector Machines (SVMs), and Decision Trees with Random Forest ensemble.
●Proven creative thinker with a strong ability to devise and propose novel ways to look at and approach problems using a combination of business acumen and mathematical methods.
●Experience in using statistical models on big data sets using cloud-based cluster computing assets with AWS, Azure, and other Unix-based architectures.
●Identification of patterns in data and using experimental and iterative approaches to validate findings.
●Working alongside Data Engineers to productionize algorithms and solutions.
●Advanced predictive modeling techniques to build, maintain, and improve on real-time decision systems.
●In-depth knowledge of statistical procedures that are applied in both Supervised and Unsupervised Machine Learning problems.
●Machine learning techniques to promote marketing and merchandising ideas.
●Contributed to advanced analytical teams to design, build, validate, and re-train models.
●Excellent communication skills (verbal and written) to communicate with clients, stakeholders, and team members.
●Ability to quickly gain an understanding of niche subject matter domains, and design and implement effective novel solutions to be used by other subject matter experts.
●Experience implementing industry standard analytics within specific domains and applying data science techniques to expand these methods using Natural Language Processing, implementing clustering algorithms, and deriving insight.

Technical Skills

●Analytic Development/Platforms: Python, R-Programing, SQL, Excel.
●Python Packages: Numpy, Pandas, scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn, Azure Notebook.
●IDE: Jupyter, Spyder, RStudio, Google Colab, MySQL.
●Version Control: GitHub.
●Machine Learning: Natural Language Processing and Understanding, Machine Learning algorithms including text recognition, image classification, and forecasting; XGBoost.
●Data Query: Azure, Google, SQL, data warehouse, data lake and various SQL databases and data warehouses.
●Deep Learning: Machine Perception, Data Mining, Machine Learning algorithms, Neural Networks, TensorFlow, Keras, PyTorch, Long Short-Term memory (LSTM).
●Artificial Intelligence: Text Understanding, Classification, Pattern. Recognition, Recommendation Systems, Targeting Systems, Ranking Systems, and Time Series.
●Analysis Methods: Advanced Data Modeling, Statistical, Exploratory, Bayesian Analysis, Inference, Regression Analysis, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Design and Analysis of Experiments, Factorial Design and Response Surface Methodologies, Optimization, and State-Space Analysis, KNN Regression.
●Analysis Techniques and Tools: Classification and Regression Trees (CART), Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, Recurrent Neural Network (RNN) including LSTM, Linear and Logistic Regression, Naïve Bayes, Simplex, Markov Models, and Jackson Networks, Docker, Kalman filtering, Gaussian Mixture Models (GMMs).
●Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series Analysis.
●Applied Data Science: Natural Language Processing, Machine Learning, Text Recognition, Image Classification, Social Analytics, Predictive Maintenance.
●Cluster Management: Kubernetes, Databricks.
●Open-Source Data Platforms and Algorithms: XGBoost, MLFlow, CatBoost, LightGBM.
Professional Experience
Data Science Consultant
Anthem Inc.
Jul 2021 to Present
Atlanta, GA

I am serving as a Data Science Consultant interacting with 3 Data Science dev teams working on data extraction (mining), UI development/optimization, and pipeline builds.
My consulting work involves me interacting with 4 Data Scientists, 3 Software Engineers, 1 Project Manager, and 1 Head of Data Science.

●Extracted text from documents using OCR.
●Applied cosine similarity and Bert to find relevant sections of text in documents.
●Applied OCR to extract handwritten signatures and dates.
●Generated Regex patterns to collect text from relevant sections.
●Utilized OpenCV to find page numbers and text coordinates.
●Stored data on local Hadoop cluster.
●Led weekly presentations to business stakeholders to refine output.
●Used Jira for sprint planning and cards.
●Used Bitbucket and Git for code management.
●Built deep learning neural network models from scratch using GPU-accelerated libraries like PyTorch.
●Employed PyTorch, Scikit-Learn and XGBoost libraries to build and evaluate the performance of different models.
●Utilized Amazon Textract machine learning (ML) service to automatically extract text, handwriting, and data from scanned documents.
●Configured Pandas for data manipulation.
●Troubleshot machine-learning models in Python (TensorFlow) code to keep pipeline moving using PyTest packages.
●Applied Fuzzysearch algorithms to help locate records relevant searches.
●Used MS Teams for communication.

Lead Data Scientist
Neal Analytics
Mar 2021 to Jul 2021
Seattle, WA

The project at Neal was for Niagara Water, to help them predict their shipping costs when they needed to make ad-hoc shipments to supplement their existing shipping contracts. Challenge was to improve Model accuracy. The solution involved clustering shipping lanes, and then using the clusters to fit a Regression tree. A filter based on a prediction-correction model for linear and time-variant or time-invariant systems was applied.

The team consisted of myself in the Lead Data Scientist role, 2 Data Scientists who reported to me, 1 Project Manager, 1 Data Engineer, and 1 ML Operations Specialist. The team applied an Agile methodology with daily standups and bi-weekly presentations. My focus point of responsibility was on primary modeler development and deployment. Project represents approximately $2M in estimated annual cost savings. Work highlights:
●Clustering shipping lanes using a Gaussian Mixture model.
●Fitting clusters to an XGBoost Regression tree to make the final prediction and target negotiation goals based on the statistical likelihood of achieving that goal based on the bid prediction.
●Applying a Kalman filter at bid time to update any mispredictions to give a target negotiation goal to Niagara’s bid negotiators.
●Building the model on an Azure Notebook and Azure Databricks data analytics platforms.
●Programming model functions using Python.
●Working within Databricks developed largely in Python, Spark, PySpark, Mlib, Pandas, and NumPy.
●Fitting several preliminary Bayesian and machine learning models in Scala and Python (with PySpark for data retrieval in Python) for the purpose of improved understanding of data, and for feature selection.
●Applying and running search and decision algorithms such as XGBoost, MLFlow, CatBoost, LightGBM.
●Conducting regression tests using KNN regression.
●Applying Long Short-Term memory (LSTM)/Recurrent Neural Network (RNN) architectures to deep learning.
●Creating, deploying, and running container applications using Docker.
●Using Kubernetes for clustering.

ML and Data Scientist
Buoy Health
Jan 2020 to March 2021
Boston, MA

Buoy Health claims that its symptom checker chatbot leverages AI to deliver personalized and more accurate diagnoses. The company’s algorithm was trained on clinical data from 1000s of medical papers in an effort to mirror the literature referenced by physicians. Examples of data include 5 million patients and approximately 1,700 conditions. Beginning with the symptoms provided by the user via natural language processing, the chatbot will match the symptoms to all possible conditions and then ask clarifying questions to narrow them down to the best selection.
Technical work highlights:
●Worked as a Data S to generate Data Models using Erwin and developed relational database system.
●Analyzed the business requirements of the project by studying the Business Requirement Specification document.
●Converted business problem into a well-defined Machine Learning problem. Created Key Performance Indicators (KPI) for project success. Delivered actionable insights to stakeholders.
●Used R and Python for Exploratory Data Analysis, Anova test and Hypothesis testing.
●Applied linear regression in Python and SAS to understand the relationship between different attributes of dataset and causal relationship between them.
●Utilized matplotlib in Python to generate data visualizations to convey results, diagnostics, and useful insights to team members and team lead.
●Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, MLLib, R, a broad variety of machine learning methods including classifications, regressions, dimensionality reduction etc.
●Designed mapping to process the incremental changes that exists in the source table. Whenever source data elements were missing in source tables, these were modified/added in consistency with third normal form based OLTP source database.
●Provided expertise and recommendations for physical database design, architecture, testing, performance tuning and implementation.
●Designed logical and physical data models for multiple OLTP and Analytic applications.
●Designed the physical model for implementing the model into oracle9i physical data base.
●Involved with Data Analysis primarily Identifying Data Sets, Source Data, Source Meta Data, Data Definitions and Data Formats
●Tuned database to optimize performance of indexes and SQL statements.
●Wrote simple and advanced SQL queries and scripts to create standard and adhoc reports for senior managers.
●Used Expert level understanding of different databases in combinations for Data extraction and loading, joining data extracted from different databases and loading to a specific database.
●Worked very close with Data Architects and DBA team to implement data model changes in database in all environments.

Data Scientist
Baltimore Orioles –
Nov 2017 – Sept 2019
Baltimore Maryland (Remote)

Working with the Baltimore Orioles. I worked on pitcher relief prediction. Game statistics for the 2017-2018 season were analyzed and pitchers were clustered into one of three categories. Pitcher clusters were then passed to a random forest to determine the optimal time for substitution. The model was then used to create a paper-based framework for General Manager decisions in the bullpen. Work highlights:
●Visual data exploration performed in Python using the Matplotlib and Seaborne libraries.
●Applied PCA utilizing SVD for variable selection to reduce model complexity
●K-Means, GMM, DBSCAN used to classify pitchers into distinct strategic classes for analysis
●Normalized Data to improve accuracy and performance, used statistical and analytic tests such as Grubb’s to find and remove outliers.
●Explored Data statistically and visually to support model construction.
●Built specific domain knowledge by finding nontechnical experts and integrating their knowledge into models.
●Performed feature engineering and selection to generate high performing, understandable models.
●Implemented a Final model using decision trees in a random forest ensemble to predict pitcher substitution.

Data Scientist
YRC Freight
Jan 2015- Sept 2017
Atlanta, GA

For YRC I worked to optimize trailer loading and unloading. Utilizing machine vision, a convolution neural network was used to recognize irregular loads, and then a linear programming solution based on convex hulls was applied to fill trailers. The model reduced labor costs, and time investments to loading trailers. Work highlights:

●Applied linear programming and optimization techniques to tessellate the space on each trailer.
●Used Tensorflow in Python for machine vision to recognize non-standard labeling.
●Implemented a Convolution Neural Network for machine vision to read non uniform labeling.
●Coded solution using Python while utilizing Numpy,CVXPY, PuLP for linear programming.
●Deployed Solution onto an EC2 instance using AWS providing access to forklift operators
●Integrated models create a solution in compliance with federal and company regulations.
●Explored data visually in python using the Matplotlib and Seaborn packages

Data Scientist
Target Corporation
March 2012 - 2014
Minneapolis, Minnesota

Worked to forecast future sales. Sales data for the past three years was analyzed and fit to models. AN ARIMA model was fit to the data in order to forecast weekly sales into the next quarter. Models revealed shopping trends that were being under-capitalized by Target.
●Engineered a solution in the R programming language
●Experimented with time series models such ARIMA GARCH to produce reliable forecasting.
●Accessed and integrated large datasets from remotes servers using SQL
●Applied Statistical testing to the model to determine appropriate autocorrelation and partial auto-correlation lags.
●Forecasted sales for the next quarter.
●Cleaned and normalized data set to optimize performance and reliability of predictions.
●Collaborated with advertising to form a plan to capture the market during newly revealed consumer trends.
●Communicated results through interactive visuals using the Javascript library D3.

Education
Master of Science in Analytics
Georgia Institute of Technology
Atlanta, Georgia

Bachelor of Science in Applied Mathematics
Georgia Institute of Technology
Atlanta, Georgia

Bachelor of Science in Physics
Georgia Institute of Technology
Atlanta, Georgia

"
,
ML Engineer & Data Scientist,"
Professional Summary

Detail-oriented Data Scientist with 19+ years’ combined experience in Data Science and Data Analysis. Skilled at operating in a wide range of industries and technology platforms. Excellent written and oral communication skills to explain complex Data Science concepts in easily digestible terms for stakeholders and clients.
●Experience in the application of Naïve Bayes, Regression Analysis, Neural Networks/Deep Neural Networks, Support Vector Machines (SVM), and Random Forest machine learning techniques.
● Creative thinking/strong ability to devise and propose innovative ways to look at problems by using business acumen, mathematical theories, data models, and statistical analysis.
●Advanced statistical and predictive modeling techniques to build, maintain, and improve on real-time decision systems.
●Work on productizing algorithms and solutions.
●In-depth knowledge of statistical procedures that are applied in both Supervised and Unsupervised Machine Learning problems.
●Advanced analytical teams to design, build, validate and refresh data models.
●Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
●Ability to quickly gain an understanding of niche subject matter domains, and design and implement effective novel solutions to be used by other subject matter experts.
●Machine learning techniques to marketing and merchandising ideas.
● Experience implementing industry standard analytics methods within specific domains, such as Online Analytical Processing (OLAP) Cubes within the field of indirect procurement, and applying data science techniques to expand these methods, for example, using Natural Language Processing methods to aid in normalizing vendor names, implementing clustering algorithms for supplier justification analysis, and deriving novel metrics for prioritization in addressing commodity fracturing.

Technical Skills
Analytic Development: Python, SQL
Machine Learning: Natural Language Processing & Understanding, Imaging Recognition and Detection, Forecasting
Artificial Intelligence: Text comprehension, Classification, Pattern Recognition, Recommendation Systems, Targeting Systems, Ranking Systems
Analysis Methods: Advanced Data Modeling, Forecasting, Predictive, Statistical, Sentiment, Exploratory, Stochastic, Bayesian Analysis, Inference, Models, Regression Analysis, Linear models, Multivariate Analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Design and Analysis of Experiments
Analysis Techniques: Classification and Regression Trees (CART), Support Vector Machine, Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, RNN, Regression, Naïve Bayes
Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series Analysis
Applied Data Science: Natural Language Processing, Machine Learning, Social Analytics, Predictive Maintenance
Python Packages: Numpy, Pandas, Scikit-learn, Tensorflow, SciPy, Matplotlib, Seaborn, Keras, fastai
Version Control: GitHub, Git
IDE: Jupyter Notebook, Spyder, Visual Studio
Data Query: SQL
Deep Learning: Machine Perception, Neural Networks, TensorFlow, Keras, Pytorch
Soft Skills: Excellent communication and presentation skills; ability to work well with stakeholders to discern needs accurately; leadership, mentoring, and coaching

Data Scientist May 2021 – Present
Pekin Insurance
Peoria, IL

Pekin Insurance offers multiple lines of insurance in 24 states through its growing agency force, which now totals over 1,100 independent agencies and over 11,000 insurance producers. As Senior Data Scientist at Pekin, I lead a small team to develop predictive models on a variety of problems. These include Fraudulent Claim Detection. Claims were submitted in the form of claim notes. Our responsibilities were to find patterns and develop an NLP model to detect fraud in the claim notes. I also worked on Market Segmentation using Mosaic Data to find Pekin Market-Share across all verticals. Finally an interesting project was to determine why Arizona has a greater non-weather water loss claims than any other state. The work was performed using AWS tools such as EC2s,S3 and Sagemaker.
My hands-on technical work included:

●Building a text model to find connection with fraud claims. Data was collected from SQL server. General cleaning process was done. Used AWS environment to complete the project.
●Accessed production SQL database to pull extract for validation with third-party data.
●Performed data validation between SQL Servers and third-party systems.
●Worked with big data (10M+ observations of text data). [ SQL, SQLite}
●Cleaned the text data using different techniques.
●Integrated with AWS platform environment.
●Utilized cloud computing resources for model optimization/tuning of hyperparameters, and cross-validation of statistical data science models.
●Used Pandas, NumPy, Seaborn, Matplotlib, SciKit-learn in Python for developing various machine learning models and utilized algorithms such as Logistic regression, Random Forest, Gradient Boost Decision Tree and Neural Network.
●Built and analyzed datasets using Python and R.
●Applied linear regression in Python and SAS to understand the relationship between different attributes of dataset and causal relationship between them.
●Applied Exploratory Data Analysis (EDA to analyze datasets to summarize their main characteristics,
●Performed EDA such as bag of words, K-means and DBSCAN etc.
●Utilized Git for version control on GitHub to collaborate work with the team members.
●Used different embedders such as Universal Google Encoder, DocToVec, TFIDF, BERT and ELMO to identify the best embedder that yields the best performing result.
●Implemented models to predict previously Identified Key performance indicators (KPI's) among all attributes.
●Developed several ready-to-use templates of machine learning models based on specifications given and assigned clear descriptions of purpose and variables to be given as input into the model.
●Prepared reports and presentations using Tableau, MS Office, ggplot2 that accurately convey data trends and associated analysis.
●Worked with Data Warehouse architecture and wrote SQL queries.

Data Scientist July 2017 – May 2021
Northwestern Memorial Hospital
Chicago, IL

This interesting project I was working on at Northwestern University Feinberg School of Medicine, Northwestern Memorial Hospital. HIV is still a mysterious virus. Using machine learning image detection and classification problems I used a model to track HIV behavior in different cells. After Covid started I started using the same technology for SARS-2. The research target was identifying spike protein-14 which is believed to target protein for covid-19.

●Cleaned and pre-processed data by removing images with labels and markings along with resizing images to a standardized size.
●Used a generator to rotate and mirror images to synthetically increase image pool size and introduce rotational invariance to the model.
●Constructed several Autoencoder and classification models both supervised and unsupervised with various combinations in-between.
●Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.
●Constructed several Autoencoder and classification models both supervised and unsupervised with various combinations in-between.
●Constructed several Convolutional Neural Networks based off architectures such as VGG-16.
●Utilized Python libraries such as Pandas, NumPy and Plotly to preprocess and clean image data.
● Used Keras, Torch and other frameworks including Theano and TensorFlow for neural network generation and optimization.
●Used fastai library for deep neural network

Data Scientist April 2013 - July 2017
Echo Global Logistics
Atlanta, GA

Trucking company, organization of Japanese manuals and information into a usable resource for the business using NLP techniques and Database management tools to develop a cohesive resource for use by the business.
●Used SQLand Python to collect, analyze, and preprocess the data.
●Utilized ggplot and other visualization libraries to perform initial data exploration and previsualization.
●Used Python to explore the data using statistical methods along with visualization with the use of Python packages.
●Worked on data preprocessing and cleaning the data utilizing Pandas, NumPy, and performing feature engineering and data imputation techniques for missing values in the dataset using Python.
● Performed stemming and lemmatization of text to remove superfluous components and make the resulting corpus as small as possible while containing all important information.
●Experience with Keras and TensorFlow in developing predictive algorithms.
●Solved analytical problems, and effectively communicated methodologies and results.
●Concept space embedding via ELMo was also tested and found to have similar results to bag of words with significant increase in computational time.
●Constructed an NLP-based filter utilizing embedding and LSTM layers in Tensorflow and Keras.
●Produced classifications whether given japanese manual correctly identified or not
●Checking Japanese addresses for shipping purposes using a machine learning classification model.

Data Scientist February 2008 - April 2013
Black Rock, New York, NY
Created alternative methods to scout companies in anticipation for LBO negotiations. Data training sets were created, and classification of documents to relevant categories were used in the search for companies with low market visibility. Statistical Models and NLP techniques and were used to filter data from several sources on the web. Relevant documents led to the discovery and acquisitions of several companies structured as LBOs that yielded estimated averages of IRRs of 25% with expected exit times of 2-4 years. Revenue and Earnings were forecasted using time series analysis such as ARIMA and similar models.

●Use of a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction using NTLK, word2vec and TF/IDF
●Cleaned, parsed, and tokenized 1.6 million sentences using NLTK and scikit-learn.
●Developed, deployed, and maintained production NLP models with scalability in mind.
●OLAP cubes used in preparation for data mining, behavioral and attitudinal segmentation, predictive modeling, insight extraction, and data visualization
●Implementation of machine learning algorithms and concepts such as: K-means Clustering (varieties), gaussian distribution, decision tree etc.
●Analyzed data using data visualization tools and reported key features using statistic tools and supervised machine learning techniques to achieve project objectives.
●Analyzed large data sets and applied machine learning techniques and develop predictive models, statistical models.
●Established Earnings Forecasts using ARIMA (AutoRegresive Integrated Moving Average) and other time series models.
●Used key indicators in Python for machine learning concepts like regression, boot strap aggregation and random forest.
●Use of inferential statistics and machine learning data pipelines.
Sr. Data Analyst November 2004 - February 2008
Wegmans
Rochester, NY

Wegmans is a grocery store primarily located in the northeast. Their produce division sells organic versions of most of their products in addition to the regular versions. I was tasked with finding a strategy to determine how many more people are willing to pay for organic produce. My primary tool was the Linear Regression model to find how much more the average person would be willing to pay for the organic version of the same product.

●Utilized statistical methods to analyze pricing and sales data to determine the value-added of organic labelling on produce products.
●Built linear regression models in R-Programming to determine statistically significant coefficients.
●Outlined a prescriptive plan to improve sales profits by using more accurately targeted pricing.
●Performed data visualization in the data exploration phase using ggplot2.
●Utilized a Tobit Regression model to adjust the results for a high amount of censored data.
●Presented my findings to stakeholders and decision makers to better inform future decisions.
●Performed feature engineering to clean and process the data to feed to my model.
●Used outside resources to supplement the data we had already gathered.

Data Analyst August 2002 – November 2004
Sonali Bank Limited
Dhaka, Bangladesh
As a Data Analyst I was tasked with developing a model for fraud detection. Using unsupervised clustering techniques such as DBScan and Spectral Analysis the model was able to identify anomalies and flag possible fraud and alert internal resources for further review. Following this success, we performed churn analysis on various aspects of customer behavior to provide insights to the business team using Logistic Regression. Similarly, analysis was performed on financial data analysis to predict credit improvement and recovery.
●Database Management: designed and built a MySQL data to store and manipulate the collected data.
●Implementation of DBSCAN
●Developed a segmentation solution using various clustering analysis methods including k-means clustering, Spectral and DBSCAN.
●Identified Key performance indicators (KPI's) among all the attributes.
●Participated in data cleaning and data engineering via pre existing features
●Conducted data analysis on the reports/dashboards - identify gaps.
●Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.
●Created a theoretical model to explain patterns in the data based on the perceived primacy of leverage.
●Prepared reports and presentations using MS Office, matplotlib that accurately convey data trends and associated analysis at Board meetings.
●Participated in all phases of data mining, data collection, data-cleaning, developing models, validation, and visualization and performed statistical analysis/machine learning.

Education

Master of Business Administration in Management Information System
University of Dhaka

Master of Science in Applied Statistics
University of Dhaka

Bachelor of Science in Applied Statistics
University of Dhaka

"
,
Sr Data Scientist,"
404-***-****

adppva@r.postjobfree.com
Demonstrated skill applying statistical modelling, statistical hypothesis testing, and optimization.
Strong track record executing applied machine learning projects.
Skilled applying practical machine learning (e.g., regularization, cross-validation, feature engineering and selection, dimensionality reduction, generalization, and model selection).
In-depth understanding of neural networks, including convolutional and recurrent architectures, as well as unsupervised approaches, such as auto encoders or Restricted Boltzmann Machines.
Expertise in all common supervised machine learning methodologies – Naïve Bayes Classifiers, Linear Regression, Logistic Regression, Support Vector Machines, Support Vector Regression, Random Forests, Regression Forests, and Poisson Survival Modeling.
Strong proficiency with TensorFlow for building, testing, validating, selecting, and deploying successful and reliable machine learning algorithms using Python.
Skilled working with NumPy stack (NumPy, SciPy, Pandas, and Matplotlib).
Experience with automated data collection from online sources using BeautifulSoup and RoboBrowser (Python) or RSelenium, RCurl, Curl, HTTR, and RVest (R).
Experience with a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction in Python.
Adept at discovering patterns in data using both algorithms, visual representation, and intuition.
Hands-on applying machine learning techniques such as Naïve Bayes, Linear and Logistic Regression Analysis, Neural Networks, RNN, CNN, Transfer Learning, Time-Series Analysis, Trees and Random Forests.
Perform exploratory analysis on varying types of data and datasets, allowing for a full knowledge of the subject matter, a nuanced understanding of the variables in question, and a technically sound insight into the required modeling approach.
Design and presentation of interactive data visualizations and widgets in Python using Matplotlib, Ggplot2, Plotly, Seaborn, and in R using Tidyverse and R Shiny for visualization.
Produce Custom BI reporting dashboards in Python using Dash with Plotly for rapid dissemination of actionable, data driven insights.
Transform business requirements into analytical and statistical data models in Python and TensorFlow.
Utilize Docker to handle deployment on heterogeneous platforms such as Linux, Windows, OSX, and AWS.
Provide value-add interacting with stakeholders/customers and gathering requirements through interviews, workshops, and existing system documentation or procedures, defining business processes, and identifying and analyzing risks using appropriate templates and analysis tools.
Experience working with relational databases with advanced data SQL skills.
Strong proficiency with non-relational (NoSQL) databases, such as MongoDB, DynamoDB, etc.
Experience with third-party cloud resources – AWS, Google Cloud Platform, and MS Azure.
Hands-on skill with Tidyverse, Dplyr, Readr, Zoo, Glmnet in R, Pandas, Numpy, Matplotlib, Seaborn, Scikit-Learn in Python for performing exploratory data analysis (EDA), and data preprocessing.
Experience with data analysis methods such as data reporting, ad-hoc data reporting, graphs, scales, pivot tables, OLAP reporting with Microsoft Excel, R Markdown, R Shiny, Python Markdown.

TECHNICAL SKILLS TABLE
Data Science Specialties: Natural Language Processing, Machine Learning, Internet of Things (IoT) analytics, Social Analytics, Predictive Maintenance, Stochastic Analytics
Analytic Skills: Bayesian Analysis, Inference, Models, Regression Analysis, Linear models, Multivariate analysis, Stochastic Gradient Descent, Sampling methods, Forecasting, Segmentation, Clustering, Naïve Bayes Classification, Sentiment Analysis, Predictive Analytics
Analytic Tools: Classification and Regression Trees (CART), H2O, Docker, Support Vector Machine, Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, RNN, Linear and non-Linear Regression
Analytic Languages and Scripts: R, Python, HiveQL, Spark, Spark MLlib, Spark SQL, Hadoop, Scala, Impala, MapReduce
Languages: Java, Python, R, Command Line, C++/C, JavaScript, SQL, SAS
Python Packages: Numpy, Pandas, Scikit-learn, Tensorflow, SciPy, Matplotlib, Seaborn, Plotly, NLTK, Scrapy, Gensim
Version Control: GitHub, Git, SVN
IDE: Jupyter Notebook, VS Code, Intellij IDEA, Spyder, Eclipse
Data Query: Azure, Google Bigquery, Amazon RedShift, Kinesis, EMR; HDFS, RDBMS, SQL, MangoDB, HBase, Cassandra and NoSQL, data warehouse, data lake and various SQL and NoSQL databases and data warehouses
Deep Learning: Machine Perception, Data Mining, Machine Learning algorithms, Neural Networks, TensorFlow, Keras
Soft Skills: Experienced with delivering presentations and technical reports; collaboration with stakeholders and cross-functional teams, and advisement on how to leverage analytical insights. Developed analytical reports which directly address strategic goals
PROFESSIONAL WORK EXPERIENCE
Mobile Apps Co.
Atlanta, GA
May 2020 - Now
Machine Learning Engineer

Assigned to a dev team that built a pipeline for automatic data entry of scanned government forms using AWS as storage for our database. The pipeline accepts two images (the image for information extraction and the reference form) and outputs information such as names, id, and other tax information. The information extraction steps were implemented using Google Tesseract and AWS text Extract as well as Convolutional Neural Networks. We performed OCR on the image to extract wanted information and stored them in our AWS database. We reduced the data entry time exponentially by a power of 3.
This project included using Machine Learning techniques and Python libraries to derive relevant analysis and metrics, including building proof of concepts to determine value of implementing in future projects. As part of a team focused on understanding the customer experience across the organization, I worked with business partners to understand the business objectives, explore data sources, and build NLP/ML solutions. I was able to think outside the box to uncover new ways to analyze unstructured data leveraging our Open-Source Data Science Platform (OSDS) and other analytical tools to solve complex business objectives.

Applied business analytics skills, integrated, and prepared large and varied datasets, and communicated results.
Worked with specialized database architecture and computing environments.
Developed analytic approaches to strategic business decisions.
Performed analysis using predictive modeling, data/text mining, and statistical tools.
Built predictive modeling using Machine Learning algorithms such as Random Forests, Naive Bayes, Neural Networks, MaxEnt, SVM, Topic Modeling/LDA, Ensemble Modeling, GB, etc.
Used common NLP techniques, such as pre-processing (tokenization, part-of-speech tagging, parsing, stemming).
Performed semantic analysis (named entity recognition, sentiment analysis), modeling, and word representations (RNN / ConvNets, TF-IDF, LDA, Word2vec, Doc2vec).
Synthesized analytic results with business input to drive measurable change.
Performed data visualization and developed presentation material using Tableau.
Defined key business problems to be solved while developing and maintaining relationships with stakeholders, SMEs, and cross-functional teams.
Provided knowledge and understanding of current best practices and emerging trends within the analytics industry.
Participated in product redesigns and enhancements to know how the changes would be tracked and to suggest product direction based on data patterns.
Applied statistics and organized large datasets of structured and unstructured data.
Worked with applied statistics and applied mathematics tools for performance optimization.
Facilitated data collection to analyze document data processes, scenarios, and information flows.
Determined data structures and their relations in supporting business objectives and provided useful data in reports.
Assisted in continual improvement of AWS data lake environment.
Used Agile approaches, including Extreme Programming, Test-Driven Development, and Agile Scrum.
Promoted enterprise-wide business intelligence by enabling report access in SAS BI Portal and Tableau Server.

Freeport McMorran
Phoenix, AZ
June 2018 – May 2020
Data Scientist/Computer Vision Researcher
Freeport McMorran is a large mining operation with major interest in copper and other metals. Operations span the globe, with main operations in the United States and Peru. The Data Science/Machine Learning mandate was to create analysis tools to optimize the delivery and production of process ore as well as preventative maintenance of mining/crushing equipment.
I worked as a Data Scientist/Computer Vision Researcher to find state-of-the-art solutions in classifying crushable and uncrushable materials for the company’s production pipeline. The dev team built a full pipeline to make inferences in real time. Before sending material for further processing, our model had to decide whether material was crushable or not. We tested various computer vision architectures such as VGG16, Resnet, Inception, EfficientNet, etc. Resnet gave the best performance for a recall of 0.85 and precision 0.7 and fast inference time (~3s). We aimed to maximize recall in this problem because it was more cost-effective to filter out as many uncrushable materials than to keep as many crushable ones.
This project built “digital twins” — computer models replicating mining equipment behavior. Input from sensor readings was applied to specific field issues such as ore truck loading and conveyor system optimization.

Worked in Git development environment.
Applied expert-level Python and SQL Server development skills.
Utilized Tensoflow, Keras, Python, and deep neural network and analytical techniques.
Transformed business requirements into analytical models, designed algorithms, built models, and developed data mining and reporting solutions that scaled across massive volumes of structured and unstructured data.
Applied Statistical NLP / Machine Learning, especially Supervised Learning- Document classification, information extraction, and named entity recognition in-context.
Worked with Proof of Concepts (POC's) and gap analysis and gathered necessary data for analysis from different sources.
Prepared data for data exploration using data wrangling.
Designed physical data architecture of new system engines.
Implemented neural networks.
Worked with Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, neural networks, Principal Component Analysis, and Recommender Systems.
Developed Logical and Physical Data models and organized data per the business requirements using Sybase Power Designer, ER Studio in both Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) applications.
Designed Star Schema and Snow Flake schema for Data Warehouse and Operational Data Store (ODS) architecture.
Worked with Python and Scala languages and software packages such as Stata, SAS, and SPSS to develop neural network and cluster analysis.
Designed visualizations using Tableau software and published and presented dashboards,
Developed Logical Data Architecture with adherence to Enterprise Architecture.
Used Pandas in Python for performing Exploratory data analysis.
Worked with data modeling tools Power Designer and ER Studio.
Applied Normalization and De-Normalization techniques for optimum performance in relational and dimensional database environments.
Applied System Analysis, E-R/Dimensional Data Modeling, Database Design, and implemented RDBMS-specific features.
Implemented Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio, SSIS, SSAS, and SSRS.
Responsible for Data Analytics, Data Reporting, Ad-hoc Reporting, Graphs, Scales, PivotTables, and Online Analytical Processing (OLAP) reporting.
Interacted with data from Hadoop for basic analysis and extraction of data in the infrastructure to provide data summarization.
Created visualization tools and dashboards with Tableau, Ggplot2 and D3.js.
Worked with and extracted data from various database sources like Oracle, SQL Server, and DB2.

ETSY
New York. NY (Remote)
April 2016 – June 2018
Computer Vision Specialist

Etsy, Inc. is an American e-commerce company focused on handmade or vintage items and craft supplies. I was part of the Data Science Marketing team. I helped enhance the user experience by building a similarity search model to reduce identical search results, and a classification model to categorize real prices against exaggerated prices of an item. In the end, my team proposed a novel approach to improve the company’s Recommendation system with computer vision. We leveraged object detection with Fast RCNN using PyTorch to generate datasets from Etsy’s large database. Our model outperformed the baseline model using just normal CNN architectures and was featured during the wrap-up of the program.

Used pretrained model to segment frame images into objects for identification and removal/replacement.
Utilized Python libraries such as Pandas, NumPy and Plotly to preprocess and clean image data.
Developed processes and tools to monitor and analyze performance and data accuracy.
Enhanced data collection procedures to include relevant information to build and continuously optimize the analytics systems.
Collaborated with IT to continuously optimize business performance.
Processed, cleansed, and verified the integrity of data from various sources used for analysis and reporting.
Leveraged the latest data visualization tools and techniques to present and communicate analysis to the leadership team utilizing data management, analytics modeling, and business analysis.
Used predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.
Advised leadership team and stakeholders with data-driven solutions and recommended strategies that address business challenges.

Axiom Tech Group
Chicago, Illinois (Remote)
February 2015 – April 2016
Trainee/student Internship Data Analyst

Axiom provides enterprise advisory solutions and risk management. The firm relies on big data analytics to provide strategic insights and reporting to outside clients. I worked on analytics projects for various clients, and my focus was on cleaning data and performing analysis and reporting.

Involved in ETL/BI requirement gathering and conversion into useful functional requirements.
Performed source-to-target data mapping document preparation.
Developed report wireframes along with SQL schema data element definitions.
Worked with Data Warehouse architecture and wrote SQL queries.
Applied dimension modelling to identify dimension and fact tables and associated data elements.
Conducted in-depth data analysis on the reports/dashboards to identify gaps.
Involved in data governance to find authoritative sources for the critical data elements used in the governance reports.
Profiled data to validate data quality issues for the critical data elements.
Participated in user acceptance testing to ensure software satisfied all requirements before it was deployed to production.
Applied nnowledge in BFSI domain and financial markets.
Worked as an ETL/Reporting Tester.
Prepared test plans and strategy documents and tested case preparations based on the requirements.
Performed end-to-end testing in DWH projects.
Validated ETL jobs against requirements by running through Control-M scheduler.
Validated target table structures and constraints against ETL requirements.
Validated target data against source data based on ETL requirements.
Worked with module testing, including defect capturing in ALM.
Experienced with complete software development life cycle (SDLC) and software testing life cycle (STLC) life cycles.
EDUCATION

Bachelor of Science - Electrical and Computer Engineering - Texas Southern University
CERTIFICATIONS
TensorFlow Developer Certificate in 2021: Zero to Mastery - Aug 2021
Automate the Boring Stuff with Python Programming - May 2021
LANGUAGES

Proficient in English, French, Spanish and Portuguese (intermediate level)

"
,
Sr. Data Scientist,"
Professional Summary
Over ** years of quantitative modeling, data science, machine learning, and Python programming experience.
Extensive experience in cross-department project management.
Experience in the application of Naïve Bayes, Regression Analysis, Neural Networks/Deep Neural Networks, Support Vector Machines (SVM), and Random Forest machine learning techniques.
Experience in machine learning models and statistical models on big data sets using cloud/cluster computing assets with AWS and Azure.
Extensive experience on time series analysis and survival analysis.
Work with product development department for optimal product design, pricing, and marketing strategy.
Hands-on experience in credit risk modeling from hazard models, severity models, and exposure at risk models.
Extensive quantitative Natural Language Processing (NLP).
Convolutional Neural Networks, Computer Vision.
Customer behavior predictive modeling on lapse/churn, withdraw.
Fraud detection and prevention with financial transactions.
Extensive model validation experience in data science, and quantitative modeling.
Excellent communication skills (verbal and written) with demonstrated ability to communicate with clients/stakeholders and team members.
Technical Skills
Analytic Development: Python, Javascript, Matlab, SAS, Spark, SQL VBA, C++, C
Python Packages: Numpy, Pandas, scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn, Numba, SpaCy, NLTK, LightGBM, XGBOOST, CatBoost, Dask, Gensim
IDE: Jupyter, Spyder, MatLab, Visual Studio.
Version Control: GitHub, Git
Machine Learning: Time Series Prediction, Natural Language Processing & Understanding, Machine Intelligence, Generalized Linear Models, Machine Learning algorithms
Data Query: Azure, Google Cloud, Amazon RedShift, Kinesis, EMR; RDBMS, Snowflake, SQL and data warehouse, data lake and various SQL and NoSQL databases.
Deep Learning: Machine perception, Machine Learning algorithms, Neural Networks, TensorFlow, Keras.
Artificial Intelligence: text understanding, NLP, Computer Vision, customer behavior predictive modeling, classification, pattern recognition, targeting systems, ranking systems.
Analysis Methods: Advanced Data Modeling, Time Series Analysis, Forecasting, Predictive, Statistical, Sentiment Analysis, Exploratory, Stochastic Calculus, Bayesian Analysis, Inference, Models, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Predictive Analytics, Big Data and Queries Interpretation, Design and Analysis of Experiments, Association Analysis
Analysis Techniques: Classification and Regression Trees (CART), Support Vector Machine, Random Forest, Gradient Boosting Machine (GBM), TensorFlow, Principal Component Analysis, Recurrent Neural Networks, Regression, Naïve Bayes
Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series Analysis, Survival Analysis
Applied Data Science: Natural Language Processing, Machine Learning, Social Analytics, Predictive modeling
Soft Skills: Excellent communication and presentation skills; ability to work well with stakeholders to discern needs accurately, leadership, mentoring, coaching

Professional Experience
Sr. Data Scientist August 2020 – Present
Compass Insurance Group
Grand Rapids, MI
Compass Insurance Group is one of the largest insurance brokers in the region, with presence in 20 states. Compass Insurance Group serves large corporations to individuals. I used several advanced Machine Learning models and Deep Learning techniques for different use-cases across the organization. I developed a recommender system that incorporated Principal Component Analysis and Singular Value Decomposition in making collaborative suggestions about prospective customers. Early A/B testing showed that insurance agents who used the recommender system were able to close deals 13% more often than those who had not adopted the system.
Developed Recommendation systems, Churn prediction, Customer Life Time Value models.
Developed time-series models using ARIMA, SARIMA, and Deep Learning approach with Recurrent Neural Networks and LSTM to forecast and optimize sales and revenue.
Automated time-series analysis with Prophet.
Developed personalized product recommendations with machine-learning algorithms that used Collaborative filtering to better meet the needs of existing customers and acquire new customers.
Created machine-learning algorithm and employed logistic regression, random forest, KNN, SVM, neural network, linear regression, lasso regression and k-means.
Developed optimization algorithms for use with data driven models such as with supervised and unsupervised machine learning or reinforcement machine learning.
Researched statistical machine-learning methods that included forecasting, supervised learning, classification, and Bayesian methods.
Advanced the technical sophistication of solutions through the use of machine learning and other advanced technologies.
Performed exploratory data analysis and data visualizations using R and Tableau.
Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.
Performed data integrity checks, data cleaning, exploratory analysis, and feature engineering using R and Python.

Lead Data Scientist November 2019 – August 2020
SoundOff Signal
Hudsonville, MI
SoundOff Signal is one of the largest US manufacturers of integrated, strategic lighting and controls products (e.g., lighting, sirens, speakers, and switches). Purchase order processing was automated using Optical Character Recognition (OCR) on scanned documents. Prior to my work, they were creating handwritten purchase orders and filing them in a filing cabinet. This took additional time and manpower when a record needed to be referenced. This system was automated so that scanned documents could be submitted to an API and the results of the OCR were stored in a SQL database for easy referencing. I used NLP and Computer Vision techniques for it.
Led the development team and implemented the complete solution.
Wrote code in Python and SQL
Implemented various NLP techniques to identify text fragments.
Used Tensorflow and Keras to implement word embeddings.
Used Google Tesseract and AWS Text Extract.
Used common NLP techniques such as pre-processing (tokenization, part-of-speech tagging, parsing, stemming).
Used NLP techniques to sort and classify documents.
Used Hierarchical time-series analysis to forecast competent usage across various products.
Used AWS Redshift Data Warehouse and Boto 3 to access AWS Resources from Python.
Worked with product development team for optimal product design, pricing and marketing strategy.
Performed semantic analysis (named entity recognition, sentiment analysis), modeling and word representations (RNN / ConvNets, TF-IDF, LDA, word2vec, doc2vec).
Built predictive modeling using Machine Learning algorithms such as Random Forests, Naive Bayes, Neural Networks, MaxEnt, SVM, Topic Modeling/LDA, Ensemble Modeling, GB, etc.
Used CNN techniques for different object recognition techniques.
Used pre-trained models (VGG16, ResNets, Inceptions, DenseNet, U-Net, etc.) for transfer learning on small datasets.

Sr. Data Scientist/ML Engineer June 2018 – October 2019
Frito-Lay Inc.
Byron Center, MI
Frito-Lay is one of the largest snack food manufacturers in the world. Frito Lay is property of PepsiCo. I was responsible for regional analytical activities where I created and updated ML models. The machine-learning algorithms we employed were able to continue predicting consumption on a very granular (SKU-State) level. This provided demand planners with significantly more accurate, actionable information.
Developed a demand-forecasting model based on different time-series techniques to assist demand planners effectively allocate resources.
Utilized machine-learning models to implement a high-performing demand forecasting framework from scratch.
Satisfied critical requests from executive leadership: previous models could not adjust to the market demands.
The model obtained consistent, high-quality results through the use of hierarchical modeling (MLib/GBT).
Advised about how best to modify existing predictive out-of-stock models to accurately forecast for a longer time-horizon.
Worked in PySpark, Python on Azure Databricks.
The model was a significant improvement over the baseline univariate time-series forecasts that were used previously.

Data Scientist May 2016 – June 2018
Dollar General
Kentwood, MI
Dollar General is one of the largest discount retailers in the US. I was a Data Scientist for the marketing regional team. I worked to forecast future sales. Sales data for the past three years was analyzed and fit to models. An ARIMA model was fit to the data to forecast weekly sales into the next quarter. Models revealed shopping trends that were being under-capitalized by Dollar General.
Engineered a solution in the R programming language.
Experimented with time-series models such ARIMA GARCH to produce reliable forecasting.
Accessed and integrated large datasets from remotes servers using SQL.
Applied statistical testing to the model to determine appropriate autocorrelation and partial auto-correlation lags.
Forecasted sales for the next quarter.
Cleaned and normalized data set to optimize performance and reliability of predictions.
Collaborated with advertising to form a plan to capture the market during newly revealed consumer trends.
Communicated results through interactive visuals using the Javascript library D3.

Data Scientist for Sales January 2014 – April 2016
Sears Parts and Service
Grand Rapids, MI
Sears Parts and Service was part of Sears Roebuck Co. I served on an Analytics team for the Sales department as a Data Science consultant for Sears Parts and Service store division. I led a team to optimize a recommendation engine using a Collaborative Filtering and Content based Recommender system. The revenue impact from deployment of the new optimized Recommender system on their regional sales was expected to be more than 9%.
Used techniques such as collaborative and content based filtering demographics for creating different recommender systems.
Used A/B testing to test the effectiveness of different types of recommender systems and optimized the most effective recommender system after careful tests and research.
Partially solved the “cold start” problem of Recommender system by incorporating the demographic-based Recommender system in the final SVD Recommender system.
Worked on data pre-processing and cleaning the data utilizing Pandas, NumPy, and performed feature engineering and data imputation techniques for missing values in the dataset using Python.
Performed stemming and lemmatization of text to remove superfluous components and make the resulting corpus as small as possible while containing all important information.
Solved analytical problems and effectively communicated methodologies and results.

Data Scientist January 2010 – January 2014
Independent Contractor
California, CA
Worked on several small projects in data science and statistics as a freelance data scientist.
Examined the relationship between SAT/ACT scores and college admissions.
Performed deep mathematical analysis of large datasets using R and Ggplot to produce visualizations that revealed the relationships and trends within the data.
Investigated correlations between temperature and energy demand.
Created logistic regression model to demonstrate likelihood of acceptance into various industries.
Performed NLP, topic modeling, and clustering analysis on job titles and descriptions to identify multiple employment opportunities in the same field with different names.
Utilized decision trees in Python to explain feature importance and observe effect of weather data on product sales.

Data Analyst May 2006 – January 2010
Sears Holdings Company
Chico, CA
Marketing Data Analyst for the Regional Sales Division. Supported many aspects of the business with internal analytics and provided lean, actionable, data-driven insights for clients on indirect procurement consulting engagements. Performed various statistical analyses and linear optimization techniques to create the greatest possible profit impact for clients.
Maintained and contributed to many internal R packages used for building and diagnosing models, and automated reporting.
Used R to perform ad-hoc analyses and deeper drill downs into spend categories of particular interest to clients on a project-to-project basis.
Performed large data cleaning and preparation tasks using R and SQL to gather information from disparate and incompatible data sources from across a client’s entire enterprise to provide a complete view of all indirect spends.
Helped maintain a large database of commodity and vendor information using SQL.
Maintained various visualization tools and dashboards used to provide data-driven insights.

Operations Management Tutor August 2013 to May 2014
Chico State University
Chico, CA
Quantitated methods for process optimization.

Education and Training
Master of Science: Applied Statistics
Michigan Technological University - Houghton, MI
Advanced Statistical Methods:
oDeveloped understanding of statistics, predictive modeling, probability, and time-series data.
oEnhanced experience with graphical methods, probability models, parameter estimation, and hypothesis testing.
Programming and Technology:
oCombined tested techniques with emerging technologies.
oImproved familiarity with industry-standard software and tools such as R, Python, and SAS.
oExperience with real-world datasets to overcome common challenges.
Communication and Leadership:
oBuilt the skillset necessary to draw accurate conclusions, present outcomes with confidence, and drive organizational decision-making.

Bachelor of Science: Applied Mathematics
California State University Chico - Chico, CA
Developed skills necessary to utilize mathematically rigorous methods, modeling insights, and computational algorithms to solve real-world problems from science, engineering, and society.

"
,
Ezan Marlon Kodjo -ML Engineer & Data Scientist,"
Professional Summary
●* years’ overall experience covering Software/Information Technology/Web Development and Data Science.
●7 Years in Data Science
●Expertise in Machine Learning, Deep Learning, Natural Language Processing, and Data Analytics Ml_Ops, Model Productionizing and Monitoring
●Projects involving Sentiment Analysis, Fraud Detection, Predictive Analytics, Artificial Intelligence
●8 years of Python development
●Extensive work in Natural Language Processing and Predictive Analytics using Machine Learning Algorithms, Visualization Tools, and Web Deployment Technologies.
●Used Neural Networks, Trees, Clustering Algorithms, and Statistical Models to propel systems which perform Sentiment Analysis, Fraud Detection, Client Segmentation, Predictive Maintenance, Demand Forecasting.
●Business understanding, Data understanding, Data preparation, Modeling, Evaluation and Deployment.
●Experienced in practical application of data science to business problems to produce actionable results.
●Experience in Natural Language Processing (NLP), Machine Learning & Artificial Intelligence.
●Experience with AWS, Kubernetes, and Azure cloud computing.
●Spark (especially AWS EMR), Kibana, Node.js, Tableau.
●Able to incorporate visual analytics dashboards.
●Experience with a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction.
●Knowledge on Apache Spark and developing data processing and analysis algorithms using Python
●Programming strength in Python, C#, C++, Java, SQL, R, Matlab, Mathematica, Javascript
●Use of libraries and frameworks in Machine Learning such as NumPy, SciPy, Pandas, Theano, Caffe, Sci-Kit Learn, Matplotlib, Seaborn, TensorFlow, Keras, PyTorch, NLTK, Gensim, Urllib, Beautiful Soup
●Ability with algorithms, data query and process automation.
●Evaluation of datasets and complex data modelling.

Technical and Project Team Collaboration Skills
LEADERSHIP - Push project goals, determine business use cases, and mentor/lead teams
QUALITY - Continuous improvement in project processes, workflows, automation and ongoing learning and achievement CLOUD Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud)
CLOUD - Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud)
ANALYTICS - Data Analysis, Data Mining, Data Visualization, Statistical Analysis, Multivariate Analysis, Stochastic Optimization, Linear Regression, ANOVA, Hypothesis Testing, Forecasting, ARIMA, Sentiment Analysis, Predictive Analysis, Pattern Recognition, Classification, Behavioral Modeling
PROGRAMMING LANGUAGES - Python, R, SQL, Java, MATLAB, Mathematica, C#, C++, Javascript, PHP
LIBRARIES - NumPy, SciPy, Pandas, Theano, Caffe, SciKit-learn, Matplotlib, Seaborn, Plotly, TensorFlow, Keras, NLTK, PyTorch, Gensim, Urllib, BeautifulSoup4, PySpark, PyMySQL, SQAlchemy, MongoDB, SQLite3, Flask, Deeplearning4j, EJML, DPLYR, GGPLOT2, Reshape2, TIDYR, PURRR, READR, Apache, Spark, MapReduce, WPF, Entity Framework Core, Node.js
DEVELOPMENT - Git, GitHub, GitLab, Bitbucket, SVN, Mercurial, Trello, PyCharm, IntelliJ, Visual Studio, Sublime, JIRA, TFS, Linux, Unix
DATA EXTRATION AND MANIPULATION - Hadoop HDFS, Hortonworks Hadoop, MapReduce, Cloudera Hadoop, Cloudera Impala, Google Cloud Platform, MS Azure Cloud, SQL, NoSQL, Data Warehouse, Data Lake, SWL, HiveQL, AWS (RedShift, Kinesis, EMR, EC2, Lambda)
NATURAL LANGUAGE PROCESSING - Document Tokenization, Token Embedding, Word Models, Word2Vec, FastText, Bag Of Words, TF/IDF, Bert, Elmo, LDA.
MACHINE LEARNING - Supervised Machine Learning Algorithms (Linear Regression, Logistic Regression, Support Vector Machines, Decision Trees and Random Forests, Naïve Bayes Classifiers, K Nearest Neighbors), Unsupervised Machine Learning Algorithms (K Means Clustering, Gaussian Mixtures, Hidden Markov Models, Auto Encoders), Imbalanced Learning (SMOTE, AdaSyn, NearMiss), Deep Learning Artificial Neural Networks, Machine Perception
APPLICATIONS - Machine Language Comprehension, Sentiment Analysis, Predictive Maintenance, Demand Forecasting, Fraud Detection, Client Segmentation, Marketing Analysis

Professional Work Experience

Feb 2021 – Present
Location: Newark, NJ
Position: Senior ML Engineer/Data Scientist
Company: Prudential Financial
Experience Summary:
Prudential Financial, Inc. is an American Fortune Global 500 and Fortune 500 company whose subsidiaries provide insurance, investment management, and other financial products and services to both retail and institutional customers throughout the United States and in over 40 other countries. As a Senior ML Engineer, I worked in predicting whether a complaint will be filed on life insurance policies. My efforts were spent converting scripts created by Data Scientist and transforming it into something that meets standards for production (ie optimizations, logging, etc), communicating and facilitating solutions requested by the DS team, and meeting with stakeholders.
Coordinated work from 2 teams (Machine Learning Engineer and Data Science) based around 2 different time zones (EST and PST)
Read, studied, and refactored Data Scientist code
Used acquired knowledge to better communicate concerns, needs, and solutions to product owners
Developed models for the usecase utilizing various tools such as XGBoost, TensorFlow, and PCA
Created Architecture for production plan
Created scripts to utilize the tools in already established production pipelines to automate the process of training models and using said models for inference
Created preprocessing scripts from an amalgamation of already written python scripts
Refactored said scripts to reduce Big O complexity
Added logging and comments
Consistently updated feature engineering to reflect latest Data Science efforts
Refactored scripts to fit within the constraints of the previously mentioned architecture
Created AWS Processing jobs to automate computations
Developed deployments in AWS SageMaker
Communicated usecase needs with Data Engineers
Collaborated with Data Engineers to curate Project migration across AWS accounts
Participated DS team collaborations to find and give feedback for solutions related to other usecases.

June 2019 – Jan 2021
Location: San Francisco CA
Position: Senior ML-Ops Engineer
Company: Levi Strauss & Co.
Experience Summary:
Levis is a manufacturing and merchandizing store that supplies over 500 stores and multiple output channels. As lead ML-Ops engineer, I designed and implemented a restocking solution using AWS Batch and Docker Containers. The Purpose of the restocking solution was to populate an app running on objective C edge devices to inform and design the restocking of product according to predicted demand. The models were built using Sagemaker and Scheduled using Adobe Airflow. At Levi’s, I:
●Built a personalized in-session product recommendation engine.
●Wrote scripts in Python that automated text summarization and clustering.
●Next-Best offer prediction.
●Designing Microassortments for Next-Gen stores.
●Anomaly detection and Root Cause Analysis.
●Prepared data for collaboration with machine learning models.
●Unified consumer profile with probabilistic record linkage.
●Visual search for similar and complementary products.
●Architected, built, maintained, and improved new and existing suite of algorithms and their underlying systems.
●Analyzed large data sets apply machine learning techniques and develop predictive models, statistical models and developing and enhancing statistical models by leveraging best-in-class modeling techniques
●Implemented end-to-end solutions for batch and real-time algorithms along with requisite tooling around monitoring, logging, automated testing, performance testing and A/B testing.
●Worked closely with data scientists and analysts to create and deploy new product features on the ecommerce website, in-store portals and the Levi’s mobile app.
●Established scalable, efficient, automated processes for data analyses, model development, validation and implementation.
●Implemented deployment solutions using Tensorflow, Keras, Docker, and Elastic Kubernetes Service.
●Implemented Model Drift Monitoring and Retraining Strategies.
●AWS, GCP, Azure.
●Sagemaker Model transformation into dedicated preprocessing, inference and model validation scripting.
●ECR, EMR, Azure Kubernetes Service Experience.

March 2017 – June 2019
Location: Atlanta, GA
Company: Bank of America
Position: Data Scientist
Experience Summary:
At Bank of America, I worked as a Natural Language Processing expert and model architect where I built, trained, and tested multiple Natural Language Processing models which classified user descriptions and wrote SQL code based on user questions. The goal of the project was to centralize and search for Splunk dashboards within the Bank of America network and to create an A.I. assistant to automate the coding process to extract information from these dashboards.
●Used Python, and SQL to collect, explore, analyze the structured/unstructured data.
●Used Python, NLTK, Tensorflow to tokenize and pad comments/tweets and vectorize.
●Vectorized the documents using Bag of Words, TF-IDF, Word2Vec, GloVe to test the performance it had on each model.
●Created and trained an Artificial Neural Network with TensorFlow on the tokenized documents/articles/SQL/user inputs.
●Performed Nearest Entity Recognition (NER) by utilizing ANNs, RNNs, LSTMs, and Transformers.
●Involved in model deployment using Flask with a REST API deployed on internal Bank of America systems.
●Wrote extensive SQL queries to extract data from the MySQL database hosted on Bank of America internal servers.
●Built a deep learning model for text classification and analysis.
●Performed classification on text data using NLP fundamental concepts including tokenization, stemming, lemmatization, and padding.
●Performed EDA using Pandas library in Python to inspect and clean the data.
●Visualized the data using matplotlib and seaborn.
●Explored using word embedding techniques such as Word2Vec, GloVe, and Bert.
●Built an ETL pipeline that could read data from multiple macros, processed it using self-made preprocessing functions, and stored the processed data on a separate internal server.
●Used Datamodelr R package to document relational data.
●Automated ETL tasks and scheduling using self-build data pull-request functions.


November 2014 – March 2017
Location: Atlanta, GA
Company: Interactive Intelligence Company
Position: ML Scientist
Experience Summary:
Interactive Analytics is a software development firm. Established the NLP (Natural Language Processing) Lab at II to research ways to better interact with users in written form. Used TF-IDF and NLTK techniques to classify, segment and analyze sentiment of textual data. Implemented various text analysis projects and built-in house NIP libraries in Python to clean, preprocess and tokenize incoming textual data.
●Implemented application of various machine learning algorithms and statistical modeling like Decision Trees, Text Analytics, Sentiment Analysis, Naive Bayes, Logistic Regression and Linear Regression using Python.
●and determined performance.
●Interrogate analytical results to resolve algorithmic success, robustness and validity.
●Use of a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction.
●Developing, deploying, and maintaining production NLP models with scalability in mind.
●Implemented Agile Methodology for building an internal application.
●Use of knowledge databases and language ontologies.
●Wrote a Flask app to call CoreNLP for parts-of-speech and named entity recognition on natural English queries.
●Optimized SQL queries to improve performance of data collection.
●develop an estimate of uncertainties for the semantic predictions made by deep convolutional model.
●Derived high quality information, significant patterns from textual data source. Used Document Term Frequency and TF-IDF (Term Frequency- Inverse Document Frequency) algorithm in order to find information for topic modelling.
●Analyzed large data sets, applied machine learning techniques and developed predictive models, statistical models and developed and enhanced statistical models by leveraging best-in-class modeling techniques.
●Design, develop and produce reports that connect quantitative data to insights that drive and change business.
●Implemented both Elmo and BERT embeddings to correctly encode text.

May 2013 – November 2014
Location: Milford, NJ
Company: Warren Heating & Cooling
Position: Software IT/Web Development
Warren Heating & Cooling is a HVAC service provider.
I worked on web site upgrades:
●Modified the web site’s PHP framework.
●Wrote new functions in Java and modified existing functions in Java.
●Modified multiple scripts written in JavaScript.
●Developed Web API functionality for data validation and back-end database communication using ASP.NET, C#, and SQL Server to support the development of front-end interfaces.
●Assembled unit tests for a variety of Web API scenarios using Visual Studio’s testing components.

Education
BS in Mathematics, Rowan University Galsboro

"
,
Data Scientist,"
Summary
Experienced Data Scientist with more than 7 years in the field, including 3+ years as a Senior Data Scientist. I am skilled at thinking outside the box and finding creative solutions to real world problems. I’m able to formalize these problems and solutions by expressing them with advanced mathematics, statistics, and models, while keeping them understandable to both technical and non-technical audiences with professional reports and intuitive visualizations.

Over 7 years of experience in Data Science, Machine Learning & Deep Learning

Data science, data mining, SQL queries, data modeling, data/business analytics, and data visualization, machine learning in Python using TensorFlow & R using tidyverse.
Experience using R, Python, and SQL & SQL queries (multiple flavors, including MemSQL, MySQL, HQL/HiveQL).
Extensive data science experience in Python (NumPy, TensorFlow, Matplotlib) & R-tidyverse, using data modeling, data analytics, and evidence-based approaches to find lean and actionable solutions and insights to various real-world business problems.
Clearly communicate data analytics and solutions to stakeholders.
Design and presentation of interactive data visualizations and widgets in Python using Matplotlib, ggplot2, Plotly, Seaborn, and in R using tidyverse and R Shiny for visualization.
Custom BI reporting dashboards in Python using Dash with Plotly for rapid dissemination of actionable, data driven insights.
Expertise in transforming business requirements into analytical & statistical data models in Python and TensorFlow, designing algorithms, and designing, building, and deploying custom BI software solutions.
Strong experience in interacting with stakeholders/customers, gathering requirements through interviews, workshops, and existing system documentation or procedures, defining business processes, and identifying and analyzing risks using appropriate templates and analysis tools.
Building statistical models in Python using TensorFlow, data mining, data streaming, and BI reporting solutions that scale across massive volumes of structured data and unstructured data.
Experience in the application of naïve Bayes, regression analysis, neural networks/deep neural networks, support vector machines (SVM), decision trees, random forest, and XGBoost using machine learning and statistical techniques in Python using TensorFlow.
Experience in handling and implementing statistical models on big data sets using cloud computing assets with AWS and Azure.
Creative thinking/strong ability to devise and propose innovative ways to look at problems by using business acumen, mathematical theories, data models, and statistical analysis.
Discover patterns in data using algorithms and SQL queries and use experimental and iterative approach to validate findings in Python using TensorFlow.
Expertise in advanced statistical and predictive modeling techniques to build, maintain, and improve on real-time decision systems in Python and TensorFlow.
Experience in working with relational databases with advanced data SQL skills.
In-depth knowledge of statistical procedures that are applied in both supervised and unsupervised machine learning problems.
Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
Strong experience in Software Development Life Cycle (SDLC) and in supervising teams of domain specific experts to meet product specifications and benchmarks within the deadlines given.
Ability to quickly gain an understanding of very niche subject matter domains and design and implement effective novel solutions to be used by other subject matter experts.
Technical Skills
Machine Learning & Deep Learning:
Linear regression
Logistic regression
Multivariate regression
Engineering features
K-Nearest Neighbors (KNN)
K-means clustering
Random forests
Decision trees
XGBoost
Reinforcement learning
Artificial Neural Network (ANN)
Convolutional Neural Network (CNN)
Recurrent Neural Network (RNN)
Transfer learning
Management:
Experience managing small inter-disciplinary teams consisting of data scientists and subject matter experts in the project domain
Able to prioritize tasks to reach project goals

Communication:
Excellent listener
Able to convey complex ideas to both technical team members and non-technical stakeholders
Programming & Languages:
Python:
Object oriented programming: custom classes, parent/child, inheritance, decorators, attributes, methods, private/protected/public
Building custom libraries and modules
Data science: data mining, scraping, machine learning, regression, classification, Artificial Intelligence, AI, deep learning, automation
Libraries: NumPy, Pandas, TensorFlow, SciPy, PyTorch, Keras, Scikit-Learn, Scikit-Image, Selenium, Matplotlib, Plotly, Seaborn, ggplot2, PySpark, os, and Keras with a TensorFlow backend
• R:
Data science: data mining, machine learning, regression, classification, visualization
Familiar Libraries: tidyverse, dplyr, Shiny, Plotly, ggplot2
• MATLAB:
Differential Equations and Linear Algebra
• SQL:
MemSQL, MySQL, HQL/HiveQL
o SQL queries
• LaTeX:
Creating documentation, reports, presentations, technical writing
• Git:
Branch management
o Pushing, pulling, merging
• Linux:
Terminal
Bash/shell scripting

""RExperience
Senior Data Scientist
American Express
Phoenix, AZ
Jun 2021 – Present

American Express is a multinational corporation that specializes in payment card services. I was assigned to a technology team to set up a local development environment for SOLR and the Amex Search app.
Modified production code from my own Git branch for local testing.
Gathered and studied KT documentation.
Implemented API POST calls with POSTMAN.
Wrote custom Body and Header tags for API GET requests.
Updated POM Endpoints for new Maven projects.
Opened Timsbox/Citrix boxes to access the root project of AS.
Utilized PySpark to load certain data sets in PAE.
Updated Apache Zookeeper XML files for appropriate ledger maintenance.
Organized data dumbs of User Activity data for reporting.
Read JSON files and wrote scripts for text extraction.
Implemented topic modeling methods for feature engineering.
Completed monthly reports for the status/usefulness of tools.
Wrote algorithmic scoring functions to assist with monthly reporting.
Wrote SQL queries to gather data as requested by managers.
Attended WebEx meetings to brainstorm product details.
Used Angular and Java code for product features.
Created PowerPoint presentations of code edits and features.
Provided KT on Text extraction and topic modeling methods to the team.
Assisted in QCing search queries and feature modifications on Angular.
Submitted RFCs for version vulnerabilities (of the packages/software we used).
Reloaded and added new SOLR collections as part of an extractor script.

E
Senior Data Scientist
Fanatics
San Mateo, CA
Oct 2019 – Jun 2021

Fanatics is an online retailer of licensed sports merchandise for all major professional sports leagues, media brands, and a large number of collegiate teams. To increase their online retail KPIs such as click-through rate (CTR) and the number of items sold, I worked with the Online Sales Department to create a hybrid recommender engine to offer relevant suggestions to visitors. After deployment, the company saw an increase in all relevant KPIs which resulted in higher profits.
·Used SQLAlchemy to perform queries and pull data from Amazon S3 MemSQL database into Pandas DataFrames in Python.
·Implemented a Singular Value Decomposition (SVD) collaborative filtering algorithm to recommend items to users.
·Explored a K-Nearest Neighbors (KNN) algorithm with Cosine Similarity to preform collaborative filtering.
·Created Flask app API that returns a software agnostic JSON file for software developers to implement in the site.
·Experimented with PySpark’s MLlib for building recommender engines on big data.
·Utilized Scikit-Learn for creating and training collaborative filtering algorithms.
·Evaluated model performance using Click Through Rate (CTR) and Mean Average Precision (MAP).
·Coordinated with many departments such as the sales department, data engineering team, and software development team.
·Defined different metrics and indicators for item similarity in the content-based approach.
·Worked with multiple data imputation strategies to uncover unknown user-item ratings.
·Evaluated performance vs existing systems using A/B Testing and Canary Deployment.
·Preformed bi-weekly presentations to stakeholders to show progress toward project goals.
·Using Git version control to manage projects, assign tasks and report issues
·Coordinated with the UI/UX team to plan the implementation of recommendations.
·Packages used: NumPy, SciPy, Scikit-Learn, PySpark, MLlib, Pandas, Matplotlib, Seaborn, Flask

Senior Data Scientist
Index Fresh
Bloomington, CA
January 2018 – Sept 2019

Index Fresh is worldwide marketer of avocados, sourcing from all major growing regions around the globe. To ensure appropriate procurement of avocados, Index Fresh needed to build a image recognition tool to help their growers identify potential diseases on avocado trees. I was tasked to build a classification model using a Keras Convolutional Neural Network that can take in photos from field workers, to then returns specific disease classification. The model utilized transfer learning from Densenet201v2, and my demo had a accuracy rate of 93%. After finishing the model training, the weight were stored, and shared with the deployment team.
·Used SQLAlchemy to perform queries and pull data from Amazon S3 MemSQL database into Pandas DataFrames in Python.
·Created python scripts that converts images to a jpeg filetype then resizes to a set size
· Utilized Tensorflow’s Keras Image Processing to develop ImageDataGenerator objects to augment(rotate, blur, flip ) images for a Convolutional Neural Network
·Imported from Tensorflow Kera Applications InceptionResNetV2 as the base model for the initial CNN using sample images from field workers and online resources
·Utilized a second pretrained CNN called DenseNet201 as transfer learning technique to train again data collected from the MemSQL database
·Developed basic Flask app using a static HTML template as a UI to upload images, then return a processed version image with a disease classification
·Evaluated training data, and testing data using SOUS, along with various over sampling, and under sampling techniques to placate unbalanced image data.
·Coordinated with field workers, logistic warehouse workers, and other departments to ensure the image recognition product fits the Avocado disease use case in deployment
·Defined different metrics and indicators for ensure the CNN model maintains its integrity

Machine Learning Specialist
BNSF
Fort Worth, TX
April 2016 – December 2017
The BNSF Railway Company is the largest freight railroad network in North America and is one of the top transporters of the products that help communities survive and flourish. To minimize failures in the braking system and the associated costs, I helped to analyze data that were streamed from IoT devices and stored in AWS RDS SQL database. I created predictive maintenance and survival analysis models by applying classification and regression techniques to determine the probability of failure of the train’s braking machinery. Once deployed, these algorithms reduced failures and significantly cut unnecessary maintenance costs.
Used multiple Python packages for data pre-processing, such as Pandas and NumPy for data manipulation, Scikit-Learn and SciPy packages for data cleaning and manipulation, as well as Matplotlib and Seaborn for visualization exploratory data analysis (EDA).
Several machine learning techniques such as: XGBoost with Cox Proportional Hazard, Accelerated Failure Time Regression, and Scikit-Learn were explored for training and testing models for Survival Analysis.
Built Cox Proportional Hazard models using Lifelines, NumPy, and Tensorflow packages.
Applied Gradient boosted decision tree models built with XGBoost and utilized the Cox Objective function for classification and survival analysis.
Performed hyper-parameter tuning using Grid Search and K-Fold Cross-Validation techniques.
Utilized the concordance index as a metric for the performance of my survival models.
Prepared and delivered several presentations during the project for both technical and non-technical staff/administrators.
Analyzed panel data extracted from remote IoT devices.
Deployed my pickled model inside a Flask app on an AWS EC2 instance that returns a JSON formatted data that the software development then used to integrate into the maintenance scheduling system.
Coordinated with the software development team using Git version control on Bitbucket.
Used R Shiny for data visualization & building dashboards.
Gathered domain knowledge through interviews and online conferences with subject matter experts.
Collaborated with a team of data scientists during exploratory analysis using Python to determine how data sources could be used to satisfy implementation of software features.
Reduced the dimensionality of my data using Principal Component Analysis (PCA) for increased training/prediction speed and model performance.

Data Scientist
Santander Bank
Boston, MA(Remote)
June 2014 – April 2016
Santander Bank is a diversified financial business that helps people and businesses prosper and is in the Northeast region of the United States. I was part of a small team of data scientists that worked with the Security Department tasked with fraud detection. My duties included lesion between the US based team and the overseas team. I applied an ensemble of classification and unsupervised models to identify fraudulent activity within the incoming transactions. Once deployed, we were able to reduce the number of incorrectly identified fraudulent charges and save our customers time and money.
Stratifying imbalanced data to ensure fair representation of the minority data in all data sets used for cross validation of the model.
Consulting with regulatory and subject matter experts to gain clear understanding of information and variables within data streams.
Utilizing cloud computing resources for model optimization/tuning of hyperparameters, and cross-validation of statistical data science models.
Used R’s dplyr for data manipulation, as well as ggplot2 for data visualization and EDA.
Utilized Scikit-Learn, SciPy, Matplotlib, and Plotly for EDA and data visualization.
Built Artificial Neural Network models to detect anomalies using PyTorch and Scikit-Learn.
Used Scikit-Learn’s model selection framework to preform hyper-parameter tuning using GridSearchCV and RandomizedSearchCV algorithms.
Developed Unsupervised K-Means and Gaussian Mixture Models (GMM) from scratch in NumPy to detect anomalies.
Employed a heterogeneous stacked ensemble of methods for the final decision on what transaction was fraudulent.
Deployed model using a Flask app stored in a Docker container.
Evaluated the performance of our model using a confusion matrix, accuracy, recall, precision, and F1 score. Took careful consideration of the recall score.
Utilized Git for version control on GitHub to collaborate work with the team members.

Data Entry Assistant
Flowers By Michael
Beverly Hills, CA
January 2012 – March 2014

Flowers by Michael is a flower shop started by Michael Jackson, then taken over by Marlon Brandos wife in Beverly Hills. They provide Flower sets, and floral decoration for high-end clients and businesses in the area. I was in charge of basic Data Entry tasks, Excel Sheet creation, and inventory management in coordination with operations manager.
·Integrated files and invoices in the
·Performed basic regresions and extrapolations using python
·Accessed and updated databases using Oracle SQL and DBase

Education
Bachelor of Science in Statistics
University of California Davis, Davis California

"
,
Data Scientist,"
GLEN MACKEY
**** * **** ****, **** Lake City, Utah 84105
385-***-**** adpl85@r.postjobfree.com
* * * * *
DATA SCIENTIST
Q U A L I F I C A T I O N S P R O F I L E
Highly analytical and process-oriented professional, with hands-on experience in data analysis, database and software tools development, data extraction, filtering, and importation process automation, as well as computer programming Growth-driven individual, seeking a challenging position to fully utilize knowledge and skills honed from continuous educational pursuits and work experiences. Capable of utilizing programming languages in creating codes to deliver accessible data to support organizational decision-making and strategic planning. Armed with in-depth knowledge of statistical techniques, along with critical thinking and problem-solving skills in translating research data to formulate and recommend cutting-edge plans and solutions to consistently achieve project goals and objectives. Equipped with solid interpersonal skills in building positive and long-term relationships with all levels of professionals. A R E A S O F E X P E R T I S E
Algorithm Development Data Collection and Processing System Administration Trends Visualization Linear and Non-linear Regression Analysis Team Leadership and Training Database Management Risk Management Financial Market Estimation
E D U C A T I O N
Doctor of Philosophy in Geology, 2019
UNIVERSITY OF UTAH SALT LAKE CITY, UT
Master of Science in Geological Sciences, 2009
THE UNIVERSITY OF TEXAS AT AUSTIN AUSTIN, TX
Geology Department Best Master’s Thesis Presentation Award Bachelor of Arts in Near Eastern Studies, 2003
THE JOHNS HOPKINS UNIVERSITY BALTIMORE, MD
Phi Beta Kappa Distinguished Military Graduate
P R O F E S S I O N A L E X P E R I E N C E
UNIVERSITY OF UTAH SALT LAKE CITY, UT
Graduate Teaching and Research Assistant 2009–2021
Led efforts in increasing lab sample throughput and enabling analysis of non-traditional samples through development and validation of new geochemistry lab techniques
Designed various Excel and Visual Basic-based products to streamline geochemical data importation, filtering, and processing that allowed extensive data distribution among laboratory clients
Capitalized on knowledge of inversion and optimization theory in creating novel data processing algorithms to obtain information from the measured geochemical data as well as to enable the following processes:
Deblurring of low signal-to-noise ratio time series data;
Deconvolution of chromatography data for high-precision isotope ratio measurements; and
Calibration of non-linear mass spectrometer detector behavior without the use of costly calibrated standard
Initiated the preparation and analysis of geological and biological samples for elemental composition and heavy isotope ratios
Demonstrated effectiveness in conceptualizing new chemical, analytical, and mathematical techniques for problem identification and resolution
E A R L I E R CAREER
UNIVERSITY OF TEXAS AT AUSTIN AUSTIN, TX
Graduate Teaching and Research Assistant 2007–2009 UNITED STATES ARMY FORT HOOD, TX
Captain 2003–2007
Battalion Intelligence Officer (S2)
Battery Executive Officer
Field Artillery Platoon Leader
GLEN MACKEY
1552 S 1100 East, Salt Lake City, Utah 84105
385-***-**** adpl85@r.postjobfree.com
2 P a g e
P R O J E C T S
Deblurring Laser
Ablation Mass
Spectrometry Data
Main Objective: Development of a deblurring algorithm for the elimination of measurement-based blurring in spatially-resolved Laser Ablation Inductively Coupled Plasma Mass Spectrometry (LA-ICP- MS) elemental and isotope ratios.
Software Used: MATLAB to solve iterative, regularized linear inversion problem. Improved efficiency of overall program by using an adaptive algorithm to automate model selection for a series of sub-problems based on the relevant selection criteria (Akaike Information Criteria and Unbiased Predictive Risk Estimator).
Strontium (Sr)
Isotope Ratio
Measurement
Automation
Main Objective: The adaptation of a commercially-available automated chromatography sample introduction system to drive automation of Sr isotope analysis on a Thermo Neptune ICP-MS, as well as improvement of sample throughput and data accuracy by processing chromatographic data with a deconvolution algorithm.
Software Used: Excel and Visual Basic to import data while facilitating outlier filtering, iterative mass-bias isotope ratio correction, and data visualization of routine samples. MATLAB for filtering and non-linear inversion of blanks, samples, and standards for non-typical sample sets (High Rubidium and/or large variations in Sr isotope ratio). Mass Spectrometer
Ion Counter
Calibration
Main Objective: Creation of an improved method for the calibration of a dead-time and non- linearity correction for the Thermo Neptune ICP-MS ion counters. Software Used: MATLAB to filter measured data and fitting to an existing theoretical model using non-linear inversion.
Uranium-Thorium
(U-Th) Radiometric
Dating for “Dirty”
Samples
Main Objective: Formulation of a sample preparation approach and computational scheme for the application of U-Th radiometric dating to samples with high detrital Th, along with measurable hydrogenous Th, which were previously undateable.
Software Used: Excel and Visual Basic to import and filter data. Visual Basic to estimate the projected blank correction for continued Th rinse from the mass spectrometer by non-linear regression as well as to solve the transcendental age equation. Uranium-Lead (U-
Pb) Detrital Zircon
Geochronlogy
Main Objective: Establishment of both analytical and computational methods for U-Pb radiometric dating of detrital zircon with high throughput.
Software Used: Visual Basic to solve transcendental age equations and for Monte Carlo simulation of uncertainty for strongly non-normally distributed variables. Automate
Geochemistry Data
Processing
Main Objective: Implementation of Excel and Visual Basic-based products for the automation of geochemical data import, filtering, and processing for a wide range of laboratory clients. Software Used: Visual Basic to batch import and process tens-to-hundreds of data files from each analytical session, as well as solve non-linear data corrections unique to each chemical system. P R O F E S S I O N A L D E V E L O P M E N T
Courses Google
Python Course
Coursera
IBM Data Science Professional Certificate, In Progress United States Army
Jump School Field Artillery Officer Basic Environmental Coordinator Iowa State University
Geology Summer Field Camp
T H E S I S A N D D I S S E R T A T I O N
Mackey, G.N. (2009). Provenance of the South Texas Paleocene-Eocene Wilcox Group, Western Gulf of Mexico Basin: Insights from sandstone modal compositions and detrital zircon geochronology [Master’s thesis, University of Texas at Austin]. Texas ScholarWorks. http://hdl.handle.net/2152/ETD-UT-2009-08-206 Mackey, G.N. (2019). Refining geoanalytical data with inverse theory [Doctoral dissertation, University of Utah]. Digital Library. https://collections.lib.utah.edu/ark:/87278/s6v6wagq P R E S E N T A T I O N S
Mackey, G.N. (2010, April 13). Provenance of the South Texas Paleocene-Eocene Wilcox Group, Western Gulf of Mexico Basin: Insights from sandstone modal compositions and detrital zircon geochronology [Section talk]. American Association of Petroleum Geologists, New Orleans, LA. Mackey, G.N. (2011, December 7). High throughput Sr isotope analysis using an automated column chemistry system
[Poster presentation]. American Geophysical Union, San Francisco, CA. T E C H N I C A L A C U M E N
MATLAB Python Pandas NumPy SciPy Requests LaTeX Statistics Programs (R and Stata) Microsoft Office Suite (Word, Excel, PowerPoint, VBA) Google Sheets

"
,
Data Science,"
DON AGIRO
Data Scientist
Atlanta, GA 402-***-**** adkrv6@r.postjobfree.com

Professional Summary
Detail oriented Data Scientist with 10 years of experience solving complex problems through proven Machine Learning, statistical and project management skills to optimize data and improve quality of business Insights.
●Data science, data mining, SQL queries, data modeling, business analytics, and data visualization, machine learning in Python using TensorFlow.
●Experience using R, Python, and SQL.
●Extensive data science experience in Python (Numpy, Pandas, TensorFlow, Matplotlib) & R tidyverse to find lean and actionable solutions and insights to various real-world business problems.
●Exceptional communication skills able to clearly relay results and solutions to stakeholders.
●Design custom BI reporting dashboards in Python using Dash with Plotly.
●Expert in the transformation of business requirements into statistical data models in Python. Design and build solutions using TensorFlow.
●Build statistical models in Python using TensorFlow and report using BI solutions that scale across massive volumes of structured data and unstructured data.
●Experience in the application of machine learning models (e.g., naïve Bayes, linear regression, deep neural networks, support vector machines (SVM), decision trees, random forest, XGBoost etc.).
●Experience implementing statistical models on big data sets using cloud computing services (e.g., AWS and Azure).
●Strong ability to devise and propose innovative ways to analyze problems by using acquired business acumen, mathematical theories, data models, and statistical analysis.
●Discover patterns in data using algorithms and SQL queries and use an experimental and iterative approach to validate findings in Python using Tensorflow
●Experience working with relational databases using advanced SQL skills.
●In-depth knowledge of statistical procedures that are applied in both supervised and unsupervised machine learning problems.
●Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
●Strong experience in Software Development Life Cycle (SDLC) and in supervising teams of domain specific experts to meet product specifications and benchmarks within the deadlines given.
●Ability to quickly gain an understanding of very niche subject matter domains and design and implement effective novel solutions to be used by other subject matter experts.

Skills
●Python, R, MySQL
●Tensorflow, Keras, Pytorch, Prophet
●Pandas, Numpy, NLTK, SQLAlchemy, Matplotlib, Seaborn
●Linear Regression, Logistic Regression, KNN, Neural Networks, Timeseries,
●Project Coordination, Interpersonal Communication, Emotional Intelligence

Professional Experience

Data Scientist at Southern Company
Atlanta, Georgia Apr 2021 – Present

I am working with a team mandated to analyze the pipe systems of the gas operations level of Southern Company. The main objective of the project is to come up with a data model that alerts to the probability of pipe failure (e.g., puncture) occurring within a predicted time range so the company can optimize preventative maintenance on their pipe systems and address pipes identified for maintenance review prior to the event of an actual puncture.
●Working with stakeholders to evaluate the existing system to determine best algorithm to manage resource demand.
●Assessing the company’s existing Data Analysis and Reporting of pipe damages data.
●Leading cross-functional collaborations between technical vendors and company leadership.
●Working with Data Science team to map out solution for generating real-time streams of relevant pipeline data and creating appropriate Data Sets.
●Working with Data Science team to identify, design, and evaluate third-party tools to build the predictive modelling solution to determine threat levels of pipes within the company’s industrial pipeline operations infrastructure.
●Developing time series models to help with resource management.
●Programming in Python.

Data Scientist at Coca Cola
Atlanta, Georgia Jan 2020 – Apr 2021

At Coca Cola Co, a beverage company, I worked with manufacturing equipment datasets to provide complex data extracts, programming, and analytical modeling. This was performed to support the automation of various routine manufacturing processes by predicting time-to-failure to prevent extended downtime, scheduling appropriate preventative maintenance. Incorporated IoT data for up-to-date predictions. When Covid-19 hit we focused on generating automated system alerts and predictive solutions to increase the reliability of the plants under reduced staff. Some of the tasks included:
●Through survival analysis techniques and machine learning algorithms, we tried to improve how the manufacturing teams could predict part failures.
●The project required use of data mining methods, hypothesis testing, regression analysis and various other statistical analysis and modeling methods.
●Presented weekly updates to managers and key stakeholders to preview the user interface designs and analytical results of stress analysis findings, etc.
●Presented using PowerPoint, Tableau, Excel for data work and charts.
●Participated in Software Development Life Cycle (SDLC) including Requirements Analysis, Design Specification and Testing following Agile methodologies. Operated in 2-week sprints, and weekly stand-ups.
●Worked in Git development environment.
●Responsible for preparation for data for collaboration with machine learning models.
●Used Python to create a semi-automated conversion process to generate raw archive linked data file.
●Provided software training and further education about model applications to incoming team.
●Initial findings reported for conversion of Excel to CSV, text to CSV and image to CSV.
●Collaborated with the computer vision team to better understand how to extract meaning from images and PDF files.
●Used Predictive Modeling, Data Mining Methods, Factor Analysis, ANOVA, Hypothetical Testing, and Normal Distribution.
●The project was implemented with custom APIs in Python and use of visualization tools such as Tableau and ggplot to create dashboards.

Data Scientist at Ameritas Insurance Company
Lincoln, Nebraska Jan 2018 – Dec 2019

Worked with Actuaries to calculate total claims using a time series model. was tasked with working on a time series ARIMA model using Python to determine the value of a claim and append the results to a dataset that was used to make a linear regression model for predicting the sum value of expected claims. Model results were deployed on an AWS EC2 instance and shareholders could get results through a web API capable of fetching real time results as new data was added. Some tasks include:
●Database Administration: Built a MySQL database to house and manipulate the project data
●Python libraries (Prophet, Pandas, Numpy): used Python to explore the data and evaluated if it was fit to conduct a time series analysis.
●Dickeyfuller test: Ran a Dickeyfuller test to determine whether the data being used was stationary
●Data decomposition: did a one- step differencing to make the data more stationary without overfitting
●Statistical model exploration (ARIMA and spectral analysis): ran two different time series models to determine which would give more accurate projections.
●Model Tuning: performed routine checks on the model to make sure anomalies such as structural breaks were adapted to the model and performance was not affected.
●Deployment: used Flask to build an API used to display results on a website for stakeholders to view.

Senior Data Scientist at Humanity and Inclusion
Houston, Texas Dec 2014 – Nov 2017

Tasked with supporting the monitoring and Evaluation officer with collecting data on refugees for analysis. By applying a logistic regression model, the refugee population would be classified based on their immediate need of attention (i.e., need for immigration support from IOM, need for food from WFP, need for medical support from Handicap International etc.). The model was deployed in an AWS EC2 instance, and the resulting analysis was stored in a MySQL Server. Stakeholders accessed the data through
●Database Management: designed and built a MySQL data to store and manipulate the collected data.
●Data Collection: Manually collected the data and entered into the built MySQL database
●Data Cleaning, Imputation, Tokenizing: used Python libraries (Pandas, NLTK, Numpy, Keras) to clean and prepare the data for analysis.
●Statistical model exploration (Naive Bayes Model, Logistic regression): tested multiple models to determine how well the outcome would perform given different techniques.
●Hyperparameter tuning: used Keras HyperModel to conduct some feature engineering and improve the model's performance.
●Model Deployment/implementation: deployed the model on an AWS EC2 instance and built a web API for stakeholders to gain access and view the results.
●Monitoring and Evaluation: performed routine checks on the model and checked its performance against new data and evaluated whether it needed to be fine-tuned or overhauled for a new model.
●Project Coordination: managed a team of two data officers who assisted in collecting the data. Provided direction on how to organize the data and feed it to the MySQL database.
●IT support: provided employees with ICT support (i.e., software installations, hardware maintenance, troubleshooting computer related issues, etc.).

Data Scientist at Saerbeck Municipality
Saerbeck, Germany Jun 2012 – Nov 2014

Worked on a deep learning solution using an LSTM model that regulated power output with demand fluctuation funded by Shell and the European Union. A time series analysis shows recurring peaks during certain hours of the day; however, this does not always satisfy demand when irregular events such as soccer games, farmers market, tourists, etc., suddenly increase demand for power. By accommodating long term and short-term data, the LSTM model predicted demand given current events to provide efficient management and regulation of clean energy
●Database Management: designed and built a MySQL data to store and manipulate the collected data.
●Data Collection: data was collected through a digital IOT device that would then store all the information in the MySQL database.
●Data Cleaning, Normalization, Imputation, Tokenizing: used Python libraries (Pandas, NLTK, Numpy, Tensorflow, Keras) to clean and prepare the data for analysis using models built through Keras.
●Statistical model exploration (ANN, RNN, LSTM): experimented on different statistical models to test how performance fluctuated in different approaches.
●Feature Engineering: used Random search by Keras to conduct some parameter tuning and make sure the model was at its best performance at all times.
●Model Evaluation and Adaptation: did scheduled checks to confirm the model was not drifting with new data. Would tune the model or overhaul it and build a new one if need be.
●Public Relations: worked with the marketing department to inform the general public about the renewable energy project and its financial, economic and environmental benefits to everyone who was a part of the program.

Lead Data Scientist at Centre of Expertise Water Technology
Leeuwarden, Netherlands Aug 2010 – April 2012

Designed an artificial neural network for the local wastewater plant. The ANN model was fed data such as weather, water salinity, pH levels, etc.from the pretreatment phase and analyzed this information to predict whether bacteria and mangroves needed an intervention to keep them efficient and running all year round.
●Statistical model exploration (Logistic regression, ANN): conducted tests on more than one model to evaluate the best solution for this project
●Database management: constructed a MySQL database to define, manipulate and manage the acquired data for this project.
●Data Collection: collected the data using digital meters that directly fed their information to the database. Some of the data was manually collected and entered into the database
●Data Preprocessing: used python libraries (Pandas, NLTK, Numpy, Tensorflow, Keras) to clean and prepare the data for analysis using the ANN model.
●Feature Engineering: used Keras HyperModel tuner to fine tune the model and increase performance.
●Model Deployment: deployed the model using an AWS EC2 instance. Stakeholders could view real time results through a web API
●Project Coordination: managed a team of three undergraduate students who assisted in collecting the data and storing it in the MySQL database.

Education
Master of Science in Environment and Energy
University of Twente
Enschede, Netherlands

Bachelor of Science in Actuarial Science
University of Nebraska Lincoln
Lincoln, Nebraska

Bachelor of Science in Information technology
United States International University
Nairobi, Kenya

Certifications
Google Cloud Platform
●Machine Learning Models: https://google.qwiklabs.com/public_profiles/31d72a3b-5679-4f40-9c77-952a0cadbb08
●Deploy on Kubernetes: https://google.qwiklabs.com/public_profiles/31d72a3b-5679-4f40-9c77-952a0cadbb08
●Explore ML and AI: https://google.qwiklabs.com/public_profiles/31d72a3b-5679-4f40-9c77-952a0cadbb08
●Data insights with BigQuery: https://google.qwiklabs.com/public_profiles/31d72a3b-5679-4f40-9c77-952a0cadbb08

"
,
"Data Scientist, Operations/Marketing Lead","
Kristen Nerantzis
Sandpoint, ID 540-***-**** adpkhd@r.postjobfree.com
EDUCATION
****, ** ** ***********, ***** Madison University
PROFESSIONAL EXPERIENCE
SONORAN RECOVERY LIVING Chief Operations/Marketing Officer Jan 2020 - Dec 2020 Scottsdale, AZ Conceptualized, built, and launched Sonoran Recovery Living (SRL), a recovery housing provider company. Operations Manager and Content Creator for SRL. Owner provided financial backing and high-level input.
> Handled all licensing / certifications; obtained all upon requisite first inspections.
* Extensive town, state, federal, and healthcare-specific licenses were required to open the homes.
> Designed, implemented, and expanded all recovery programming within the homes.
* Drafted company policies; compiled SRL Policy Handbook for licensing and reference.
* Designed SRL offerings (in-house meetings, transportation to treatment, home gyms, drug testing, etc.).
* Determined house rules (support group attendance, sobriety requirements, therapeutic engagement, etc.).
* Kept up with new literature/studies published in the areas of mental-health, addiction, and recovery housing.
> Handled all business development, including all marketing efforts.
* Initiated and managed all partnership outreach and coordination to continuously grow client base.
* Developed a la carte packages tailored to various resident needs (sober coaching, career counseling etc.).
* Developed and ran company website/blog as well as online advertisements (Google, Facebook, etc.).
* Created all marketing content: brochures, informational booklets, one-sheets, business cards, pitch decks, etc.
> Created all formal SRL documents/templates for internal and external use. Included (not limited to):
* Residents’ legal contracts, financial agreements, medical logs, Resident Handbook, licensing forms, etc.
> Hired, onboarded, and managed staff: internal (house managers), and contractors (therapists, drivers, etc.).
* Created requisitions, developed training material, constructed staff reference manual, ran regular meetings.
> Oversaw all day-to-day operations for SRL’s three recovery homes.
* Ensured proper legal, license-specified, and contractually-specified compliance by staff and residents.
> Maintained house and development budgets; worked with managers and SRL owner to optimize costs.
> Regularly updated/strategized with SRL owner on growth goals, financials, and foreseeable roadblocks. BOOZ ALLEN HAMILTON Consultant / Senior Consultant July 2014 - Sept 2017 McLean, VA Began working at Booz Allen as a cloud-cohort intern while in undergrad, joined full-time following graduation. Developer/Data Engineer/ Task Lead for Federal Telecommunications Infrastructure contract.
> Led two of the twelve contractual workflows as Tasks’ Lead and sole developer.
> Wrote all technical portions of winning proposal to extend firm’s ten-year contract when up for renewal.
> Automated extraction/processing of telecom GIS data; eliminated need for $200k/year enterprise software.
> Developed pipeline and web application to collect, process, and visualize external vessel-location data.
* Obtained $250k in funding to continue development after first-iteration client demo.
> Updated processing/analysis of North American internet registry data (Java, cron, Accumulo, MapReduce). Developer/Data Scientist for a Federal Critical Infrastructure (CI) Sector contract.
> Led team in building real-time CI news aggregator/filter and classifier (hLDA, NLP, Python, PostgreSQL).
* Developed web application for analysts to access pertinent CI news in near real-time (Django, JavaScript).
* Designed multivariate regression models for US broadband data and county demographics (R).
* Developed cloud-based unsupervised topic modeling application on CI corpora (Python, Scala, Spark). TECHNICAL SKILLS
Python • Django • data mining • UNIX scripting • various GIS tools • NoSQL work • data science / machine learning / statistical modeling • distributed computing work • basic frontend • data architecting /pipelining

"
,
Argonne National Mechanical Engineer,"
Arman Mikaili
Senior Software Engineer
Davenport Iowa
708-***-****

Objective
To be a leader in technological innovation/modernization, and thus to leverage the excellence of the organization, as a technical direction leader, information technologies/enterprise-software architect, senior consultant, senior full stack software engineer, or other commensurate role.

Experience Summary
More than a generation of experience as a lead developer/architect, in multi-million dollar projects, with Fortune 500 companies and Federal agencies (DOD, DOE, NIH, USDA), including Raytheon, Motorola, Caterpillar and Textron, in vertical domains such as financial (investment banking), and e-Commerce (POS). Another decade+ experience as Mechanical Engineer in the automotive and production industries.

Education
Bachelor of Science (Dual-Major):
Computer Science
Iowa State University, completed all requirements for the CS Major curriculum. December 1998. Major GPA: 3.75/4.00.
Mechanical Engineering
Iowa State University, graduated BSME May 1981. GPA 3.1 (Minor in Mathematics).

Work Status:
US Citizen. DOD Security Clearance, 2014; National Agency Check Inquiries (NACI) Public Trust (USDA), 2017.
TECHNOLOGY EXPERIENCE SUMMARY
Computer Software and Biomedical Data Science
• Expert in Java full stack development; embedded agents, Java Instrumentation, JMX JConsole
• Data Science based on AI, Machine Learning (ML) medical prognosis, ElasticSearch, GraphQL, Knowledge Graphs (KG), Natural Language Programming (NLP): Python, PySpark
• Biomedical informatics, Clinical/Research data platforms, EHR analysis pipelines; Clinical Studies
• Spring MVC Frameworks, Spring Boot; Maven, Gradle, Jython, Spring Cloud, Oracle Coherence clusters
• Mobile App. Dev, JavaScript toolkits, TypeScript, Angular, Dojo, REACT, AJAX/REST protocols
• POSIX-compliant Unix-based, Mac OSX, iOS; C++, real-time software/driver development
• RealTime Operating Systems, Wind-River VxWorks, Tornado, pSOS
• Open Linux: Red Hat, Ubuntu; Qt4.5; GNU devops; Cronjob, admin and process automation, shell scripting
• Perl based web/server development; Catalyst MVC Perl framework
Communication Networks Software Design and Development
• Cloud based API development, Open PAAS Cloud Foundry, AWS API Gateway/Buckets/S3, OpenStack
• Current Expertise in data and voice communication over fault-tolerant elements/networks
• Real-Time software/driver and network protocol algorithm development;
• Network systems design/architecture, Motorola 3G wireless, CDMA, GSM, LTE
• Network Planning/Management applications, SNMP, Mobile Device Management
• Cell or Packet Switched computer networks, ATM, SS7, wireless/wireline telephony protocols
Web, Mobile and Application Server Development
• Service Oriented Architecture, RESTful & SOAP API based Services
• J2EE development and deployment, IBM WebSphere, distributed Message-stream real-time platforms: Apache Kafka/Confluent; JMS, MQ Queues/Topics; virtual clusters
• J2EE; Tomcat Servlet Containers, micro-services, Oracle JDeveloper/WebLogic 12c, Eclipse, IBM WebSphere (WAS), Spring MVC/Boot; Hibernate ORM, EJB3, JSP, Swing, Python/Jython extension language customization; OSF Apache;Tomcat; Virtualization/Container technologies e.g. Docker, Puppet and Agave; JDBC
• XML-based systems configuration, integration, information exchange and data persistence, WSDL; Document Object Model (DOM), Hibernate ORM; JSON, MongoDB, XSLT transformations
• ASP.NET, ADO.NET; Active-X, COM+, DCOM, C#, MS Visual Studio 6.0, C++, MFC
Software CM, Build Stack/Tools and DevOps
• Agile/Scrum development, Product Owner and Scrum Master roles; Jira; Software Development Life Cycle
• UML Component-based software architecture methodology and Component based design; OMG UML2; CMMI; IBM Rational Software’s UML/CM Suite, ClearCase, Telelogic Rhapsody Designer
• Software testing experience includes unit, system, integration level testing, DOD Verification, Validation and Accreditation (VVA), test plans and customer acceptance; QA audits; Test automation and scripting (Perl); Test Driven Development (TDD); Visualization tools for Application performance tuning/monitoring: Splunk, D3
• Software Configuration Management (Build Manager Certified); Telelogic CMSynergy; Clearcase, BitBucket, GitHub; Test Driven Development (TDD): Jenkins/Travis CI/CD automated build-integration-deploy pipelines
Employment History
Professional Staff, Univ. of Iowa, Graduate College (PIA3-GC)Cer
Biomedical/Health Data Scientist
Application Developer 3
Enclave Fellow, NIH-NCATS Grant
Oct 2019 to present
Developing, building, and optimizing, secure, distributed, computational EHR registries, and warehoused bionetwork HPC. Conducting disease mitigation, application software development, and analytics pipelines; to facilitate data-driven, centralized and point of care, Clinical Decision Support (CDS) systems; for medical knowledge-transfer, inferences and prognoses; among scientists, involved in mitigating the formidable COVID-19 crisis.
Fostering the development of analytic toolsets, and resource suites; both on-premise and AWS-EC2 cloud based; for the NIH consortium, of academic medical institutions: cd2h.org. Enabling informed clinical trial designs, therapeutics, care guidelines, and clinical outcome predictions, for novel diseases. Standardized OMOP multimodal, longitudinal, observational data analytics, and use-cases. Curating COVID-19 vaccine and therapeutics trials data, and developing next generation sequencing platforms; to support the development of viral-vector/mRNA vaccines. Providing the community with a secure PHI national database, for harvesting data, and deriving medical decisions, and prognostic support resources: covid.cd2h.org.
Technology stacks include Java8, AWS, Angular, React, NodeJS, Python; GraphQL/SPARQL, knowledge graphs, Elasticsearch, natural language search engines; Google Analytics. Web and metadata crawlers and harvesters. Migration of premise software to the AWS instance: NIH-NCATS.
Under the auspices of NIH Grant awards, to the National COVID Cohort Collaborative (N3C) Academic medical consortium, supporting NIH, and its joint-agency partners CDC/FDA/HHS. Software development: multimodal Analytics resources, for harnessing biomedical phenotype-genotype-pharma data. Build analytic pipelines, aggregate EHR data, with correlated genotypes, and radiographic image data. Deploy and leverage modern COTS and organizational tools, e.g. MDClone, PHOEBE, ATLAS, ATHENA, RxNAV. Implement Clinical Data Management (CDM) and analytics platforms: Palantir Technologies “Foundry” platform, palantir.com.
Data harmonization is provisioned to ensure standardized, verified, PHI protected, patient-level EHR data. Data-driven cluster based, and graph based machine learning. Supporting publications, in the research community’s journals, e.g. Cell, Nature. Frontline clinicians among the target user communities. Enhance the role of data science, and population health informatics, while bridging the gap between clinical care, public health, and academia.
Certification: Human Subjects Protections (HSO), Certified Investigator: https://hso.research.uiowa.edu/certifications-human-subjects-protections-citi Registered @ the Univ. of Iowa, Institutional Research Board (IRB), ORCiD ID: 0000-0001-9779-1512.
Publication:
NIH launches analytics platform to harness nationwide COVID-19 patient data to speed treatments: Journal of the American Medical Informatics Association, Volume 28, Issue 3, March 2021, Pages 427–443: https://academic.oup.com/jamia/article/28/3/427/5893482?guestAccessKey=3855e394-ad3a-4b31-9d9b-36f56b02a858

Prior to the current role:
TD Ameritrade, Oct 2018 to Oct 2019
Full stack development and support of a high throughput, high availability large scale Online Transaction Processing (OLTP) application chain, to include life cycle support and real-time network/app monitoring roles and responsibilities, for TD Ameritrade (now Charles Schwab) next gen, retail and institutional, equities trading, and streaming stock quotes applications. Secure, Scalable platforms based on Oracle (cache) Coherence; Spring MVC/REST and Angular. TDD deployment pipelines, Docker/Puppet containers and performance analysis and tuning using Splunk. Scrum master roles in a full Agile process environment. Expertise in user experience web based front-end development, utilizing the AngularJS and Angular 2+ frameworks.
InfoWeb, May 2018 to Oct 2018
Caterpillar contract: Developed intranet secure web based application, for accessing sensitive (PII data) automotive supply chain information. Role-based access control (RBAC); network/application access, LDAP. Single Sign On (SSO), using “Strong Passwords”, AES256 encryption. Java Secure Socket (JSSE), X509 Certificates, SSL/TLS, HTTPS:// and other security protocols, for applications security.

DOE, Office of Science, Argonne National Lab; April 2017 to April 2018
Computer Env. for Life Sciences (CELS) division: Computational Biology Apps, Web Development.
End to end refactoring of web-based application ModelSEED.org, a co-venture with the Mayo Clinic. Enabled research scientists to build genomics, metabolic models, based on the data curated at Argonne. Angular, Java middleware: Microservices architecture, Openstack cloud, Docker, CI-CD. 508 Accessibility compliance.

Authorship and software dev. credit; in the following publication, et. al:
23-Jan-2018
Manuscript No. TPJ-00087-2018
Title: PlantSEED Enables Automated Annotation and Reconstruction of Plant Primary Metabolism with Improved Compartmentalization and Comparative Consistency
Authors: Seaver, Samuel; Lerma-Ortiz, Claudia; Conrad, Neal; Mikaili, Arman; Hanson, Andrew; Henry, Christopher

Monsanto/Bayer, Ankeny MBT Genetics BioTech Lab; January 2017 to Apri 2017
LIMS/Data Science, Analytics Automation software development department of Monsanto/Bayer molecular breeding technology (MBT) Genomics laboratories. Ported analytics and compute Resource Management (RM) applications to the AWS cloud. Open PAAS Cloud Foundry, AWS API, Apache OSF frameworks, KAFKA, SOLR, streaming real-time, workflow TDD pipelines, Travis CI/CD. Micro-services architecture implementation, configuration and deployment to the PCF cloud, Spring Boot.
US Department of Agriculture Statistical Services USDA NASS On site @ USDA HQ in DC (STG Inc, Contract vehicle); October 2014- January 2017
I was the onsite manager and technical lead, for a team of USDA Genomics Institute, web middleware and mobile client software developers. Translated business scopes and operational requirements into new multi-tiered web and mobile, channels, data pipelines, and solutions, as well as optimizing existing instruments, to enable and empower USDA statisticians and analysts to collect agricultural and genomics data, over the web. Enabling USDA statisticians and other stakeholders to strategize US agricultural data collection, analyze Biological and Supply Chain Management (SCM) data.
Provided rich content, mobile application software expertise, to enable direct real-time field access, for entering and analyzing statistical and biological data, curated by USDA, using various Internet/cellular network connected mobile devices, such as all versions of Apple iPads
Provide technical expertise and oversight, for the implementation phase of SDLC, full cycle software testing, Test Driven Development (TDD), Government acceptance quality assurance plans and test implementation/automation scripting. Expertise in utilizing state of the art, UML compliant, software architecture design and productivity tools, e.g. IBM Rational software suite, with total design and engineering documentation responsibilities. Supporting Software Configuration Management (SCM) and release/deployment teams. GitHub, BitBucket.
Rich Internet Application (RIA) development expertise includes responsiveness (mobile apps, cloud-based deployments…) and UX/optimization. Managed full cycle SDLC projects, based on the Agile process. JIRA. Providing Mobile and web application architecture design and solution strategy expertise.
Accomplishments and Awards: Our NASS Team won the REE/Federal Customer Service Award Program (CSAP) award, for the CAPI computer-aided personal interviewer mobile app, in August 2016: “Selected by the Secretary of Agriculture to receive a USDA Abraham Lincoln Honor Awards”

DOD Contracts Work History
Software Engineer III Textron Systems AAI Test Systems Division Austin, TX; October 2012- September 2014:
Designed, developed and integrated Java and C++ application software, to control Automated Test Equipment (ATE), for the Air Force (B1 avionics). Implemented Computation Resource Management (RM) and ETL pipelines, for curation, transformation and analysis of RF and digital instrumentation test data, across databases with disparate schema/hosts, to enable analysis and inference of test results, as made available via a secure intranet web portal. Transaction based data across Oracle databases, JDBC2 distributed transactions. Utilized Spring MVC/Boot frameworks. ATML test specification transformations, XSLT.
Senior Systems Engineer Raytheon Missile Systems (RMS) Tucson, AZ Nov. 2008-Feb. 2012:

Provided system engineering expertise, software development, product life-cycle and IT support, for a wide spectrum of Test Equipment (TE) form-factors, in a “factory of the future” production environment, with a focus on factory infrastructure software/middleware/network/database development, digital control systems and instrument drivers. Worked in both missile production factory, as well as various performance testing laboratories, for test, simulation and development of avionic systems:

Production factory IT Systems Development and Support/Admin: Designed and developed intranet/server (n-Tier Service Oriented Architecture) software/middleware, and secure web-based, data driven, portal application software. Red Hat Enterprise Linux and JBOSS J2EE Application Server support/admin
Missile Automated Production Test Station Development
Hardware In the Loop Simulation Dept (HWIL)

Consultant; Oracle 360-Commerce POS Software Developer, Starmount July 2012 to Sep. 2012, Infogain Mar. 2012 to July 2012
Lead development of retail store Point of Sale (POS) and multi-channel solutions software. Experience in providing excellence in Software Development, in the Retail/e-Commerce business domain; included providing solutions and enhancements to the POS software, to support multi-channel commerce (Mobile, Web and brick/mortar), and integration thereof, e.g. implementing Mobile POS, and Buy-On-Line-Return-In-Store (BORIS) capability enhancements. This software featured latest technologies in mobile payment and high volume transactions: J2EE and WebSphere Application Server development and deployment, Spring MVC/Boot, Apache open stack, Tomcat, NGINX, RESTful and SOAP based services, payment and other POS transactional data transfer, across Oracle databases, JDBC2 distributed transactions.

Communication Networks
IoT Smart Devices
Engaged in consulting assignments under the following DOD and Private Sector Contracts:
MOBILE DEVICE NETWORKS, AND DEVICE MANAGEMENT
JTRS Subnet Modeling and Analysis, as:
Senior Software Engineer and Lead Analyst:
2004 – 2008; 3 Contract Vehicles, contracts between: Physical Science Laboratory, New Mexico State University Staff, June 2004 to June 2005; and (On site consultant at) US Army TRADOC Analysis Center (TRAC) June 2005 to Dec. 2007
Full Cycle Development and support for the newest analytic combat simulation software, named COMBATXXI (high fidelity, high resolution, stochastic simulation app.), for the US Army Research Laboratory. Enhanced the core-model in Java, Software Driven Radio (SDR) subnet protocols, JTRS, enabling data and voice, over a relay-capable frequency-hopping, self-forming, self-healing network model. Consultant, Program Analyst US Army Depot, Corpus Christi (CCAD), Corpus Christi, TX, Sept 2003 to Jan. 2004; US Navy Chief of Naval Air Training (CNATRA), Corpus Christi Navy Air Station (NAS), June 2002 to Sept 2003

Senior Member Technical Staff, Tellabs, Bolingbrook, IL, Jan. 2000 to May 2001

Software Engineer Motorola Network Solutions Sector, Schaumburg, IL, Jan. 1999- Jan.2000 Developed real-time embedded software for Motorola’s 3G Wireless/Cellular platforms, for broadband communication.

Notable Achievements
Experience in excellence, in performance, in many US Federal, and DOD contract engagements, stretching back to the 1980’s (US Army contract for the M110 Howitzer STS, 1988 to 1994, here in QC). In 2016 I provided presentations at the Library of Congress, to demonstrate the software our team developed, for “USDA-NASS”. Presently working on solving the world’s challenging healthcare problems, infections and pandemics, as a scientific Staff, at the University of Iowa; thanks to NIH grants, for development of software, for medical (clinical studies) use cases.

"
,
Bioinformatics Scientist/ Molecular Biology Laboratory Assistant,"
Feranmi Favour Aboderin
**** ******** ***** *****, *******, GA. 30034-2477 (Open to relocation)
Email: adpiy7@r.postjobfree.com Phone: 404-***-****
EDUCATION
Master of Science, Biology December 2021
Concentration: Molecular Genetics and Biochemistry GPA: 4.20
Georgia State University- Atlanta, GA
Bachelor of Science, Microbiology April 2017
CGPA: 4.54/5.00 (First Class Honours), Best graduating student. Obafemi Awolowo University, Ile-Ife, Osun State, Nigeria RESEARCH EXPERIENCE
Current Projects
Bioinformatic Analysis of Composition, Diversity and Functional Capacity of the Microbiome in the Social Blue-banded Goby (Lythrypnus dalli)
Next generation sequence (NGS) data analysis, QIIME2, R language scripting using libraries ggplot2, tidyverse, vegan, DESeq2 and WCGNA, Python data visualization. Classification of Breast Cancer into Triple Negative and Non-Triple Negative Subtypes Using Machine Learning Approaches
Python programming: scikit-learn, pandas, Matplotlib. Past Projects
Molecular Characterization of Microbes in Goby Fish (from Catalina Island, California), using 16S rRNA gene analysis.
DNA isolation, agarose gel electrophoresis, PCR, TOPO TA Cloning, DNA sequencing (using Sanger sequencing), and bioinformatic analysis of sequence data, using BLAST (from NCBI Database) and MegaX software for sequence alignment. Bioinformatic comparison between the bacterial HU protein and mitochondrial DNA packaging protein Glom based on sequence/structural similarity. A Study on the Susceptibility of Staphylococci Isolated from Wastewater to Chloramphenicol, Ciprofloxacin, Fusidic Acid, Gentamicin and Tetracycline. WORK EXPERIENCE
Supply Chain Management Intern September 2017- March 2018 Addax Petroleum Development Nigeria Limited
During 2017 End of Year inventory exercise, carried out reconciliation of variances- shortages/overages observed, filing of count sheets and reconciliation sheets, uploading of count data to company’s accounting software (Proactis), and participated in drafting of the end of year inventory report.
Executed advanced Excel data analysis and reporting of various supply chain data such as annual spend percentage per contract, annual cost savings plan, Nigerian content spend and item pricing for purchase agreements.
Prepared and handled contractual documents.
Performed analysis of vendor bids optimizing for cost and delivery time. Quality Assurance Laboratory Intern April 2016 – June 2016 Ragolis Waters Limited, Lagos, Nigeria.
Carried out half-hourly testing of the manufactured water products to ensure they met the required microbiological and chemical standards.
Recorded and compiled daily and weekly reports of the microbiological and chemical tests of water source.

TEACHING EXPERIENCE
Graduate Teaching Assistant January 2020- Present
Introduction to Biology I (BIOL 2107) Laboratory, Georgia State University, GA
Taught two laboratory sessions per week, with a minimum of 24 students per session.
Was responsible for directing lab activity and guiding students through experiments which included microscopy and gel electrophoresis.
Graded quizzes, assignments, lab notebooks and practical reports. Laboratory Assistant May 2018- March 2019
Medical Microbiology Department, College of Medicine, Enugu State University of Science and Technology
• Assisted in preparing equipment and materials (culture media and cultures) for students’ weekly practical sessions.
• Assessment and grading of practical reports.
• Taught interns about standard microbiology laboratory practices, and important microbiological techniques such as aseptic technique, culturing and Gram staining.

ADDITIONAL SKILLS
Google Cloud Platform
BigQuery, Kubernetes engine, Compute engine, Dialogflow Computer Programming
Python and R programming
Basic proficiency in FORTRAN and JavaScript programming languages Bioinformatics Software
QIIME 2, MegaX, Clustal Omega, Autodock Vina, ChimeraX, PyMol Computer Literacy
Microsoft Office Suite, Advanced MS Excel skills- Advanced Formulas, VBA & Macros, Conditional Formatting, and Pivot tables & Pivot reporting

AWARDS
Best Graduating Student in the B.Sc. Microbiology Program, 2015/2016 Session MEMBERSHIPS
American Society of Microbiologists
Nigerian Red Cross Society
EXTRACURRICULAR ACTIVITIES
Obafemi Awolowo University NUGA Chess Team
Member, 2017

"
,
Data Science,"
AMIR KAMALI PROFESSIONAL PROFILE
Senior data scientist with over 15 years of IT experience, and 11 years of Data Science and Data Engineering experience. Developing Python solutions for over 9 years with experience with R, Java, JavaScript, SQL and C/C++.
-Experience in the application of machine learning techniques including Naïve Bayes, Linear and Logistic Regression Analysis, Neural Networks, RNN, CNN, Transfer learning, Time-Series analysis, trees and Random Forests.
-Experience in statistical models on, big data sets using cloud/cluster computing assets with Azure, databricks, AWS and GCP.
-Experience with CI/CD technologies Azure Devops, Azure Data Factory, Azure Databricks, MLFlow, Docker, Kubernetes, yaml ADO/ADF pieplines and AWS.
-Strong ability to devise and propose creative and innovative ways to look at problems by using business acumen, data models, statistical analysis, and a practical and direct understanding of the subject matter.
-Highly capable of discovering patterns in data using both algorithms, visual representation, and intuition. Ability to use experimental and iterative approaches to validate findings.
-Advanced statistical and predictive modeling techniques to build, maintain, and improve on real-time decision systems. Recommendations are strengthened with precise analysis, a real-world intuition, and an adogmatic approach to modeling techniques.
-In-depth knowledge of statistical procedures that are applied in both Supervised and Unsupervised machine learning problems.
-Ability to perform exploratory analysis on varying types of data and datasets, allowing for a full knowledge of the subject matter, a nuanced understanding of the variables in question, and a technically sound insight into the required modeling approach.
-Strong background of working with advanced analytical teams to design, build, validate and refresh data models.
-Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
-Highly perceptive with other people, allowing for a strong ability to facilitate a dynamic and constructive team environment.
-Ability to quickly gain a keen understanding of niche subject matter domains via research and communication with all parties involved. Ability to design and implement effective novel solutions to be used by other subject matter experts.

TECHNICAL SKILLS
Analytic Development: Python, R, SQL, Excel
Python Packages: Numpy, Pandas, scikit-learn, TensorFlow, PyTorch, SciPy, Matplotlib, Seaborn
IDE: Databricks notebook, Jupyter, Spyder, RStudio, Google Colab, MySQL, Oracle
ML Ops: Azure Devops, Azure Data Factory, Azure databricks, CI/CD, yaml pipelines, MLFlow, Kubernetes, Docker, AWS, GCP
Version Control: Git, GitHub, Jira
Machine Learning: Natural Language Processing & Understanding, Machine Learning algorithms including text recognition, image classification, and forecasting.
Data Query: Azure, Google, SQL, data warehouse, data lake and various SQL databases and data warehouses.
Deep Learning: Machine Perception, Data Mining, Machine Learning algorithms, Neural Networks, RNN, CNN, Transfer learning, TensorFlow, Keras. PyTorch
Artificial Intelligence: Text Understanding, Classification, Pattern Recognition, Recommendation Systems, Targeting Systems, Ranking Systems, and Time Series.
Analysis Methods: Advanced Data Modeling, Statistical, Exploratory, Bayesian Analysis, Inference, Regression Analysis, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Design and Analysis of Experiments, Factorial Design and Response Surface Methodologies, Optimization, and State-Space Analysis.
Analysis Techniques: Classification and Regression Trees (CART), Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, RNN including LSTM, CNN, Transfer learning, Linear and Logistic Regression, Naïve Bayes, Simplex, Markov Models, and Jackson Networks.
Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series Analysis
Applied Data Science: Natural Language Processing, Machine Learning, Text Recognition, Image Classification, Social Analytics, Predictive Maintenance
Soft Skills: Excellent communication and presentation skills; ability to work well with stakeholders to discern needs accurately and articulate issues clearly; leadership; mentoring; coaching.


PROFESSIONAL EXPERIENCE
Humana
Senior AI Engineer Remote (5/21 – Present)
Project involved making models made by data scientists ready for large scale use and creating templates for data scientists to make sure their models followed general guidelines. There were three Azure DevOps (ADO) pipelines and one Azure Data Factory (ADF) pipeline. The ADO pipelines oversaw deploying, code, model, and model files. These pipelines were
executed by Azure DevOps UI using Yaml files that defined what each pipeline did. The ADF pipeline ran the corresponding code and made the model available to the public. The models were executed in two separate environments: staging and prod. Staging was used to examine models and modify them before making it accessible to the public. When a model was moved to prod, it was accessible to the public and could no longer be changed.
oWorked on a development team that conducted work in alignment with an Agile project methodology that included the practices of Scrums and Sprints. Time was split into two-week periods called Sprints, and each team member was assigned a few stories per Sprint.
oSplit the whole project into main and broad sub-tasks called Epics. Each Epic
consisted of small tasks called Stories so that a Story could not be split
further.
oCreated multiple pipelines per model using different Yaml files.
oCreated templates for NLP models as well as other ML models.
oSet up triggers for model monitoring and other related pipelines.
oEnhanced the performance of models by refactoring code using Python and PySpark.
oDeployed Data Science models using Azure Devops, Azure Data Factory,
Azure Databricks, and MLFlow.
oUtilized Yaml file CI/CD to implement CI/CD processes for different statistical
models.
oRegistered models, created computer vision and NLP models, and created a
centralized and unified mechanism for executing Yaml pipelines.
oUsed Jira tool for automated project task tracking.
oUtilized Spark-NLP and JohnSnowLabs libraries for NLP models.
oParticipated in defining goals and mapping a path to achieve them for bi-weekly, quarterly, and yearly intervals.
Form.com
Data Scientist Remote (11/20 – 5/21)
Computer vision was the main focus of mine during my time at Form.com. I was working on a tool developed by CNN to identify different types of beverages (both boxes and bottles) in a picture taken of the racks or coolers in shops or supermarkets. My duty was to use a version of the model trained with real data and test the model with synthetic data generated by graphic artists. The model consisted of two steps: detection, and identification. In first step, a CNN detected all bottle and box shapes in each image by drawing a bounding box for each. In the second step, another CNN determined what type of drink or beverage each of those bounding boxes represented. My focus was on the second part. The technologies I used were MLFlow, Kubernetes, Python, among others. A top-n accuracy measure was used to evaluate the performance of the model.
oMaintained different models using Docker, MLFlow, and Kubernetes.
oManaged historical data from different sources in GCP, making sure the data met the need of the current models.
oCoordinated for generation of synthetic data for future model executions.
oFed different mixture of original and synthetic images to models.
oMeasured performance of models and decided on next steps based on performance measure results.
oParticipated in group meetings with other stakeholders to discuss the results and choose the best approach for future stages.
Micware North America
Principal Data Scientist, ML Ops Engineer Torrance, CA, USA (9/19 – 9/20)
Sales prediction using time series analysis: I developed a tool for predicting sales of a company based on its previous sales records. I used time series analysis techniques with Tensorflow and Keras. The goal was to provide the company analysis, insight and suggestions for the future. Since time series analysis can be easily applied to different use cases, this model can be used in many other enterprises. Data was scraped from online sources using SQL queries among other tools. Python’s Statsmodels package and Arima model were used in this project. The franchises were analyzed both individually and in groups. The model successfully identified branches that are doing well as well as those that are not performing as expected:
oData scraping and preprocessing.
oClassification of the branches based on their size.
oTrained and tested a time series model to forecast the future sales of each group.
oIdentified branches that perform the best and the worst among all branches.
oUsed Statsmodels to decompose the time series into trend, seasonality, and
residual data.
oUsed Dicky-Fuller test to prove that the residual has stationary data.
oUsed the Arima model to model the stationary data.
oTrained and tested a time series model to forecast the future sales of the individual branches.
oModelled the trend and seasonality and stationery data and combined them to provide the forecast for future.
oProvided insight and suggestions to the managerial staff for the future.
oDeveloped and delivered an Android radio application for Honda Motor Co using Android, Android SDK, Java, C++, XML, JSON, Gradle, eclipse, GIT, SourceTree.
oResponsible for planning, monitoring and execution of deployment and product releases.
oParticipated in status meetings, progress report to track progress, risk
management, defect triage, defect tracking and resolution.
Screen Engine/ASI
Senior Data Scientist Century City, CA, USA (11/16 – 8/19)
Screen Engine/ASI is a research and data analytics firm focused on maximizing market potential while assessing risk for clients in the entertainment and media industries.
Applying Sentiment Analysis using NLP techniques, I developed a sentiment analysis tool to classify quotes taken from scripts as they appeared on social media. Once quotes and verbal memes were identified as belonging to one property, further analysis can be executed. Among its potential applications is to build a tool to filter Twitter feeds and send the related tweets to the target audience group (followers, fans, etc.). The contents of the input documents were analyzed using Google’s universal sentence encoder to embed the textual data into the appropriate vector space. Three competing models were trained using Gaussian classifiers, ANN, and LSTM to choose the best performance. LSTM was chosen as it yielded the best performance.
oScraped data using API provided on the web.
oCleaned and balanced data using Pandas and Numpy.
oEmbedded test using Google’s universal sentence encoder.
oUsed PyTorch, Tensorflow, and Keras to build an LSTM architecture.
oUsed Bag of Words, NLTK, and TF-IDF to analyze text.
oImplemented embedding using Word2Vec, Glove, and Doc2Vec.
oUsed Elmo, Bert, and ANN Classifier for sentiment analysis
oDelivered multiple projects in Java, JDBC, SQL, and MySQL for intelligent applications.
oUsed JavaScript, jQuery, Ajax, Google Maps API and fusion tables, HTML, and CSS to deploy online tools for teaching and taking exams.
oProvided detailed estimates for development efforts as needed. Conducted customer requirement gathering, functional/technical specifications, release schedule, and project reporting with direct client interaction.
oDesigned integrated SDLC processes and made use of project management methodologies and continuous improvement and quality standards to deliver a quality product solution.

Indium Analytics
Founder, Data Scientist Los Angeles, CA USA (9/14 – 11/16)
In this start-up, I developed a machine-vision facility for the improvement of closed-circuit camera networks. The project consisted of a network path prediction component using linear programing and forecasting techniques, as well as a computer vision system using vectorized features. The system utilized a facial embedding algorithm that compared favorably with Convolutional Neural Networks at the time, but had the advantage of not requiring retraining to identify new users.
oDesigned Vectorizing function to embed facial features.
oCreated specialized algorithm to store and compare vectorized features and verifications.
oImplemented Convolutional Neural Networks using PyTorch and Python.
oPerformed data cleaning on images and tabular data.
oPerformed image augmentation techniques to introduce rotational, motion, and scale invariance.
oDesigned Statistical evaluation techniques to test the model performance.
oDeployed using Flask and Pickle.
Cybula
Data Scientist/Research Associate York, UK (9/09 – 8/14)
Cybula is a research and tech company created off the department of Compute Science focused on AI and ML. I had a variety of tasks in this role ranging from research to data analysis and making statistical models.
oAnalyzed and designed new database management systems to meet the constantly changing needs of the company.
oImproved customer service by deploying database management tools using SQL.
oProvided business analysis and insight using complex SQL queries and MS Excel reports.
oPlanned, developed, and managed the end-to-end schedule and work with stakeholders toward meeting the release success.
oEnsured continued improvement in product development and process improvement to boost efficiency and standardization.
oLed a team of developers/analysts to perform a large data cleansing and ETL procedure to feed data to a new database system.
oGuaranteed data consistency and integrity by rigorous oversight and cross-checking of random samples from the generated data.
oHeld periodic meetings with inter-departmental operatives to ensure the smooth transition from the old services to the new one.
oSupervised the training of the end-users for operating the software.
oSecured the availability of services by maintaining the company’s online Oracle database.

EDUCATION
University of York, York, United Kingdom
PhD, Computer Science
Thesis title: Ball Computer; a wireless 3D grid for a parallel platform.
•Implementing a simulation and data visualization toolkit using Java.
•Proposing a new 3D hexagonal wireless grid for massively parallel systems.
•Publishing and presenting 3 papers in international conferences and journals.
Shiraz University, Shiraz, Iran
MSc, Computer Engineering (AI and Robotics)
•Implementing an AI software in C++ and Matlab to play a simulated soccer match.
•Used supervised and unsupervised learning, reinforcement learning, statistical models and artificial neural networks among other AI and ML techniques to analyze complex data sets.
Amirkabir University of Technology, Tehran, Iran
BSc, Computer Engineering

PUBLICATIONS
1)A. M. Kamali, C. Crispin Bailey, J. Austin; “On Advantages and Limitations of 3D wireless Grids as Parallel Platforms”; International Conference on Selected Topics in Mobile and Wireless Networking, MoWNeT’2013; August 19th-21st 2013; Montreal, QC, Canada; pp 48-55.
2)A. M. Kamali, C. Crispin Bailey, J. Austin; “Evaluating 3D wireless Grids as Parallel Platforms”, The Special Issue on Dynamic and Mobility handling in Mobile & Wireless Networking, International Journal of Ad Hoc and Ubiquitous Computing, July 2015, pp: 279-289.
3)A. M. Kamali, C. Crispin Bailey, J. Austin; “Performance Analysis of a 3D Wireless Massively Parallel Computer”, Special Issue of Energy Efficiency; Journal of Sensor and Actuator Network (JSAN); 7(2): 18, April 2018.

"
,
Accounting Administrator Data Scientist,"
Arina S. Faulkner
760-***-****
adpc7c@r.postjobfree.com

Education
University of California San Diego
Major: Economics. Minor: Accounting
GPA: 3.6
MiraCosta Community College, Oceanside CA
Associate Degree in Liberal Arts: Mathematics and Science
Associate Degree in Business Administration: Accounting
GPA: 3.9
Eligible for CPA exam
Experience
March 2020 – September 2021
EARNON.SOCIAL, Los Angeles CA – Data Scientist II (https://earnon.social)

●Designed and developed real time recommendation engine to rank sales leads for upsell opportunities
●Refined personalization algorithms for 50K+ customers on web and mobile
●Used Python to scrape, clean, and analyze large datasets
●Led 3-person team on a data mining project to predict sales in the retail domain
●Coordinated with front and back-end software developers to implement predictive models and monitor outcomes
●Used Google Analytics databases to drive optimization and improvement of product development, marketing techniques and business strategies
●Assessed the effectiveness and accuracy of new data sources and data gathering techniques
●Developed company A/B testing framework and test model quality through scenario analysis and simulation

May 2019 – March 2020
Likefinity, Los Angeles CA –Data Scientist I (https://likefinity.com)
●Helped build tools for detecting botnets with machine learning and data mining
●Developed an algorithm in R that automated financial forecasting
●Increased customer response rates with predictive models in R
●Used random forest algorithm to help identify loyal customers and predict the likelihood of customers buying a recommended influencer product
●Utilized accuracy, precision, and recall techniques to monitor and analyze performance of ML models

September 2019 – August 2020
AMN Healthcare Inc., Del Mar CA – Accounts Payable Specialist
•Performed invoice and payables processes that included data entry into Microsoft Dynamics GP and PeopleSoft systems, general ledger coding, proofing, and voiding vendor payments
•Processed corporate invoices over 500K on a weekly basis
•Administered accounts payable aging reports and pre-paid account reconciliations
•Responded to payment inquiries in a timely manner utilizing customer focus techniques and clear communication in order to educate on process and resolve issues
•Executed month-end close which included recording expenses and revenue in the proper month, journal entry preparation and reporting
•Created and maintained desktop procedures to meet on going needs of the organization
August 2018 – May 2019
Airefco, Inc., Tualatin OR – Co-Op/Accounting Administrator
Worked in FlexGen system to run databases, bill, and credit customers
Administered marketing programs such as rebates and seasonal promotions
Processed dealer claims and bills for payments
Researched, tracked and resolved accounting discrepancies
Prepared general ledger postings and statements
Worked with Senior Accountant and CFO on reconciling financial reports
Populated Excel spreadsheets for regulatory financial audits
January 2016 - May 2018
MiraCosta Community College, Oceanside CA - Math tutor in Math Learning Center

●Performed a wide range of mathematical procedures that included elementary algebra, DE, and Linear algebra
●Delivered instructions to individual or small groups of students to improve academic performance, and occupational skills.
●Introduced students to study skills, note-taking skills, and test-taking strategies.
●Participated in training and development sessions to learn and implement new tutoring practices.

Additional skills
Proficient in Microsoft applications: Word, Excel, Outlook, PowerPoint, and Dynamics GP. Well-versed in mathematical operations. Customer service experience present. Strong understanding of GAAP. Intermediate QuickBooks Desktop knowledge. Typing 55 wpm speed. Python, C++, Java, R, and STATA programming knowledge present. Time series forecasting. Fluency in Russian.

"
,
Data Scientist Committee Member,"
SungHoon Yoon

Main: +1-646-***-**** adpahb@r.postjobfree.com

**/****-******* ******.***: Newtown, PA Sr. Fintech & Data Scientist
●Lease senior Data Scientist Cannabis & Medical Data with Glue, Pyspark, Spark Scala, & Delta Lake
●Linked Petabytes of Personally Identifiable Information (PII) of the POS Customers via Datavant with Consumer Characteristic Data, Medical Diagnosis, Medical Claims + RX for insight & analysis for our own internal Forian Data Factory in AWS
●Created Cannalytics 2.0 dashboard application to provide dashboards for retailers, growers, & state government regulatory agencies using NLP, various Encoders (OHE, Ordinal, Hash, Binary, Target, etc.), Machine Learning, & Artificial Intelligence models
●Using Splink to create a Person Master via probabilistic record linkage & deduplication

4/2019- 10/2020 Neon Partners: NYC Sr. Fintech & Data Scientist Advisor
●Private Investment Advisor of Fintech firms technology merit & Market Analysis
●Gathering relevant data for analysis & report findings to the partners
●Advised & worked closely with AWS deployment of S3, Boto3, Glue, Crawler, Glue, Glue Endpoint, VPC access, Athena, EC2, PySpark, & Sagemaker
●Wrote ETL vs XML files & automated model execution in Python using Glue in pyscript & pyspark
●Improved processing algorithm from 24+ hours to less than 2 hours
●Implemented PCA (Principal Component Analysis) model and Multi Factor Non-Linear (piece wise) models to improve classification & predictability.

4/2018- 4/2019 Arthena: NYC Lead Data Scientist
●Ensemble Modeling (SVM (Support Vector Machine) & Boosted Tree) & Timeseries Backtesting strategies of Art Pieces for Bidding & Portfolio construction based on historical data implemented on with Docker within Kubernetes cluster & auto-checked by CircleCi
●Enhancement of existing models using facilitated Python platform (H2O)
●Visualization & Modeling in Jupyterlab & Plotly w/ Python Packages (scikit, scipy, numpy, spaCy, etc.) in pyenv & Pipfile environment
●ETL via Python w/ heavy usage of re, pandas, tqdm, peewee, sqlalchemy, jinja2, fuzzywuzzy, pyodbc, etc.
●Proposed Artwork index based on genre, style, medium, auction results, counts, equivalent works, etc...
●Guaranteed product development for the bidding as a 3rd party insurance provider

02/2018- 5/2018 Qraft: Seoul External Project Lead
●As an external lead, analyzed the first Artificial-Intelligence ETFs (Qraft AI-Enhanced U.S. Large-Cap & Qraft AI-Enhanced U.S. Large Cap Momentum) for potential investment & utilization of intellectual property by 2nd largest Chinese asset management firm by AUM
●Extensive backtesting & performance attribution of GAN (Generative Adversarial Network) model in Python with Scikit, Plotly, & Pandas
●Invited by Harvest Group’s Chairman to present project findings to the management
●QRAFT & AMOM were listed on NYSE as of July 2019

11/2017- 02/2018 Citigroup: NY Sr. Data Scientist / Contract
●External project-specific consultant for the Repricing of Bank Transactional Clients by ingesting accounting receivable & payable records using Python & Conda
●Supported AdHoc requests for data analysis & created visual reports support quality checks & transaction pricing strategy sanity, which aided Financial Planning in pricing configuration & structures changes
●Gathered requirements for the Model & applied Machine Learning & Visualization techniques in Python w/ Matplotlib, numpy, pandas, etc.. in Xhost & SSL connect to Redhat Linux analysis cluster w/ PySpark

05/2017-08/2017 Securities & Exchange Commission Subject Matter Expert / Contract
●Regulatory & Compliance analysis of Registrants using Python w/ SEC’s NEAT in Jupyter Notebook
●Working closely with legal, analysis to spot regulatory violations such as insider trading, Rule 105 violations, front-running, churning, excessive trading, & deliberate index ex/inclusion during rebalancing

11/2016-5/2017 Bank of America: NYC Sr. Data Scientist / Contract
●Analyzed unstructured log Data with Python, utilizing regex, Levenshtein, Quartz, & Sandra (BoA internal platform) to improve trade/settlement flow, & adhere to emerging accounting & regulatory standards
●Contributed to Muni trading & accounting operation system to enhance the quality of calculations, add new instruments/products, & migrate from legacy to Python-based Quartz platform

01/2013 – 06/2016 Fort Inno: Seoul StartUp Business Development Head (Sr VP)
●Setup local Dynamic hedging Desk for Variable Annuity for Life Insurance Industry, which required a full trading & execution cycle for derivative & ETF products for Asset Liability Risk management
●Proposed benchmark index for the comparative framework of the underlying rainbow asset portfolio choices available to VA products with an Asian funding mechanism
●Integral part of Structured Product based Variable Annuity (spVA) solutions – functional & Data science requirement processing via Python for legacy databases
●Big Data project lead: worked with Hadoop w/ Anaconda (Python) to feed the calculation of VA solutions
●Market Analysis projects in Spark, Scala, & Python for the immigrants' insurance product demand pattern
●Structured products sales & consultation for Asset Managers & Insurance Industry lead
●Successfully completed a joint venture with the main competitor & engaged in potential VC funding
10/2011 – 08/2012 Accenture: Seoul Senior Manager
●Sales & implementation lead for newly initiating OTC derivative Central Counterparty (CCP), Alternative Investment, and Prime Brokerage (PB) adoption in Korea
●Engaged in one RFI & four RFPs, working with 14 consultants, and won a new project worth $2m USD
●Developed vertical alliances with vendors (data & systems) & developers for initiating local PB industry
●Led Alternative Investment ($30 billion Alternative Investment AUM) reporting & control system for Korea National Pension Service & responsible for winning an additional $1.2m USD in new business. Managed seven consultants over four months working with Credit Suisse Asset Management
●Prototyped alternative portfolio mockup in Anaconda Python with Pandas & Scikit

01/2009 – 03/2011 Samsung SDS: Seoul Principal Consultant / Contract
●Thought Leadership Center (TLC) financial team leader for Samsung Asset Management (SAM) & Samsung Securities Inc. (SSI) affiliates. Managed four consultants. TLC mission was to advise future directions to CEO, CFO, CIO, & COO within the Samsung Group
●Worked with SAM’s Quant Team on new ETF developments & index replication projects. Currently, SAM ETF product line commands 60% of the local market
●Consulted for Samsung Securities, Life, and Asset Management on International expansion, Darkpool, Algorithmic Trading, Private Banking, Alternative Investment, Hedge Funds, FICC, etc ...
●Allianced and benchmarked cloud operators, GPU, fund admins, & multilateral trading facility (MTF)
●Engaged in SSI’s ‘Long Term IT Innovation’ task force to win $21m USD project for SDS

07/2003-01/2009 IQ Strategy Inc.: NY & London Quantitative Consultant
●Acted as Benchmarking & Strategy consultant for Samsung Securities on International expansion, Darkpool, Algorithmic Trading, Private Banking, and 130/30 business, Wrap account, and Private Banking management. (Samsung introduced Wrap account in 2008 & currently has $3b USD AUM, and is the #1 in Korea. Also, ranks #1 in High Net Worth Clients in Korea according to Merrill’s Financial Advisors)
●Contributed to a Russell Index strategy that returned over 7% in a month ($14m USD in profit).
●2004: Consulted for No. 1 ranked Derivative & Quantitative Research Team (Credit Suisse) - revamped Russell Index reconstitution efforts, and developed industry model. Predicted Russell index changes were ranked as best among the brokers by independent client research in 2004 (by Barra E3 tracking-error).
●Led development effort (seven developers) of client’s pre and post-trade analytics for London Bank (Dresdener’s €11m algo project) with Impact Cost Analysis, implementation shortfall, enhancement (via creating new metrics of performance off) VWAP, TWAP, sniper, etc
●Acted as a product specialist for an Algorithmic Trading EMS company (Aegis) to promote competitive advantage, product alignment, encapsulation of trading business rules and data feed logical structures.
●Acted as sales & product specialist for a Risk Management company for the alternative investment industry
●2004/11-2005/7: Consulted with leading algorithmic trading operation to advise on improving algorithm development (VWAP, TWAP, volume participation), transaction cost analysis, implementation shortfall, S&P index tracking, and portfolio optimization using S+ and Python (Vie Capital) w/ numeric package

12/2001 –06/2003 Knight Securities, L.P. Head of Quant Analytics: Program Trading
●Authored Russell index rebalance strategies for client consumption & utilized them for prop trading
●Developed Pre-trade and Post-trade analytics with Impact, Volatility, slippage, shortfall using Python & VB
●In charge of Blind-bidding and agency trading book with an average AUM of $60m
●Implemented rule-based decimalization trading and market-making strategies based on a quantitative long/short model by working closely with quant model developers and IT team members written in C++.
●Directed seven IT and three quants staff for program trading systems implementation and regulatory review
●Sat on Firm’s 401K investment committee

03/2000 –11/2001 Bear Stearns Inc.,: New York Principal & Proprietary Trading Trader/Quant (VP)
●Traded successful Russell rebalance strategy (33% in & 14% out returns for long/short book)
●Evaluated and critiqued on risk & performance analysis of the firm’s proprietary trading strategies (earnings, reversal, and pairs trading) effects by developing GUI based backtesting & monitoring apps
●Published study of year 2000 Exchange Decimalization effect & 1st tax efficient portfolio reconstitution
●Conducted regulatory reviews on the firm’s principal, proprietary, and agency trading & monitoring
●Managed and developed algorithms for customer facilitated Guaranteed Market on Close trades with IT
●Published Research on Russell Rebalance to clients & facilitated guaranteed close & agency trades.
●Quantitative Analysis of S&P growth & value index constituents additions & deletions
●Published reports on Russell Rebalance, ETF strategy, Market Microstructure, Industry & Sector relative value, etc… based on research done in Python
●Committee member of Trading GUI and implementation of the trading procedure of new Bear Box (EMS)

03/97 –3/2000 Merrill Lynch Inc.: New York Proprietary Program Trading Desk Quantitative VP
●Contributed to Merril’s S&P Sector SPDRs initial launch as a quant by conducting analysis & working with SEC. Later continued to serve as a primary quant for the sales & trading desk.
●Ran a profitable statistical arbitrage book based on the Short Term Residual Reversal model (17% return), which is a multi-variable regression model taking account for multi-collinearity and Heteroskedasticity
●Performed return contribution & risk analysis for factors like section, industry, shocks, earnings, volatility
●Developed multi-threaded GUI based trade monitoring application in VB with real-time data feeds from Reuters, position updates, graphical and tabular interactions.
●Implemented non-linear Impact Cost model and Opportunity Cost model for Risk Blind Bidding. The impact Cost model was built on tick-by-tick data downloaded from C-API to Bloomberg and matching it to executions done by traders, which calculated of actual Impact Cost using Splus & Python
●Formulated real-time Volume Weight Average Price (VWAP) and interval VWAP mechanism for usage in client trading (overseas) and DOT strategy execution (Dot system captures institutional clients flow)
●Developed on real-time trading model based on bid/ask size & spread, past transaction/impact cost, historical patterns, industry/sector momentum.
●Developed automated VWAP execution strategy by looking at non-synchronous trading, the curvature of volume, interval risk, impact cost, binomial price volatility, real-time monitoring using Splus and Python
●Developed dividend projection model for usage in EFP, Basis trade and index arbitrage: the model accounted for future increases, jumps, manual intervention, special dividend using Python
●Ran & compared simulated model vs. actual portfolios based on technical analysis, trading execution studies, earnings studies, industry/sector rotation performance
●Managed two quants and several IT staff on the distribution of client pre and post-trade analytics applications written in VB, which included heat maps, RT updated graphs, user interactive tables, which was distributed to 30+ client sites.

09/93 -03/97 Salomon Brothers Inc.: New York
Derivative and Equity Portfolio Analysis Quantitative Analyst
●Co-developed Portwatch Product in Gauss: A stock selection System based on 400 different Style, Technical, Options, Earnings, Value, Growth, Risk, Insider, Institutional, and Industry indicators screened for trading candidate identification. Used to identify technical momentum added option writing/buying strategies for the trading desk. Alerted changes in earnings and value effect strategies for institutional investors on a daily basis, for example. Portwatch was used by nearly 300 clients.
●Developed HOLMES product in Gauss: (Heuristic Outlier Model Estimation for Stocks) portfolio investment style and risk analyzer. Used by institutional investors to check their own portfolio against prospectus benchmarks defined in the prospectus for deviations via t-test, Wilcox.test, Cook’s distance. HOLMES was used by quantitative salespeople to pinpoint risk in client portfolio and also to select optimal Index replication portfolio, focus on sales strategy
●Co-developed Salomon's U.S. & international Predicted Earnings Model, which identified earning surprise reaction, price & earnings momentum, and fundamental indicators. Familiar with International Accounting Standard and GAAP adjustments, since the definition of earnings differs significantly due to the different accounting standards around the world. Worked extensively with First Call and IBES data
●Oversaw production of Daily Index Derivative Reports: Reports contained daily Implied rate, Mispricing, Fair Value of Index Futures, & Implied Volatility of Options on major international financial markets
●Wrote and maintained Index Futures and Options Pricing model.
●Worked with index group on enhancing the construction of index & accuracy of corporate actions
●Constructed Black Scholes and Binomial Model with American exercise and discrete dividends to calculate implied volatility and spread trading strategy with cost/benefit analysis
●Programmed sanity checks, ensure data integrity, and wrote conversion C to postscript for publishing
●Contributed to monthly publication (“Quantitative Equity Strategy”)
●Modeled Liquidity Risk for U.S. and international markets using historical and real-time data.

EDUCATION:
●Masters level classes, NYU, New York NY (Econometrics I & II. Intl Accounting. Risk Management)
●BA, University of California at Berkeley, Economics w/ Business Minor {3.6 Major & 3.9 Minor GPA}
COMPUTER SKILLS:
BigData: Spark 3.0, (PySpark, MlLib, & Graphix), Hadoop, Databricks, Tensor Flow, Keras
EMR, S3, Boto, Glue, Crawler, PySpark in Glue, VPC access, Athena, EC2, Sagemaker, Delta Lakes

Languages: Python, S+/R, Visual Basic, C, & Scala
Github, Google Cloud, Kubernetes, Docker, CircleCI
Ubuntu, neoVim, pudb, sed, awk, MS Office, Postgres SQL, SQL Server,
Quick Books API, MQA, Bloomberg API

"
,
Data Analyst Illinois University,"
Danny Evans

Phone: 618-***-****
ado86p@r.postjobfree.com
*** ***** **** *****
Godfrey, IL 62035

Education

MS University of Florida, Pharmaceutical Chemistry Aug 2013

MBA Missouri Baptist University Jan 2009

BA Southern Illinois University Edwardsville, Biochemistry Aug 2006

AS Lewis & Clark Community College, Science May 2005

Experience
Self-Insured Reporting, Greenville, SC Jan 2021 to Nov 2021
Data Analyst / Programmer
Automated data loading for entire organization, saving countless man hours weekly
oPerl, Javascript
Processing weekly data via Excel template
oAlso replaced this template taking hours to process with script taking seconds
Perl
Provide point of contact support for vendors
Data cleansing, manipulation, and alignment for upload to current database
Ensuring data integrity and compliance with HIPPA
Other ad hoc requests as assigned

Spectrum, Chesterfield, MO Sept 2020 to Dec 2020
Data Analyst
Create and maintain queries impacting customer service, SQL
Ensuring data integrity
Provide point of contact support for internal customers
Other ad hoc assignments as assigned

Bayer, Creve Coeur, MO Nov 2019 to June 2020
Data Steward
Resident domain expert in data for one or more functional areas including regional data sources and business processes, and enterprise data assets and products. Mostly with Teradata or PostgreSQL
Accountable for data quality, definition, production, and usage of Enterprise data assets and their compliance with cGMP, FDA regs
Profile data to identify gaps/problems and recommend viable options to resolve.

University of Missouri St. Louis, St. Louis, MO Jan 2017 to Nov 2019
Data Analyst/Systems Integration Specialist
Create and run queries as necessary for project deliverables (SQL Server)
Evaluate data pulled for accuracy and relation to data requests (MS Excel, Tableau.)
Develop, update, and maintain current SQL queries, stored procedures, etc. (SQL Server)
Generate reports, graphics, dashboards, etc. (Excel, Tableau)
Integration of incoming data to existing claims data and other data. (SQL, Perl, Tableau, etc.)
Web site design (DNN, PHP, Windows Server)
Business Requirement Gathering
Data Warehousing design
Database development
Data mining, mapping, integrity, and integration
Business Intellegence Design
Data cleansing
Use of ICD-9 and ICD-10 diagnosis and procedure codes
Developing queries to align claims with HEDIS definitions

Sigma-Aldrich (n.k.a. Millipore-Sigma), St. Louis, MO Aug 2008 to Aug 2012
Bioinformatics Scientist
Web site design
Shell scripting utilizing
Business Intelligence Website Design (Before Power BI)
Market share analysis
Overlay analysis
Sales analysis
Database Design, Analysis and Development
Data Cleansing
Creation of GUI
Database development
Data mining, mapping, integrity, and integration
MySQL and SQL Database Administration
Master Data Management for big data sets
Data Reconciliation
Help develop SOP’s for new/developing products (cGMP, FDA, USDA)
Web Graphics Design
Design of oligonucleotides (siRNA,DNA primers, etc.) for gene silencing

Professional Training

SQL Programming
University of Missouri St. Louis, West County
Description: 2 semesters worth of SQL programming training

Advanced SQL Queries Level 1, 2, 3
University of Missouri St. Louis, West County
Description: Subqueries, Advanced joins, Table Aliases, String Functions, Running Totals, Metadata Queries, Self-Join, Common Table Expressions, Censoring/Truncations, String Manipulation, String Functions

Languages

English: Native Language

Spanish: Novice Listener, Novice Speaker

Pigeon Sign Language: Intermediate Receptive, Intermediate Signing

Computer Skills

Programming: Perl, Javascript, Java, Python, PHP, SQL, MySQL, Oracle, PostgreSQL

Applications: Teradata, Jira, Tableau, DNN, Windows Server, MS Office, MS Word, MS Access, MS Publisher, MS Visio, MS Excel

Platforms: Linux, Unix, Windows, iOS

References

Available upon request

"
,
Data Scientist,"
SUMMARY
+ *+ consecutive years dedicated to Data Science space, with roles including Senior Data Scientist, Senior Machine Learning Engineer, Data Scientist, and Computer Vision Specialist/ML Engineer.
+ Real-world experience applying Machine Learning and Data Mining with large datasets (>1M rows) of Structured and Unstructured Data.
+ Proven experience building models to handle Data Acquisition, Data Validation, and Predictive Modeling.
+ Create analytical models, algorithms, and custom software solutions based on accurate understanding of business requirements.
+ Apply in-depth knowledge and understanding and practical application of Machine Learning, Deep Learning, CNN, Naïve Bayes, Regression Analysis, Neural Analysis.
+ Skilled programming in R, Python, SQL, Spark, Scala, and MatLab.
+ Produce visualizations using R-Programming, Ggplot2, Plotly, Matplotlib, and Tableau for end-user ad-hoc reporting.
+ Hands-on with Python to manipulate data for data loading and extraction.
+ Work with Python libraries such as MatPlotLib, NumPy, SciPy, and Pandas for data analysis.
+ Proficient in using Python, R, SQL, Spark, and Scala applied to Hadoop ecosystems for extracting data and building predictive models.
+ Experience with multiple NLP methods for information extraction, topic modeling, parsing,
+ and relationship extraction.
+ Develop, deploy, and maintain production NLP models with scalability in mind.
+ Design, develop and deploy custom BI reporting dashboards using Shiny, Shinydashboard, and Plotly to providing actionable insights and data-driven solutions.
+ Support Vector Machines (SVM) and Random Forest machine learning techniques.
+ Skilled in Machine Learning, Big Data, Internet of Things (IoT), Supply Chain, Social Media Analytics, Quantitative Analysis, and Application Development.
+ Experience handling and implementing statistical models on Big Data sets using cloud/cluster
+ computing assets with AWS and Azure.
+ Experience applying statistical analysis and Machine Learning techniques to live data streams from big data sources using Spark and Scala.
+ Combine business process with in-depth technical understanding/thinking to formulate Data Science solutions that produce tangible value results to real-world business situations.
+ Apply business acumen, mathematical theories, data models, and statistical analysis.
+ Developed predictive models using Decision Tree, Random Forest, and Naïve Bayes.
+ Development of regression, classification, and recommender systems with large datasets in distributed systems and constrained environments.

SKILLS
• Analytic Development: Python, R-Programing, SQL, Excel.
• Python Packages: Numpy, Pandas, Scikit-Learn, TensorFlow, SciPy, Matplotlib, Seaborn.
• Machine Learning: Natural Language Processing & Understanding, Machine Learning algorithms, including text recognition, image classification, and forecasting.
• Data Query: Azure, Google, SQL, data warehouse, data lake and various SQL databases and data warehouses.
• Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series Analysis
• Deep Learning: Machine Perception, Data Mining, Machine Learning algorithms, Neural Networks, TensorFlow, Keras. PyTorch.
• Artificial Intelligence: Text Understanding, Classification, Pattern Recognition, Recommendations Systems, Targeting Systems, Ranking Systems, time Series.
• Analysis Methods: Bayesian Analysis, Inference, Regression Analysis, Multivariate analysis, Advanced Data Modeling, Statistical, Exploratory, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Design and Analysis of Experiments, Factorial Design and Response Surface Methodologies, Optimization, and State-Space Analysis.
• Analysis Techniques: Classification and Regression Trees (CART), Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, RNN including LSTM, Linear and Logistic Regression, Naïve Bayes, Simplex, Markov Models, and Jackson Networks.
• Applied Data Science: Natural Language Processing, Machine Learning, Text Recognition, Image Classification, Social Analytics, Predictive Maintenance.
• Version Control: GitHub.
• IDE: Jupyter, Spyder, RStudio, Google Colab, MySQL.

EXPERIENCE

Senior Data Scientist
Nestle April 2020 – Present
Arlington, VA

Nestle, USA hired me to help build a dedicated demand forecasting team. In my time there, I educated leadership on the role and benefits that data science would bring to their organization. Working with stakeholders, my duty was to ensure that any data science being performed would be relevant and useful to the company. I created several proofs of concept, including work cluster reviews using NLP (Bert/LSTM) for identifying production errors and a random forest model to predict if a customer’s order was likely going to be late. These entities allowed end-users in the supply chain a chance to mediate ahead of time and avoid any incurring late penalties.

In addition, COVID’s disruption of common time-series models gave our team a chance to employ new strategies of data replacement, resulting in machine learning algorithms that were able to continue forecasting consumption at a granular level (SKU-State). Thus, our models gave design planners significantly better accuracy of information to act upon.

+ Satisfied critical request from executive leadership: previous models could not adjust to the dramatic swings after COVID.
+ Produced a model that, despite an extremely small sample size post-COVID, delivered consistent, high-quality results using hierarchical modeling (MLib/GBT).
+ Combined several disparate data sources at different granularities (sales data, economic data, SNAP spending, and more) into one master dataset.
+ Worked in PySpark and Python on Azure Databricks.
+ Advised how best to modify existing predictive out-of-stock models to accurately forecast for a longer time horizon.
+ Developed POC, which leveraged NLP techniques and clustering algorithms (K-Means) to attribute causes to negative reviews.
+ Used webscraping to create dataset of Amazon reviews from scratch
+ Developed POC based off said dataset using NLP and worked with LDA (Latent Dirichlet Analysis) and ELMO (bidirectional LSTM) to extract and cluster key topics.
+ Worked with a variety of Python packages, including Numpy/Cupy, Nltk, Pandas, Torch/PyTorch, and Regex.
+ Built a model to determine orders that are at risk for being delivered late to allow individuals along supply chain to intervene and avoid the associated fees.
+ Created dataset leveraging numerous disparate sources (outside data from customers, internal order data, weather data, and transportation data).
+ Used PCA to deal with the very high (hundreds) number of sparse, categorical variables.
+ Delivered a model that serves as a significant improvement over the baseline, univariate time-series forecasts that were used previously.

Senior Machine Learning Engineer
Redfin September 2017 – January 2020
Seattle, WA

Real estate is constantly looking into segmentation of the housing market. I built a prediction model to estimate a home’s price. The model was trained with the statistically most important features of a home-- sold price, square feet, and location (latitude and longitude). To optimize my model’s results, I normalized the listed features by dividing by each column’s max value. Moreover, to normalize latitude and longitude, I converted location into radians. Once my EDA was complete, I split my data into 80% and 20% for training and test data, respectively. The first model resulted in a regression that predicted a price of a home; however, I wanted an even greater accuracy in my predictions. Utilizing a classification model, KNN, I reinvented my problem solution to first classify a home into a categorical bin of price per square feet, and then, using a regression model on those bins, I pinpointed the price of a home. The K that was chosen was 3, which worked efficiently on the testing data as well as predictions for unknown data. The metric I defined to evaluate the model was a percent difference between my predictions and the true price. Using my metric, it was clear that predicting price per square feet was more efficient than predicting price alone. Finally, the model was deployed via a REST API housed on an AWS EC2 instance.

+ Performed EDA on data integrated from various public and private sources.
+ Established an input pipeline that included Normalization, Imputation, and De-Noising.
+ Applied Random Train/Test Split.
+ Applied KNN Regression.
+ Defined Constructed Metrics.
+ Validated and Tested on Outside Data.
+ Used Python for the Clustering Analysis and NetLog for the simulation.
+ Enabled data exploration using statistical methods and visual packages from Python.
+ Pre-processed datasets that NetLog produced during the simulation phases and past datasets.
+ Implemented a variety of Clustering models with the preprocessed data to classify population classes.
+ Developed a segmentation solution using various clustering analysis methods, including K-means clustering, Gaussian mixture models, and DBSCAN.
+ Performed exploratory data analysis on socioeconomic class data and plotted correlation between variables.
+ Experimented with several classification methods, including decision trees, logistic regression, and KNN.
+ Experimented with forecasting methods, including time-series analysis algorithms such as SARIMA.
+ Extracted data from a SQL database using complex SQL queries.

Data Scientist
Anthem June 2015 – August 2017
Indianapolis, IN

My team was tasked with processing and ingesting large amounts of scanned documents for future data analysis. The pipeline accepted two images as input: a scanned document for information extraction and a reference form. The output contained a formatted table of the document’s features such as client names, identification numbers, and other tax information. The data extraction was implemented using Google Tesseract, AWS Text-extract, and convolution neural networks. Our group preformed optical character recognition to extract all the necessary information, which was stored in an AWS database. The project resulted in time for data entry becoming an order of 3 times exponentially faster than data entry by hand.
The tools necessary for this project included machine learning techniques and Python libraries. These tools were used for analysis as well as derivation of metrics and assisted in building proof of concepts for value evaluations for future projects. Our team primarily focused on the customer experience. Thus, an advantage I experienced was associating heavily with business partners to understand all business objective so I could explore relevant data sources to build NLP/ML solutions. Using Open-Source Data Science Platforms and analytics tools, I was able to utilize out-of-the box thinking on unstructured data to solve a given business objective.

+ Applied business analytics skills, integrated and prepared large varied datasets, and communicated results.
+ Worked with specialized database architecture and computing environments.
+ Developed analytic approaches to strategic business decisions.
+ Performed analysis using predictive modeling, data/text mining, and statistical tools.
+ Built predictive modeling using Machine Learning algorithms such as Random Forests, Naive Bayes, Neural Networks, MaxEnt, SVM, Topic Modeling/LDA, Ensemble Modeling, GB, etc.
+ Used common NLP techniques such as pre-processing (tokenization, part-of-speech tagging, parsing, stemming).
+ Performed semantic analysis (named entity recognition, sentiment analysis), modeling and word representations (RNN / ConvNets, TF-IDF, LDA, word2vec, doc2vec).
+ Worked with Big Data infrastructure and tools such as Hive and Spark.
+ Collaborated cross-functionally with team to develop actionable insights.
+ Synthesized analytic results with business input to drive measurable change.
+ Performed data visualization and developed presentation material using Tableau.
+ Defined the key business problems to be solved while developing and maintaining relationships with stakeholders, SMEs, and cross-functional teams.
+ Provided knowledge and understanding of current best practices and emerging trends within the analytics industry.
+ Participated in product redesigns and enhancements to know how changes would be tracked and to suggest product direction based on data patterns.
+ Applied statistics and organized large datasets of both structured and unstructured data.
+ Worked with applied statistics and applied mathematics tools for performance optimization.
+ Facilitated data collection to analyze document data processes, scenarios, and information flow.
+ Determined data structures and their relations in supporting business objectives and provided useful data in reports.

Computer Vision Specialist/ML Engineer
Cargill October 2014 – May 2015
Wichita, KS

Cargill is a global trading, purchasing, and distribution agriculture commodity company. My team used convolutional neural networks to build a Computer Vision machine learning model. The model was trained with pictures of plants that grow in water (hydrophytes) to identify unhealthy hydrophytes. Specifically, the model was used to optimize the health and yield of hydroponic farming. The result of the model produced a vast increase in production of harvest, which in turn increased revenue and gross profits.

+ Used SQLAlchemy to perform queries and pull data from Amazon S3 MemSQL database into Pandas DataFrames in Python.
+ Created Python scripts that converted images to a JPEG filetype then resized to a set size.
+ Utilized Pytorch’s Image Processing to develop ImageDataGenerator objects to augment (rotate, blur, flip ) images for a Convolutional Neural Network.
+ Imported from Pytorch’s Torchvisions InceptionResNetV2 as the base model for the initial CNN using sample images from field workers and online resources.
+ Utilized a second pre-trained CNN called DenseNet201 as transfer learning technique to train data collected from the MemSQL database.
+ Developed basic Flask app using a static HTML template as a UI to upload images, then return a processed version image with a disease classification.
+ Evaluated training data and testing data using SOUS along with various over sampling and under sampling techniques to placate unbalanced image data.
+ Coordinated with field workers, logistic warehouse workers, and other departments to ensure the image recognition product fit the plant disease use case in deployment.
+ Defined different metrics and indicators for ensure the CNN model maintained its integrity.

EDUCATION
Completed:
Bachelor of Science in Mathematics with Applied Specialization
Central Washington University

Associates Degree in Arts and Science
Wenatchee Valley College

In progress:
PhD Candidate in Mathematics (all but dissertation)
Washington State University

"
,
Data Science - Project Lead,"
.
.
ANITA BLEGE
ado6pg@r.postjobfree.com
+233*********
Accra, Ghana 00233
SUMMARY
Experienced Project Manager well-versed in identifying strategic opportunities to benefit businesses. Systematic and driven with strong attention to detail and dedication to developing and managing successful analytical processes. Expert Excel, SQL, Salesforce, Agile Methodology, Power Platform, Google Analytics, Mautic, Azure, and Python user.
SKILLS
Business requirements
gathering
Business Analysis
Data warehouse administration
Data Engineering
Agile methodology
Business Development
Web Development
Leadership
Project Management
Communication
Inclusion & Change
Management
Risk assessments
Organizational skills
EXPERIENCE
03/2020 - Current
Data Scientist – Project Lead, Scale Work, Remote
Liaise between multiple domains and cross-functional teams as a project lead through multiple iterations from ideation to release.
Maintain multiple marketing & sales automation platforms and assist stakeholders in building and automating simple custom reports and dashboards.
Use Agile methodology to ensure that complex projects are delivered by the team ahead of schedule or on time; meeting, or exceeding stakeholders' expectations.
Perform analytics and modeling for companies like Telefonica, Thyssenkrupp, and LafargeHolcim Ltd by focusing on business intelligence, diagnostic, predictive, and prescriptive analytics to provide useful insights from data. 02/2016 – 03/2020
Data Analyst & Developer - VII Limited, Remote
Developed insider business apps using Power Apps and integrated them with Power Platform and Azure Services.
Performed data engineering tasks and data warehouse administrative tasks.
Created machine learning models to successfully predict potential machine failures and reduce operating costs.
.
.
Created analytics reports and dashboards used by managers to help increase product sales and customer engagement in 1,000 + shops.
Used Power Platform to build business apps for companies to enable them to streamline their internal processes, reduce their cost of operation, and increase productivity among employees.
Used Power BI and Excel for business insights to increased Clients' ROI by at least 15% per month.
Taught interns how to outsource or create and manage their technical products.
Mentored interns on HTML, CSS, and JavaScript programming and proper usage of the different technology stacks.
Mentored interns on the development and commercializing of their technology projects.
01/2014 - 12/2015
Research Analyst, Southeastern Louisiana University, Hammond, LA
Assisted with DNA extraction, PCR analysis and information storage and retrieval using Oracle database and Microsoft Excel.
Processed and analyzed data obtained from DNA analysis.
Tutored freshmen from several community colleges on Genetics and Molecular Biology
Supported project leads in preparing regulatory documentation and presentations highlighting findings.
EDUCATION AND TRAINING
Master of Science, Computer Science - Data Analytics Nova Southeastern University, Fort Lauderdale, FL
Bachelor of Science, Biology
Southeastern Louisiana University, Hammond, LA
CERTICATES
Microsoft Certified: Power Platform Developer Associate Microsoft Certified: Azure Data Scientist Associate

"
,
Data Analyst Research Scientist,"
MINYEONG HAN
Woodside, NY ***** 405-***-**** ado51r@r.postjobfree.com
LinkedIn: linkedin.com/in/minyhan GitHub: github.com/promisinghan SKILLS
Tableau (Tableau Certified Associate; Finalist at the NYC Tableau Data Visualization Competition in 2019),
Python (Machine Learning, A/B Testing, Regression Models), SQL, R, SPSS, Certified Microsoft Office Specialist,
Lean Six Sigma Green Belt Certified Professional, Corporate Finance Institute Certificate of Building a Financial Model,
Tableau Data Visualization: public.tableau.com/app/profile/minyeong.han
GitHub Data Science projects: github.com/promisinghan
E-Portfolio Python Data Science projects: eportfolio.greatlearning.in/minyeong-han PROFESSIONAL EXPERIENCE
DISH Network, New York, NY
Senior Data Scientist – October 2020 to April 2021
DISH & Sling TV Ads target audience segmentation and propensity models using viewer measurement data and CRM data, KPIs
& trends tracking and reporting.
DISH & Sling TV Cross-platform data visualization for media market (DMA). Python, Tableau, AWS Athena, S3, ERM World Carp Inc., Englewood Cliffs, NJ
Data Analyst – October 2019 to October 2020
Analyzed the internal data from various sources and create dashboards to help making data-driven decisions.
Provided consulting support and recommendation to build effective work flows; Designed and developed professional presentations for executives; KPI & trends tracking and reporting. Python, Tableau, SQL, Hadoop, MS Office. DENTIDESK, New York, NY
Data Analyst Intern – June 2019 to August 2019
Provided analytical insights (competitor analysis, customer segmentation, market research) leading to develop a new product by conducting independent research on marketing optimization strategy using Python and Tableau; Analyzed the internal data from various sources and built dashboards using MySQL & Tableau to report to the CEO. Oklahoma State University Center for Veterinary Health Sciences, Stillwater, OK Research Scientist – August 2016 to July 2017
Data analysis using MS Excel and open-source software OpenCFU; produced results by utilizing Western Blot techniques and Plaque assays to elucidate the effect of SGK-1 inhibitor (GSK650394) on AKT expression and activation level in BHV1-infected CRIB cells and HSV1-infected Vero cells.
EDUCATION AND HONORS
Massachusetts Institute of Technology (MIT) Professional Education
Applied Data Science Certification GPA 4.0 – August 2021. Statistics; Machine Learning - Clustering, Classification, Regression, Random Forest, Time Series; Deep Learning - Neural Networks (CNN, GNN); Recommendation Systems using Python. Columbia University, Columbia Engineering, New York, New York
FinTech Bootcamp Certification GPA 4.0 – March 2021. Machine Learning in Finance using Python, AWS SageMaker, Deep Learning (RNN – LSTM), Blockchain and Cryptocurrency, Financial Analysis and Forecasting, Natural Language Processing
(NLP), Sentiment Analysis, Smart Contract and Solidity, PostgreSQL, Credit Card Fraud Detection Yeshiva University, The Katz School, New York, New York
Master of Science in Data Analytics and Visualization – August 2019. Dean’s Scholarship (2018-2019); Katz School Academic Scholarship (2018-2019); Graduate Student Council Member (2018-2019)
Relevant Coursework: Data Visualization and Storytelling (Tableau), Structured Data Management (MySQL), Business Modeling and Data Analysis, Computational Math and Statistics (Python), Analytics Programming (Python), Data Product Design (AI- powered mobile application design, InVision, SQL), Data Science (Python Machine Learning), Project Management Sungkyunkwan University, Seoul, Republic of Korea
Master of Science in Biological Sciences, GPA 4.0 – February 2016. Academic Excellence Scholarship (2014-2015)
Research for Master’s Thesis: Bioinformatic analysis to predict the G-quadruplex forming sequences using MS Excel, non-B DB database along with QGRS Mapper software. Genetic and biophysical techniques to study repressed transcription in HCMV genes.
Bachelor of Science in Biological Sciences, GPA 3.89 – February 2008. Academic Excellence Scholarship (2004, 2007); First Prize, National Contest for Academic Thesis on Public Relations of Patriots and Veterans (2005)

"
,
Data Analyst Scientist,"
JOWIN CHIRIYANKANDATH
860-***-**** • ado46p@r.postjobfree.com • Hartford, CT 06106
CAREER GOALS
Summary: I am graduate student looking for Full-time Opportunities as a Data Analyst/Data Scientist. I am currently working with University of Connecticut’s Office of Institutional Research. I have creating projects using Python and R for 2 Years
Key skills:
Python • R • Amazon Web Services • Microsoft Azure • Microsoft SQL Server • Tableau • Microsoft Power BI
AngularJS • Deep Learning • Pivot Tables • NetBeans • Computer Science • Statistical Analysis System
ReactJS
PROFESSIONAL EXPERIENCE & ACHIEVEMENTS
UConn Office of Institutional Research 2021 to Present
Data Analyst
• Creating a Home-Grown Data Analytics Dashboard using R and Tableau.
• Extracted data of 400,000 rows from SAS and converted into CSV files to develop models like XGBoost for predicting students' graduation and retention rate of Undergraduate and Graduate Students.

Dashboards created: Graduate Dashboard, Academic Dashboard, Student Credit Hours and HR, UConn Ranking Dashboard
Harrisburg University of Science and Technology 2019
Project Intern
• Led a Project team of 10 students to create Movie Recommender System based on KNN using Python from Data mining through IMdB Website
Revi-tech Infosolutions 2019
Data Analyst and Software Developer Intern,
• Slashed operational cost by 20% by using Deep-Learning Models such as CNN on inventory and customer data to optimize the inventory and operations
EDUCATION & TRAINING
Master of Science, Major: Business Analytics and Project Management, University of Connecticut, Hartford, Connecticut (2021)
Master of Science in Business Analytics and Project Management (MSBAPM) 3.7/4.0 GPA
Bachelor of Engineering in Computer Science, Major: Computer Science, St Francis Institute of Technology, Mumbai, MAHARASHTRA (2020)
Bachelor of Engineering in Computer Science (BE) 7.00/10.0 GPA
PUBLICATIONS
My final Year Project in building a system that helps to aid in Data Preprocessing. It is a semi-automated system of Data cleaning
https://www.researchgate.net/publication/352063621

"
,
Project Manager Web,"
SUMMARY:
●Excels at troubleshooting and leading business transformations for maximum ROI.
●Assesses business processes, architects solutions, delivers & institutionalizes successful transformations.
●Nominated by Forbes in 2016 as leader in Big Data integration delivery for Enterprise Analytic Solutions.
●Develops Technical Enterprise Portfolio, Program, and Product Roadmaps. Drives to successful delivery.
●Exemplary reporting ability using multiple Business Analytics tools for both C-Suite larger team(s).
●Oversees Development, Testing, Release, DevOps and Data Governance teams.
●Proficient in the coordination of resources for highly complex programs
●Experienced in all Waterfall, Agile techniques SAFe 4.0 Scrum, Kanban, Retrospectives, Lean, Scrum of Scrums.
●Budget Planning, Analytics, IT Strategic Alignment, SMAC, Confluence, JIRA, SharePoint, Containers
●Strong problem solving, decision making, issue analysis, and resolution skills. Highly organized.
●Certified in Smartsheets (Control Center, DataMesh) JIRA, Confluence (Connectors) Version1, Rally.

SKILLS:

Product Lifecycle Management, SDLC, SAP, ServiceNow, TOGAF, JIRA, Smartsheets, SharePoint, Version One, Visio, FACETS, MS Office, MS Project, XML, SQL, AGILE, SDLC, CMMI, ITIL, CaliberRM, Clarity, FileNet, Rally, daptiv, teamworks.com,, CPMI, Oracle, Business Objects, SAP HANA, XCelsius, Data Modeling, COBIT, SaaS, SOX, Remedy, HTML5, JavaScript, Tableau, Big Data, Predictive Analytics, Data Visualization, Hadoop, SPSS, Data Warehouse, Data Marts, Splunk, Salesforce.com, Social Media, Python, C++, UNIX, Talend, Tealium, Regulatory Management, User Stories, Epics, PI Planning, Retrospectives, Vendavo, Collibra, Snowflake, QuikSense.

EDUCATION:

M.A. Technology & Communications Management; Denver University, Denver, CO
M.I.S. Information Science; Indiana University, Bloomington, IN
B.A. double major Urban Planning & Geography; University of Arkansas
Executive Leadership Program; Berkeley Haas School of Business
Executive MBA coursework; The College of William & Mary

CERTIFICATIONS:

PMP, Agile Scrum Master, Agile Product Manager, SAFe 4.0, Smartsheets, Service Delivery Mgmt., Big Data, Business Analytics, Engineering Mgt., Portfolio Mgt., Product Management, Software Engineering, HIPPA, Rally Super User, Paralegal, Six Sigma.

EXPERIENCE:

Founder & Managing Consultant, Wallace-Rose Investments, USA. 2002 – Present
Acquire, lead, manage complex IT solutions, products, analytics and audit programs for clients in multiple industries and market sectors. Oversees all levels of a program or product lifecycle including requirements elicitation, architecture and design, budgeting, creation of effective communication and workflow channels among necessary divisions while maintaining solid vendor relationships and ensuring on-time program/product completion. Utilizes expertise in Product Management, R&D management, Solutions Sales, Marketing, budgeting, Business Analysis, Cost Controls, Executive Reporting, Compliance and QA to deliver successfully.
●HIMSS Program Manager – Worked as Program Manager in USA with McKinsey Consulting in Germany and United Kingdom to secure second round funding HIMMS subsidiary professional services organization. Worked completely remote starting March 2/21 – 4/21.

●Ball Corporation Program Manager – Assistant Director/Program Manager for Data Integration & Analytics 45 person + organization onshore and off-shore. Turned around the failing Pricing Program (Vendavo Software) integration with SAP and #6 priority corporate program that had been failing for 18 months and successfully brought it to completion in 6 months. Worked UK & US time zones daily. Analyzed all IT project/program data (ServiceNow & JIRA) and presented to C-Level that 17% of all KTLO efforts were actually project work that was not being funded by the Business. In 14 weeks delivered a #5 priority Data Governance project integrating Collibra software across the enterprise along with conducting ROI comparisons between Collibra and Safyr. Delivered POC and instituted for use of Meisterplan across IT organization for Resource and Capacity Planning. Was scrum master for reporting organization QlikSense, Talend, and Snowflake. Worked completely remote starting Mid-March 2/20 – 11/20.

●DISH Networks Program Manager for In Home Services. Brought in for a 90-day contract to re-engineer LOB delivery process from Ideation through Release to a 30 – 45 day cycle to support exclusive contracted services with Google. Developed, documented and received approval for new HIS Product delivery process using collaborative problem solving methodologies with both the Business and IT sides of the house. Rally was the PPM for the IT and established entrance and exit criteria that mapped to the new HIS Product Cycle Process as well as performed the necessary data mapping and analysis to support ServiceNow which was the source of record. Developed a 12 month Road Map for Rally improvements that included auto-population from ServiceNow. Worked with IT HIS to clean up Rally reducing over 300 Initiatives to 77 active Initiatives that mirrored ServiceNow and would support near real-time reporting on Initiatives, Features, and User Stories. 10/19 – 12/19.

●Charter Project Manager for Applications Provisioning Operations (APO) for UIDP Program entering its 3rd year. My role was to achieve buy-in from 58 Application and Product stakeholders, provide logistical guidance for 29 business critical applications across 3 telecommunications footprints (servicing 30M subscribers) to move in a phased approached to a newly architected platform in a development environment without a formal release management or PMO processes. Additionally, worked as the communications interface between Product and Operations. I managed using Smartsheets, JIRA, and MS Office tools used Agile and SAFe practices where applicable 7/18 – 12/18.

●Charter Program Manager for Advanced Engineering managed a 26 member team with 3 major work streams; Network Applications, Data Center and Network Functions Virtualization (NFV). Reorganized work into 2 Kanbans and 1 scrum. Performed project audits reduced JIRA projects by 73%. Introduced and institutionalized new workflows. Performed duties as Scrum master and well as Scrum Coach for team. Worked with Technical Writing to transfer archive of approved documents to the AE Standards Repository. 2/18 – 06/18.

●ADT Salesforce.com Project Manager - Dealer Channel extended Salesforce.com capability to enable sharing of Leads, New Orders, Content, Mass Communications, Case Management and availability to Mobile devices to Dealer LOB. Solution integrated with existing Dealer Web on Adobe Cold Fusion platform. Responsibilities included Data Governance, Budgeting, Resource Planning, build out of Project re-design, Product Owner role, PMO and executive reporting. Drafted 200+ User Stories and input into JIRA while Integration activities with Protection1 merger was occurring. Used Salesforce.com sys admin and developer screens, Planview, JIRA, Confluence, MS Project and MS Office 365. Aurora CO. 7/17 -12/17.

●Cigna Enterprise SAFe Coach, Agile Program Architect & Admin stood up Agile Center of Excellence RallyHelp@Cigna for 35,000 employees. The design requirements for RallyHelp@cigna were responsive and self-sustaining. Created 24 month roadmap for phased delivery. Provided Agile and RallyHelp@Cigna via phone and online distribution channels. Automated the licensing, software acquisition and for users. Managed software licenses. Integrated RallyHelp@Cigna with enterprise wide Cigna Support Center. Provided Rally instruction via classroom & WebEx. Documented solutions, business processes. Built out ACoE wiki, ACoE SharePoint site, provided Data Architecture services to Agile SAFe Release Trains, Programs, and Portfolios. Customized dashboards for scrum teams, Kanbans, and 75% travel, Bloomfield, CT. 9/16 -6/17.

●Truven Health Analytics Project Manager did initial One Truven Experience Program [Data Lake] incorporating Tableau, set up in Rally tool consisting of 4 Scrum teams, User Stories, tasks, Point, Hours, parent/child relationships and, Rally catalog apps along with training a team of 24+ (on-site and remote) using WebEx on-site in Agile

●IBM Watson Research Lab & McCormick & Company Project Manager for Big Data Cognitive Computing Research Program Drafted and secured Sr. Mgt. 7 sig acceptance sign off on Program Charter, Communication Plan, Roadmap, RACI, WBS, and Risk/Issue matrix for Big Data Program with MKC Chief Scientist & CIO sponsorship. Week 2 developed and hosted two day workshop for IBM Watson team to meet with MKC Product Development and IT. Led Due Diligence effort on data sources. Set up entire resource and financial templates in Clarity. Designed data structure & classification system, as developed, administered, and populated Program SharePoint site. Provided detailed analysis on SOW/MSA and analysis on suggested encryption levels. Drafted Discussion Papers re: business risk for audit purposes. Scheduled entire Big Data Program’s weekly Leadership meetings, Steering Committee meetings, as well all hosted visits and events. Baltimore, MD. 2/15 - 12/15.

●State of Colorado Governor’s Office Information Technology IT Program Audit brought in as an independent verification and validation for “major project” History Colorado Ute Museum extension as required by C.R.S. 24-37.5-105 (4)(a)(VII)(VIII). Initiated required Communication Plan, Project Road Map, WBS, BRD, Risk Assessment, DR Plan, and funding strategy for ongoing maintenance. Identified gaps in current Project Plan and offered solutions for “major project” to be in full compliance with C.R.S. 24-37.5-105, C.R.S. 24-37.5-109. Denver, CO. 01/15.
•TW telecom Project Manager brought in to turn around a customized Enterprise Salesforce.com implementation in the midst of Business Acquisition Integration activities with Level 3. Managed 7 application teams, on-site, off-shore, and remote developers. Was responsible for managing and coordinating Unit, System Integration, QA, Regression, and UAT testing as well as refresh, defect, Issue, and Risk Management. Developed Go-Live Deployment plan through building relationships and coordinating with new CM organization, Sales, Application managers, DBA teams, and developers. Used Agile/Scrum methodology. Hosted daily Scrum of Scrums. Used Version1 as Software Program Development management tool and provided all essential Executive and Program reporting. Created and maintained Salesforce.com Communication Plan, Resource Plan, Calendar, and Deployment Schedule via SharePoint. Worked with C-level executives and entire 146 members of the Implementation team. Denver, CO. 01/14 - 11/14.
•Wellmark BCBS IT Environment Project Manager managed the architecting and delivery upgrade from a 4 tier to 5 tier environment to better enable current Unit, System, and UAT testing and establish future in-house operations capability as well as meet Federal ACA requirements. Used Agile methodology, established and maintained Quantum Program Communication Plan, Calendar, Plan/Schedule via SharePoint. Partnered with Release Mgmt. and Testing Mgmt. teams. Worked with C-level executives and all members of 259 person team. 100% Travel, Des Moines, Iowa. 3/13 - 12/13.
•@5280BigData: Business IT Architect, Project Manager Architected, designed, developed (uses Hadoop framework) and maintains Social Media channel for distribution of content and mind-share of Big Data, emerging tools, thought leadership, companies promoting tangible solutions for the complexities surrounding Data Volume, Variety, and Velocity specific to both the Boulder/Denver and global Big Data ecosystem. Current.
•BigDataWeek/Colorado CoChair, Project Manager for local and global effort to promote the use and application of Big Data concepts, tools, and platforms among regional R&D and business communities. Provided oversight and delivery of Big Data Science Fair and Coffee @ Techstars, hosted presentation for start-up and data enthusiasts. Coordinated multiple Big Data demos and sessions. Owner of @5280BigData. Participants included SendGrid, Precog, FUSE, GNIP, CU, Tagwhat, Trueffect, and Techstars. Boulder, CO. 2/13.
•TriZetto Scrum Master for Consumerism Initiative provided PM capability to Big Data & Analytics Division’s Healthcare Acquisition. Managed and advised on the development of Vendor Assessment Tool Kit, all Vendor Management Assessments, and on-site Due Diligence activities, ands drafting of Business Due Diligence Plan for acquisition w/recommendations for C-Level audience. Architected, designed, developed, and administrated Program SharePoint site. Operated in a matrixed environment across multiple time-zones and exclusively at the executive level. 2/12 – 12/12.
•TMG Health Project Manager for 4 MAPD and 2 PPO customized TMG software implementations, including Medicaid and Medical Management services. Also managed the Annual Operational Readiness exercise to ensure that CMS guidance was in place with existing accounts. Worked with all areas of the enterprise to establish a reusable process for forecasting and managing annual new business implementations. Established and led the Program Focus Group to (precursor to a corporate PMO) to facilitate intra-divisional communications, coordination, and planning across the entire Sales/Delivery lifecycle. Developed, coordinated, and delivered 1 day workshop to provide training on cross-organizational requirements for gathering a Program baseline for development of an Enterprise Implementation WBS and Schedule. Created and managed 7 work groups and 5 PMs to re-engineer broken processes while managing current 2012 implementations. Documented problem areas to be re-engineered for the 2013 schedule. Scranton, PA. 7/10 – 12/11.

•ProMorphics Director of PMO developed and institutionalized ProMorphics “One Team” methodology while architecting, building, and deploying organizational infrastructure including SharePoint Server and SAP by Design. Initiated PMO processes via Org Chart, Weekly Corporate Portfolio Status views of internal and external projects, Sales pipeline, and IT development environments. Charged with building and training Solutions Delivery team in Agile/Scrum methodology. Project managed Center of Excellence and SAP By Design enterprise rollouts. Was billable 50% of time doing account management, Project Management, leading dashboard and mobile application design sessions developed in SAP Celsius, Hadoop, HANA, SAP Business Objects Explorer, is, Salesforce.com for Fortune 500 clients in the Telecommunications, Computer, and Energy industry sectors. Virtual, Elmhurst, IL. 01/10 -6/10.
•Promontory Financial Group Director of PMO established and managed start-up logistics and PMO for Federal Government OCC Ordered mortgage foreclosure independent audit review activity. Worked directly for PFG's, Sr. Managing Director. Drafted and managed all revisions of organizational charts, Engagement Letter and required attachments, PMO policies. Responsible for understanding and mapping business process, gathering requirements, analyzing data, proposing solution options, documentation as well as performing coordinating user testing, etc. Managed and mentored four PMs. Managed using AGILE/Scrum and PMP methodologies. Denver, CO. 7/09 – 12/09.
•Erie Insurance Project Manager for Data warehouse assessment deliverable for Fortune 500 Insurance Company with integration to CRM. Managed DW program website using AGILE/Scrum and PMP methodologies. Developed proposal, scope, requirements definition artifacts and project Roadmap, and delivery timeline for Data Analytics and Reporting organization. Managed team of 11 PM’s and developers. Erie, PA. 12/08 – 6/09.
•TIAA-CREF Project Manager for two phases of the “Scaling Brokerage Platform” delivering 300M in new assets to the organization. Lead global co-located team across the full development lifecycle using both Waterfall and Agile methodologies while collaborating closely with the Brokerage business unit and vendors. Responsible for Project Plan, Project Framework, Project Charter, Work Breakdown Structure, Requirements elicitation sessions and Web project management. Responsible for understanding and mapping business process, gathering requirements, analyzing data, proposing solution options, documentation performed Clarity Report configurations, and coordinated user testing. Had a combined budget responsibility of over 5M+. Denver, CO. 1/08 – 11/08.
•InfoNow Project Manager for solution design and customer delivery for Channel Data Management tool developed in Business Objects, SQL, and Salesforce.com. Gathered BI reporting requirements for Global 500 Global clients. Managed solution deliverables using an AGILE/Scrum methodology, SQL, Salesforce.com in a B2B SaaS software development environment. Successfully delivered multiple projects using JIRA with multiple customers at a time. Performed roles as Scrum Master and Risk Mitigation Analyst in a nascent Agile advanced Tech Start-up using open source web based tools for managing project. Responsible for understanding and mapping business process, gathering requirements, analyzing data, proposing solutions, documentation as well as performing configurations, and coordinating UT. Interfaced regularly with global clients, Product Mgrs., Engineering teams, and Vendors. Denver, CO. 9/07 – 12/07.
•Colorado Banker’s Life Regulatory & Compliance Manager Redefined and developed outmoded Compliance Management department including establishing a Sales & Marketing Data Mart. Managed all regulatory and compliance efforts across North America and Guam including AML program management. Established compliance web portal for BI reports, risk analysis queries, and expanded data mining techniques to improve AML practice. Drafted and updated Fraud Plan with STOLI requirements, corporate BCP and AML Program plans, and implemented Fraud management plan and developed Compliance business workflow diagrams and procedures. Responsible for understanding and mapping business process, gathering requirements, analyzing data, proposing solution options, documentation and coordinated user testing. Managed solution deliverables using an AGILE/Scrum methodology exclusively and had two direct reports. Greenwood Village, CO. 4/06 – 8/07.
•Colorado Banker’s Life Project Manager Managed business and system requirements for a multi-million IT modernization project including data warehouse creation utilizing Agile methodologies. Performed roles as Scrum Master, BPM & Systems/Data Analyst. Greenwood Village, CO. 9/05 – 4/06.
•CIGNA Sr. Scrum Master & Product Manager HIPAA Compliance Manager Converted legacy systems to FACETS software for a $250 million re-systemization project managed and ensured data security and HIPAA/SOX compliance for the IT Division, managed and administrated division website for program area. Had 5 direct reports. Managed solution deliverables using a hybrid AGILE/Scrum and Waterfall methodology. Division managed and administrated division website for program area. Greenwood Village, CO. 4/03 – 7/05.

Director of Professional Services - NA, Borland Software Int’l; Scotts Valley, CA, 2000 - 2002
Managed North American professional services division with > 10M P&L and a staff of 25+ consultants.
Integrated professional services sales with software sales, creating 50% growth in 6 months and establishing
strategic partnerships with key customers and complimentary service providers. Expanded division to
include sales of standardized courseware and customized user training along with deploying Salesforce.com
for a CRM effort. Collaborated with direct and indirect sales channels to expand service and solution
opportunities. Major accomplishments include:
●Developed web portal for Professional Services Organization, Sales, Channel Partners, consulting resource support.
●Standardized methodologies and documented best practices for sale of services including delivery of solutions.
●Established & formalized consultant career paths, professional growth plans, and technical training tracks.
●Led Acquisition & Merger opportunity.

Program Director, Compaq Computer Corp; Houston, TX, 1999 - 2000
Hired as a turnaround expert and deal closer on strategic e-business account opportunities worldwide,
representing a $40 million account base. Worked closely with senior business leaders and decision-makers on
multiple global engagements. Highly skilled in Business Architecture, BPM, business case planning, data
management, risk assessment, and client services. Prospected among Fortune 500 leaders Telecom, Energy,
Government, Financial Services, Transportation, and Healthcare sectors. Major accomplishments include:
•Salvaged a critical international manufacturing project spanning 22 companies throughout Western Europe after rolling out effective solutions to increase productivity and streamline operations.
•Succeeded in winning a $30 million bid over Dell, drafted winning proposals, worth $10 million in potential revenue from SOA and Oracle application integration for client. Solution used SaaS for billing and invoicing.

Sr. Enterprise Architect Consultant, IBM Global Services; Armonk, NY, 1997- 1999
Coordinated all aspects of multiple project management with cross-functional teams of professionals
involving enterprise-level domestic and global on-demand SOA solutions. Highly skilled in BPM
re-engineering integrating e-business applications, which incorporated CRM and marketing channel
development. Supported ongoing business development efforts through extensive proposal generation
and client presentations.
•Performed due diligence on a $400 million New Zealand telecom outsourcing bid.
•Expanded account activity by 22% within six months by implementing a $2.7M web services integration architecture to support outsourcing efforts, utilizing COGNOS BI and performance management solutions.
•As Project Manager, facilitated a global leading edge “Watershed Study” to analyze Internet impact on travel industry ROI.

Program Manager, NASA Langley Research Center; Hampton, VA, 1992 – 1997
Hired to spearhead the introduction of new “Internet” programs, along with the set-up of a searchable document web server, utilizing WAIS, HTML, and related technologies. Deployed expertise in Internet technologies, protocols, and web development languages and tools to enhance all levels of corporate communications.
•Instrumental in developing and launching the world’s second documented Intranet.
•Established searchable database engine for search and retrieval of NASA Technical Reports using IP and WAIS.
•Facilitated workstation modernization and information automation initiative for 600 aerospace professionals.
•Developed, marketed and managed the delivery of over 150 Internet and web training classes utilizing TQM, Six Sigma and Train the Trainer methodologies.
•Published over 20 original technical reports and delivered several key papers at international conferences.

"
,
Data Analyst Scientist,"
PRITHVI SHETTY
AVAILABILITY: JAN ****
WORK HISTORY
Luxottica Retail, Georgia, United States (Sep,2021 – Present) Business Data Analyst Intern
• Extracting information from multiple sources through SQL, cleaning the data and demonstrating analysis and trends through Tableau.
• Contributing to meetings by asking right questions and offering new insights and developing strategies.
• Performing Budgeting for the Financial year and forecasting the expected inventory and resources.
• Automating the reports to streamline the business processes and extending support to team members to ensure process compliance.
• Performed data mining and analysis on over 140k data for batch determination to be able to proceed for management of inventory.
• Streamlining of business processes in the distribution center at Credit center with an expected savings of
$15M.
Tata Consultancy Services Ltd, India, Germany, Japan, Indonesia,2015-2021
• Developed KPIs analyzing effectiveness of strategic initiatives; contributed in POCs or new competency building initiatives; opportunity assessment resulting in an increase in revenue by $150 M.
• Collaborated cross-functionally across 6+ teams implementing new application and facilitating quarterly business reviews, in-depth analysis of product metrics identifying new opportunities and features thereby making changes to existing products and presenting analytical reports to executives via dashboards using Tableau.
Key Projects:
Data Analyst
Siemens AG
• Spearheaded the organization-wide effort restructuring, optimizing, and automating reporting & analytics for Germany, Denmark and Canada reducing the time required for reporting by 60%.
• Defined linkages relating to the organization, facilities and people and coordinating problem resolution.
• Met with stakeholders establishing favorable business relationships and supporting mutually beneficial interests.
Solution Architect
Kao Corporation
• Created country-specific model identifying high-risk countries as Sanctioned Party list for trading as per Government agencies assisting in guiding our clients from Japan and subsidiaries in Indonesia minimizing their compliance costs by 75%.
• Led a team of four analysts developing and maintaining productive client relationships, guiding the evaluation of goals & strategies.
• Empowered a team of 20 members from Indonesia optimizing performance by conducting end-to-end business training for effective integration with compliance management & risk management. Infosys Technologies Ltd, India,2011 - 2015
• Extracted data on integration issues & reported all findings, improving processes by 75%.
• Tested all troubleshooting methods, devised innovative solutions and documented resolutions for inclusion in knowledge base for support team's use.
Key Projects:
Techno-Functional Consultant
Syngenta AG
• Owned End to End implementation project by creating an application providing cost analysis associated with delivering the seeds from manufacturing plant to destination via hubs gaining financial insights and developing strategies minimizing tax and custom liabilities by 55%.
• Scraped data through various sources of manufacturing plants/subsidiaries located in different countries using SQL &performed exploratory analysis &identified best possible subset of hubs maximizing profit by reducing associated tax &surcharges, computing an increase in revenue by 3M. ACCOMPLISHMENTS AND LEADERSHIP
• Star of the Month award for successfully leading analytics for both Gas and Power and Digital Factory streams of Client - Siemens,2020.
• Completed 2 levels (A1 and A2) of German Language from Goethe Institute, an Internationally recognized Institute.
• Mentored a team by new recruits for domain specific training (TCS – 2018, 2019, 2020).
• Organized seminars and events having footfall of over 1000 students. PROFESSIONAL SUMMARY
Motivated Analyst offering versatile professional experience in Retail and Information Technology streams delivering cost-effective, high performance technology solutions to meet shifting organizational demands. Astute technology-business
professional translates operational needs into technical solutions. Mastery of analytics and reporting tools to provide insights in product design, flows and user experiences using data-driven philosophy. Business Analysis Data analysis Critical Thinking Problem Solving Business Process Optimization Business Strategy Cross-functional collaboration Forecasting Resource allocation Budget Functional/Business Requirement Gathering Risk Management SDLC
Marketing Strategy User training
SKILLS
• Tableau
• Python
• R
• HADOOP
Subjects Planned
• AWS
• Statistical & Predictive Modelling
• Decision Support and Business Intelligence
• Google Analytics
• Integrated Deep Learning and Big Data
• MATLAB SQL
EDUCATION
Masters of Science, Analytics,2022
Northeastern University - Boston
Bachelor of Engineering, Information Technology, 2011 University of Mumbai - Mumbai, MH
ACADEMIC PROJECTS
• Sentiment Analysis on Twitter: As a data scientist, extracted subjective information to understand the social sentiment using Python.
• Data Analysis on Reviews posted by Wine Enthusiasts: As a Data Analyst, Performed Exploratory Data Analysis and performed correlation between the variables to understand the statistical hypotheses using R.
• Impact of Seasonality and Weather on Automobile Rentals: As a Data Scientist, created data visualization dashboards using Tableau and Qlik based on the analysis of results for increased revenue of rentals for automobile and created reports using Google sheets.
• Building a Prediction Model: As a Data Engineer, employed machine learning algorithm and multi-variate data analysis models for simulation and predicting the victim race to
prevent future victimizations.
Boston, MA 02115
https://www.linkedin.com/in/prithvirshetty/
617-***-****
adoz55@r.postjobfree.com

"
,
Data Scientist Polytechnic,"
Apply mathematical theories and techniques to the solution of practical
problems in business, engineering, the sciences, or other fields. Develop computational methods for solving problems that occur in areas of science and engineering or that come from applications in business or industry.
Fit theory to data, optimize methods, and solve disparate systems. MALACHI DEMMIN
Mathematician + Data Scientist
adoxkj@r.postjobfree.com
MalachiDemmin.com
714-***-****
Experience Director of Data Science
BullyingCanada + BullyingInternational
January 2020 - Present
Leads forecasts for a nonprofit organization by utilizing web analytics and contact center data. Extract structured tidy data from unstructured text logs. Modeling Lead
Belfort Group + Merck MilleporeSigma
June 2021 - August 2021
Worked with researchers to fit DLVO theory to Data obtained from Atomic Force Microscopy measurements. Developed Code in Matlab using regression for the fitting. VP for Data Science
Time for Homes
April 2020 - February 2021
Used modeling and applied mathematics to help define the homelessness problem and explore potential solutions.
Used R and Tableau to create data analysis on a large Data set, including creating novel visualizations for high level discussion.
Education Masters, Applied Mathematics
Rensselaer Polytechnic Institute
August 2019 to May 2021
Course Highlights: Computational Optimization, Numerical Solutions of Partial Differential Equations, Linear and Conic Optimization, Machine Learning and Optimization, Computational Linear Algebra Bachelors, Applied Mathematics
California State Polytechnic University, Pomona
August 2015 to May 2019
Dean’s List (Four Times), Mathematical Competition in Modeling Honorable Mention. Course Highlights: Graph Theory, Survival Data Analysis Skills AMPL Level: Skillful
Experienced with coding optimization algorithms using CPLEX and minos. LaTeX Level: Experienced
Experienced with typesetting multiple types of documents with Equations and Figures Matlab Level: Experienced
Experienced in programming Matlab techniques in Modeling, Optimization, and Partial Differential Equations. Python Level: Skillful
Experienced writing implementations of Machine Learning and Data Visualization using NumPy, Keras, Pandas, and Tensorflow.
R Level: Skillful
Experienced with conducting Data Visualization for additional insight using the ggplot, leaflets, and tmap. SQL Level: Skillful
Experienced in utilizing SQL to query varied databases and tables in order to extract data for further analysis. Projects and
Presentations
Compressed Sensing 2020
Using wavelet transforms, programed a compressed sensing algorithm to extract a cross sectional image of lungs from compressed data.
Machine Learning, Blendenpik: Randomized Solving of Linear Systems
2020
Utilized randomization to improve currently existing solvers of Linear systems Numerical Solutions to Partial Differential Equations 2020 Implemented the points per wavelength method of grid refinement for solving of the wave equation. Portfolio Optimization 2019
Programmed an algorithm and model to quantify risk and optimize a hypothetical portfolio for highest reward with lowest risk.
Optimal Exam Scheduling 2019
Built a novel program, using AMPL an algorithm, to create an optimal exam schedule where multiple constraints were imposed on the system to meet conditions from the institution's administration. Generalized Ballot Box Problems 2018
Presentation of Ballot Box problems to the American Mathematical Society during the Fall Western Sectional Meeting.
Disease Modeling 2018
Modeled and analyzed the dynamics of spread among separate tiers of population in one space.

"
,
Data Analyst Business,"
Sara Jafari
Jersey City, NJ, *****
201-***-****
adoxa9@r.postjobfree.com

Result- Driven Data Analyst, experience in Finance and Management, eager to contribute to team success through hard work, excellent organizational skills. Motivated to learn, grow, and excel in Data Sciences
SKILLS
Expert of the following tools: Python, Java, SQL,HTML,CSS,Javascript, R, STATA, EViews.
Professional in Excel, Power Point, Word, and SAP
PROFESSIONAL EXPERIENCE

Intellipaat Jersey City, NJ Intern, Jan 2020 - Present
Currently attending Data Scientist Certificate program:
Project1: Increased cross-selling of All-Mart, by using association mining in R and find the top 5 rules related to the number of their most frequently used items.
Project2: Selected the most appropriate clients for loan approval at ATM bank by running linear regression model in Python.

Publicis Group Paris, France
Financial Analyst Sep 2017-Sept 2019
Retrieved, analyzed performance data, and presented financial reports to leadership team regarding revenue of products such as Nestle, L’Oréal, and Renault
Generated invoices to agencies using Hyperion Financial Management (HFM)
Developed charts to display financial status of the organization and variance of actual and forecasted results
Monitored and supervised the accuracy of timesheet using SAP

Jafcom Telecommunication Jersey City, NJ
Business Analyst Jun 2015 – Dec 2016
Gathered and organized crucial material project information in the advertising database
Analyzed client’s data through implementation of tools such as Excel spreadsheet to interpret added value
Improved social networks such as LinkedIn, Facebook, and Instagram

BNP Paribas Bank Paris, France
Financial Analyst Sep 2013 – Sep 2014
Investigated new models to address performance issues and profit & loss measurements
Created weekly analyses of investment portfolios and escalated them to Finance leads
Registered trade requests features in Murex and Power booking system
Evaluated financial key figures of the trades and presented the results to managers

HSBC Bank Paris, France
Intern Jul 2012 – Sep 2012
Managed user inquiries regarding services assurance of hardware incidents
Assisted users with acquiring information from manuals
EDUCATION

SKEMA BUSINESS SCHOOL La Defense, Ile de France
M.A. Economics (Jun 2018)
RUTGERS UNIVERSITY-NEWARK Newark, NJ
M.A. Economics (May 2012)
PARIS WEST UNIVERSITY Nanterre, Ile de France
B.A. Economics and Management (Jun 2010)

"
,
Data Scientist,"
About Me
• * Years in Data Science
• ** years in Information Technology
• Expertise in Machine Learning, Deep Learning, Natural Language Processing, and Data Analytics
• Projects involving Sentiment Analysis, Fraud Detection, Predictive Analytics, Artificial Intelligence
• 8 years of Python development Summary
• Extensive work in Natural Language Processing and Predictive Analytics using Machine Learning Algorithms, Visualization Tools, and Web Deployment Technologies.
• Used Neural Networks, Trees, Clustering Algorithms, and Statistical Models to propel systems which perform Sentiment Analysis, Fraud Detection, Client Segmentation, Predictive Maintenance, Demand Forecasting.
• Business understanding, Data understanding, Data preparation, Modeling, Evaluation and Deployment.
• Experienced in practical application of data science to business problems to produce actionable results.
• Experience in Natural Language Processing (NLP), Machine Learning & Artificial Intelligence.
• Experience with AWS, Kubernetes, and Azure cloud computing.
• Spark (especially AWS EMR), Kibana, Node.js, Tableau.
• Able to incorporate visual analytics dashboards.
• Experience with a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction.
• Knowledge on Apache Spark and developing data processing and analysis algorithms using Python
• Programming strength in Python, C#, C++, Java, SQL, R, Matlab, Mathematica, Javascript
• Use of libraries and frameworks in Machine Learning such as NumPy, SciPy, Pandas, Theano, Caffe, Sci-Kit Learn, Matplotlib, Seaborn, TensorFlow, Keras, PyTorch, NLTK, Gensim, Urllib, Beautiful Soup
• Ability with algorithms, data query and process automation.
• Evaluation of datasets and complex data modelling.

TECHNICAL SKILLS
LEADERSHIP - Push project goals, determine business use cases, and mentor/lead teams

QUALITY - Continuous improvement in project processes, workflows, automation and ongoing learning and achievement CLOUD Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud)

CLOUD - Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud)

ANALYTICS - Data Analysis, Data Mining, Data Visualization, Statistical Analysis, Multivariate Analysis, Stochastic Optimization, Linear Regression, ANOVA, Hypothesis Testing, Forecasting, ARIMA, Sentiment Analysis, Predictive Analysis, Pattern Recognition, Classification, Behavioral Modeling

PROGRAMMING LANGUAGES - Python, R, SQL, Java, MATLAB, Mathematica, C#, C++, Javascript

LIBRARIES - NumPy, SciPy, Pandas, Theano, Caffe, SciKit-learn, Matplotlib, Seaborn, Plotly, TensorFlow, Keras, NLTK, PyTorch, Gensim, Urllib, BeautifulSoup4, PySpark, PyMySQL, SQAlchemy, MongoDB, sqlite3, Flask, Deeplearning4j, EJML, dplyr, ggplot2, reshape2, tidyr, purrr, readr, Apache, Spark, MapReduce, WPF, Entity Framework Core, Node.js

DEVELOPMENT - Git, GitHub, GitLab, Bitbucket, SVN, Mercurial, Trello, PyCharm, IntelliJ, Visual Studio, Sublime, JIRA, TFS, Linux, Unix

DATA EXTRATION AND MANIPULATION - Hadoop HDFS, Hortonworks Hadoop, MapReduce, Cloudera Hadoop, Cloudera Impala, Google Cloud Platform, MS Azure Cloud, SQL, NoSQL, Data Warehouse, Data Lake, SWL, HiveQL, AWS (RedShift, Kinesis, EMR, EC2, Lambda)

NATURAL LANGUAGE PROCESSING - Document Tokenization, Token Embedding, Word Models, Word2Vec, FastText, Bag Of Words, TF/IDF, Bert, Elmo, LDA.

MACHINE LEARNING - Supervised Machine Learning Algorithms (Linear Regression, Logistic Regression, Support Vector Machines, Decision Trees and Random Forests, Naïve Bayes Classifiers, K Nearest Neighbors), Unsupervised Machine Learning Algorithms (K Means Clustering, Gaussian Mixtures, Hidden Markov Models, Auto Encoders), Imbalanced Learning (SMOTE, AdaSyn, NearMiss), Deep Learning Artificial Neural Networks, Machine Perception

APPLICATIONS - Machine Language Comprehension, Sentiment Analysis, Predictive Maintenance, Demand Forecasting, Fraud Detection, Client Segmentation, Marketing Analysis

WORK EXPERIENCE

Gilead Sciences Nov 2020 – Present
Senior Data Scientist/ NLP Engineer I Foster City CA
Gilead is a major healcare company. As the Senior Data Scientist in my team I was tasked to create a semantic search system using Amazon kendra and tensorflow. The main issue is that there were many unlabeled documents. Used AWS sagemaker and AWS Comprehend to build a semi supervised model and label the incoming data. The business team wants to create a semantic search application using labels for filters so I used Kendra, Gensim and Tensorflow to train an AI algorithm to predict labels and used REGEX to extract pertinent data from documents. Also implemented Causal analysis to determine event causation using Topic modeling with Gensim on EET events.
• created a POC of 50 records using labeled data to see if AI can predict correctly
• Planned AI architecture by building using LSTM and CNN
• Used LSTM to predict lables
• Used CNN to predict labels
• Compare 2 neural net performance to determine which is better
• CNN performed better due to lack of training data
• Created POC of 50 records request for more labeled data was placed
• POC 2 was created off of 400 records and performance was tested
• Build Topic modeling using Gensim for external event tracking
• Used AWS comprehend to extract dates and research sites
• Used regex to extract import keywords and perform multi-variate statistical analysis
• Created text summerization using HuggingFace transformers
• Created Text Rank application using Page rank and cosine similarity
• Created TextHero NLP topic modeling to show difference between title and description topics
• Created and managed gitub repo for Data Science EET and Data Science semantic search projects.
• Instructed team members in Data Science tools such as sagemaker, comprehend and gensim
• Explained technical AI work to non-technicall stake holders
• Pickled models and deployed sagemaker pipelines
• Operationilize AI Models using AWS tools such as AWS Batch, EKS and AWS Lambda.
• Deployed ML Pipeline monitoring and Orchestration using Apache Airflow and MLFlow
•
Technologies Used: Python, TensorFlow, NoSQL, Docker, Kubernetes, AWS, NumPy, Pandas, Matplotlib, SciKit-Learn, SciPy, Plotly, Jupyter Notebook, FeatureTools, HBase, Hadoop, Spark, PySpark, HappyBase, NLTK, Gensim, AWS Sagemaker, AWS Batch, Apache Airflow, EKS, Kubernetes, REGEX, MLFlow

LOVINGLY Dec 2019 – Oct 2020
Lead Data Scientist/NLP Engineer New York, NY
Lovingly is a major flower and gift retailer with a strong On-line presence. As part of the direct to consumer strategy, Lovingly implemented a series of chatbot and nlp initiatives. I worked with a team to implement a high quality human interactive system using NLP and other Deep Learning techniques. The system consisted of a chatbot to be implemented on the company’s main webpage. The chatbot was built upon an bi-directional LSTM and other Transformer based model to establish connection between the users queries and the application knowledgebase.
• Worked in an environment using Python, NoSQL, Docker, AWS, and Kubernetes.
• Worked with the Python packages NumPy, Pandas, SciPy, Matplotlib, Plotly, and FeatureTools for data analytics, cleaning, and feature engineering.
• Used NLTK and Gensim for NLP processes such as Tokenization and for creating custom Word Embeddings.
• Imported from Python’s Tensorflow package for building Neural Network models.
• Docker was used to contain the model for use in applications.
• Worked closely with Data Scientists, Data Engineers, App Developers, and Vehicular Subject Matter Specialists.
• Consulted with the Compliance Department to determine relevant use cases.
• Kept up to date with the latest NLP tech and methodologies, reading and discussing about 5 articles and papers per week.
• Employed numerous different models, including Convolutional and Recurrent Neural Networks, LSTM, and Transformers which all utilized Word Embeddings.
• Performed data profiling on available data sources to identify potentially useful data sources for the proposed machine learning use cases.
• Models which were operationalized were deployed to a RESTful API using the Python Flask package and Docker containers.
• Used Agile approaches, including Extreme Programming, Test-Driven Development, and Agile Scrum.
Technologies Used: Python, TensorFlow, NoSQL, Docker, Kubernetes, AWS, NumPy, Pandas, Matplotlib, SciKit-Learn, SciPy, Plotly, Jupyter Notebook, FeatureTools, HBase, Hadoop, Spark, PySpark, HappyBase, NLTK, Gensim

BMO Harris, Bank May 2017-Nov 2019
Data Scientist Berkeley Heights, NJ
BMO is a major player in the banking and financial business. I worked with their Cyber Security and Fraud departments to improve their credit/debit card fraud detection using Machine Learning Algorithms. My primary function was to design a Data Driven model which detected all cases of fraud while limiting false-positives. As a secondary responsibility, I assisted in deployment of the model on IBM Mainframes.
• Worked using Python, SQL, and R.
• Used the Python packages NumPy, Pandas, and Seaborn for data analytics and feature engineering.
• A very large, proprietary feature set was engineered based on the large pool of data collected by Bank of America.
• Pulled data from their enormous SQL DataBase using MS SQL for presentable analysis with Tableau.
• Utilized Apache PySpark modules for retrieving data from the SQL database inside of Python modules.
• A Tree Based Model built with XGBoost was used to classify cases as either suspicious or safe. Appropriate action was taken for all suspicious behavior.
• Used from Python’s PyTorch package for building proposed Neural Network models which extrapolated better than the Tree baseline.
• Unsupervised Gaussian Mixture models helped to identify anomalous data points.
• Used an ensemble method to combine the results of supervised and unsupervised models.
• Model Recall was not allowed below a threshold determined by domain experts.
• Worked closely with their Fraud and Cyber Security departments.
• Worked with a team of four other specialists.
• Helped hand over the product for deployment in IBM Mainframes with my pickled serialized model.
• Performed cross validation and model selection using k-folds, train-validate-test, and information theoretic criteria.
• Unearthed raw data by doing Exploratory Data Analysis and Data Visualization using Pandas, Numpy, Seaborn, and Matplotlib.
• Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, and Naive Bayes.
Technologies Used: Python, PyTorch, XGBoost, MSSQL, PySpark, Spark, Azure, NumPy, SciPy, Pandas, SciKit-Learn, FeatureTools, Seaborn, Tableau, Jupyter Notebook

KeyMe Robotics May 2014-January 2017
Data Scientist/Computer Vision Specialist New York, NY
Key Me is as robotics dedicated start-up operating in the specialty retail space. I worked to create implemenmt and deploy a computer vision system to identify and quality control replacement key manufacture using a computer controlled compact manufacturing system. Utilized Convolutional Neural Networks and Open CV to match manufacturers with blacks and measure QC metrics for completed product. In order to maintain standarsds the visual classification was maintained at a minimum of 94% accuracy.
• Worked in an Ubuntu environment using Python, SQL, NoSQL, and AWS.
• Scraped articles and posts from the internet using Scrapy.
• Worked with the Python packages NumPy, Pandas, SciPy, Matplotlib, Plotly, and FeatureTools for data analytics, cleaning, and model feature engineering.
• Hadoop HDFS used for retrieval from the constructed NoSQL database of posts and reviews.
• Made use of Pandas, SQLite, and MySQL to create and maintain a database of parsed complaints on AWS.
• Utilized SQLite (sqlite3) modules for retrieving data from the SQL database.
• Used NLTK and Gensim for NLP processes such as document Tokenization and for creating custom Word Models (though FastText generalized best).
• Imported from Python’s PyTorch package for building Neural Network models.
• Worked closely with two Data Engineers and their Consumer Relations team.
• Employed numerous different models, including Convolutional and Recurrent Neural Networks, LSTM’s, and Transformers.
• Provided publication quality documentation for all internal and proprietary software packages.
• Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.
• Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.
Technologies Used: Python, PyTorch, MySQL, NoSQL (Hadoop HDFS), AWS, NumPy, Pandas, Matplotlib, SciKit-Learn, SciPy, Plotly, Jupyter Notebook, FeatureTools, SQLite, NLTK, Gensim, FastText, Scrapy, Selenium, RSelenium

Longi Engineering February 2012 -January 2014
Machine Learning Engineer/ Data Analyst New York, NY
Longi Engineering is a large Engineering Firm serving the Public and Private Sector, clients include The New York City Public Transit System and The New York State Depratment of Transportation among others. In order to better serve their clients Longi started a information technology branch as well as apredictive analytics department. Duties included using time series models to predict the usage of transit auithority services as well as demand forecasting modeling using Arima and its variants .
• Worked in an Ubuntu environment using Python, SQL, NoSQL, and AWS.
• Scraped articles and posts from the internet using Scrapy.
• Worked with the Python packages NumPy, Pandas, SciPy, Matplotlib, Plotly, and FeatureTools for data analytics, cleaning, and model feature engineering.
• Hadoop HDFS used for retrieval from the constructed NoSQL database of posts and reviews.
• Made use of Pandas, SQLite, and MySQL to create and maintain a database of parsed complaints on AWS.
• Utilized SQLite (sqlite3) modules for retrieving data from the SQL database.
• Used NLTK and Gensim for NLP processes such as document Tokenization and for creating custom Word Models (though FastText generalized best).
• Imported from Python’s PyTorch package for building Neural Network models.
• Worked closely with two Data Engineers and their Consumer Relations team.
• Employed numerous different models, including Convolutional and Recurrent Neural Networks, LSTM’s, and Transformers.
• Provided publication quality documentation for all internal and proprietary software packages.
• Setup storage and data analysis tools in Amazon Web Services cloud computing infrastructure.
• Updated Python scripts to match training data with our database stored in AWS Cloud Search, so that we would be able to assign each document a response label for further classification.
Technologies Used: Python, PyTorch, MySQL, NoSQL (Hadoop HDFS), AWS, NumPy, Pandas, Matplotlib, SciKit-Learn, SciPy, Plotly, Jupyter Notebook, FeatureTools, SQLite, NLTK, Gensim, FastText, Scrapy, Selenium, RSelenium

EDUCATION

Bachelors of computer information systems security
Devry college of New York

Masters of Computer Science
Pace University

Graduate Certificate- Full Stack Developer
Columbia University

Post Graduate Certificate- Data Science and Machine Learning
Flatiron School

"
,
Tapiwa Chadenga - Data Scientist,"
Professional Summary
Senior data scientist with over 10 years of Data Science experience.
Experience in the application of machine learning techniques including Naïve Bayes, Linear and Logistic Regression Analysis, Neural Networks, RNN, CNN, Transfer learning, Time-Series analysis, trees and Random Forests. Experience in statistical models on, big data sets using cloud/cluster computing assets with AWS and Azure.
An accomplished data scientist who brings a strong working knowledge of machine learning / artificial intelligence techniques and rigorous statistical methods to bear on a variety of real-world business problems to yield lean, actionable results and insights for improvement.
Highly organized and efficient individual whose leadership and thorough, precise approach to projects has yielded excellent results.
Ability to perform exploratory analysis on varying types of data and datasets, allowing for a full knowledge of the subject matter, a nuanced understanding of the variables in question, and a technically sound insight into the required modeling approach.
Strong background of working with advanced analytical teams to design, build, validate and refresh data models.
Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
Highly perceptive with other people, allowing for a strong ability to facilitate a dynamic and constructive team environment.
Ability to quickly gain a keen understanding of niche subject matter domains via research and communication with all parties involved. Ability to design and implement effective novel solutions to be used by other subject matter experts.
Very deep understanding of neural networks, including convolutional and recurrent architectures, as well as unsupervised approaches, such as auto encoders or Restricted Boltzmann Machines
Strong proficiency with TensorFlow for building, testing, validating, selecting, and deploying successful and reliable machine learning algorithms using Python.
Strong proficiency with NumPy stack (NumPy, SciPy, Pandas, and Matplotlib)
Experience with automated data collection from online sources using BeautifulSoup and RoboBrowser (Python) or RSelenium, RCurl, Curl, HTTR, and RVest (R ).
Creation of professional quality, duplicatable, and automated reporting systems using R with LaTeX and markdown via the Knitr package, as well as Shiny and Plotly for interactive web dashboards.
Experience in ensemble meta-algorithm techniques, including Bagging, Boosting, and Stacking for creating extremely powerful predictive models.
Strong ability to communicate with stakeholders and project managers to gather requirements and act as an intermediary between development team and stakeholders to set and guide realistic expectations, while ensuring the product envisioned is brought to fruition.
Expertise in Natural Language Processing (NLP) methods, such as word2vec, BERT, sentiment analysis, named entity recognition, and part of speech tagging.

Technical Skills

Deep Learning
Convolution Neural Networks (CNN), Recurrent Neural Network (RNN), Multilayer Perceptional Neural Network (MLPNN), Data Mining, Machine Learning Algorithms, Long Short-Term Memory (LSTM), Support Vector Machines (SVM), and Random Forest (RF).

Analysis Methods
Advanced Data Modelling, Forecasting, Statistical, Sentiment, Stochastic, Bayesian analysis, Regression analysis, Linear models, Multivariate analysis, Sentiment analysis, Big Data, and clustering.

Analysis Techniques
Classification and Regression Trees (CART), Gradient Boosting Machine (GBM), TensorFlow, PCA, Regression, Naïve Bayes.

Data Modeling
Bayesian Analysis, Statistical Inference, Predictive Modelling, Linear Modelling, Probabilistic Modelling, Time-Series Analysis.
Deployment
Azure, AWS, Docker, Kubernetes, Jenkins,
NLTK, Spacy, Genism, BERT, Elmo

IDE
Jupyter Notebook, Spyder, RStudio, Google Colab

Analytic Development
Python, R, IDL, SAS, SQL, MatLab.

Packages and Visualizations
Numpy, Pandas, SciPy, TensorFlow, PyTorch, Keras, Theano, Caffe, Matplotlib, Seaborn, Ggplot, Tableau, and Plotly.

Machine Learning
Supervised and unsupervised algorithms, Natural Language Processing, Image Recognition and Detection, Forecasting, Linear Regression, Lasso and Ridge, Logistic Regression, Ensemble classifiers (Bagging, Boosting, and voting), KNN, Naïve Bayes Classifier, Clustering (k-means, DBSCAN), PCA, SVD, ARIMA, Decision Trees.

Artificial Intelligence (AI)
Natural Language Processing (NLP), Text understanding, classification, Pattern Recognition, computer vision, embeddings (BERT, ELMO, Skip-gram), Encoder-Decoder, Sentiment analysis, and Latent Discriminant Analysis (LDA).

Version Control
GitHub, Git

Soft Skills
Excellent communication and presentation skills; ability to work wee with stakeholders to discern needs accurately; leadership, mentoring, and coaching

Professional Experience
Position: Principal Data Scientist/ML-Ops Engineer
Company: Vision 13
Location: Lindstrom, MN, USA September 2020 - Present

Project Situation
Vision 13 is a startup involved in the financial markets. The firm focuses on developing systems that automate the discovery and execution of short-term investment strategies in real time. I have been involved implementing a variety of algorithms for machine learning, NLP, time-series analysis, feature processing, and data visualization. My role as Principal Data Scientist was to design the architecture and assign tasks to the team comprised of Data Engineers, Modelers, and Dev-Ops. I also act as the ML-Ops Engineer and System Architect.

Hands-on work:
Worked on modelling and development employing machine learning (ML), deep learning (DL) and natural language processing (NLP) in Artificial Intelligence (AI) applications.
Used Python libraries such as Numpy and Pandas to store and access data.
Created an ML pipeline and used orchestration tools such as MLFlow and Apache Airflow.
Deployed in real time using AWS EKS (Elastic Kubernetes Service).
Developed Batch Processing Forecasts using AWS BATCH and Sagemaker.
Developed features and capabilities to make experimentation faster and cheaper to run for the Data Science and Analytics division.
Developed real-time data processing systems that turn unstructured data from different sources such as Facebook, Amazon, credit bureaus, and banking institutions into factors that inform firm's investment and risk models.
Developed a framework for the firm's Big Data platform that simulates the impact of different strategies on the company's investment portfolio.
Developed new machine-learning models and strategies for other use cases, including evaluation of firm's exposure to market risk, liquidity risk, and concentration risk.
Applied automation of model performance and monitoring, as well as ran tests as required.
Handled model containerization and deployment on AWS infrastructure.
Utilized machine learning algorithms to automatically track and monitor regulatory changes as they appear.

Position: Senior NLP Engineer
Company: CNN
Location: Atlanta, GA February 2018 – September 2020

Project Situation
At CNN, I worked as a Machine Learning Engineer and Data Researcher. I was a part of a team that built, trained, and tested a Natural Language Processing model which took in comments and tweets, and classified if they contained valuable information, and then extracted that information. The goal of the project was to increase the collection of reports and determine if it was news, if it was worth reporting, and if it was worth sending a news crew to further report on.

Hands-on work:
Used Python, R, and SQL to collect, explore, and analyze the comments/tweets.
Used Python and NLTK to tokenize and pad comments/tweets and vectorize.
Vectorized the documents using Bag of Words (BoW), TF-IDF, and N-Grams to feed to the models.
Created and trained an Artificial Neural Network with TensorFlow on the tokenized comments/tweets.
Performed sentiment analysis by utilizing ANNs, RNNs, LSTMs, and Transformers.
Involved in model deployment using Flask with a REST API deployed on AWS Elastic Beanstalk.
Built a REST API that could ingest JSON data and run a machine learning model on it.
Wrote extensive SQL queries to extract data from the MySQL database hosted on Amazon RDS.
Built a deep learning model for text classification and analysis.
Performed classification on text data using NLP fundamental concepts, including tokenization, stemming, lemmatization, and padding.
Used the DeepLearning4j deep learning library to create deep learning models such as LSTMs and Multi-Task Feed Forward Neural Networks.
Performed EDA using Pandas’ library in Python to inspect and clean the data.
Visualized the data using Matplotlib and Seaborn.
Explored using word embedding techniques such as Word2Vec, FastText, GloVe, and Bert.
Worked with Scikit-Learn for basic Proof of Concept (PoC) development.
Built an ETL pipeline that could read data from AWS S3, process it using AWS Glue, and store the processed data on AWS RDS.
Automated ETL tasks and scheduling using AWS Lambda functions.

Position: Data Scientist/CV Specialist
Company: Technosoft
Location: Southfield, MI July 2015 – February 2018

Project Situation
Route is a SaaS product for commercial cleaning companies. To supplement their application, an Object Detection model was created that could identify the type and number of “fixtures” at a location based on a camera phone photograph. In the cleaning space, a “fixture” is any item in a space that needs to be cleaned. These items could be chairs, couches, tables, computer systems, desks, etc. The model was deployed on edge devices using Tensorflow-Lite.

Hands-on work:
Developed a custom dataset for fine-tuning a deep neural network.
Fine-tuned a variety of image models with object detection heads.
Used both Single Shot Detection (SSD) and You Only Look Once (YOLO) object detection models.
Deployed finished model on edge devices using Tensorflow-Lite.
Built various statistical models Statistical algorithms involving Time-Series analysis, Survival Analysis, Multivariate Regression, Linear Regression, Logistic Regression and PCA in financial projection.
Led the development of the expected profit projection engine by applying machine learning with financial engineering actuarial science.
Performed inforce management, including survival analysis, churn/retention analysis, and risk identification.
Used pre-trained models to visualize the feature maps in the intermediate layers and performed transfer learning.
Used pre-trained models (VGG16, ResNets, Inceptions, DenseNet, U-Net, etc.) for transfer learning on small datasets.
Designed and implemented the enterprise Financial Value-at-Risk model.
Led various cross-department projects and worked closely with internal stakeholders such as business teams, product managers, engineering teams.

Position: Jr Machine Learning Engineer
Company: Santander Bank
Location: New York, NY January 2014 – June 2015

Project Situation
Santander Bank is a diversified financial business that helps people and businesses prosper and is in the Northeast region of the United States. I was part of a small team of data scientists that worked with the Security Department tasked with fraud detection. My duties included liaising between the US based team and the overseas team. I applied an ensemble of classification and unsupervised models to identify fraudulent activity within the incoming transactions. Once deployed, we were able to reduce the number of incorrectly identified fraudulent charges and save our customers time and money.

Hands-on work:
Stratified imbalanced data to ensure fair representation of the minority data in all data sets used for cross validation of the model.
Consulted with regulatory and subject matter experts to gain clear understanding of information and variables within data streams.
Utilized Cloud computing resources for model optimization/tuning of hyperparameters, and cross-validation of statistical data science models.
Used R’s Dplyr for data manipulation, as well as Ggplot2 for data visualization and EDA.
Utilized Scikit-Learn, SciPy, Matplotlib, and Plotly for EDA and data visualization.
Built Artificial Neural Network models to detect anomalies using PyTorch and Scikit-Learn.
Used Scikit-Learn’s model selection framework to preform hyper-parameter tuning using GridSearchCV and RandomizedSearchCV algorithms.
Developed Unsupervised K-Means and Gaussian Mixture Models (GMM) from scratch in NumPy to detect anomalies.
Employed a heterogeneous stacked ensemble of methods for the final decision on what transaction was fraudulent.
Deployed model using a Flask app stored in a Docker container.
Evaluated the performance of our model using a confusion matrix, accuracy, recall, precision, and F1 score. Took careful consideration of the recall score.
Utilized Git for version control on GitHub to collaborate work with the team members.
Worked on customer segmentation using an unsupervised learning technique clustering.

Position: Machine Learning Engineer
Company: MTN, Cape Town, South Africa
Location: City, State May 2011 – December 2013

Project Situation
MTN is a leading telecom company in Africa. MTN operates a portal for thousands of employees around the world. The portal included access to a repository of documents. MTN needed a tool to search for documents from a repository of files. The tool needed to take into account the format of the document (i..e.,.pdf, video, Word, etc.).

Hands-on work:
Used Tf IDF, Cosine Similarity, and RF.
Performed data cleaning and implemented imputation.

Education
University of Cape Town - BSc. Mathematics of Computer Science (Hons.).
Diverse program spanning discrete and pure mathematics, computer science and statistics

University of Cape Town - BCom. Financial Analysis and Portfolio Management (Hons.).
Intensive postgraduate program introducing advanced tools for pricing financial assets and constructing objective driven portfolios

Solusi University – Bba. Business Administration, Finance.
Fundamentals of economics, introduction to banking and advanced finance

Bindura University of Science Education - BSc.Ed., Mathematics, Geography, Education.
Introduction to advanced mathematics, geography, and pedagogy of education.

"
,
Data Scientist,"
PROFESSIONAL PROFILE SUMMARY
Data Scientist with 9 years of experience in Artificial Intelligence, Machine Learning, Deep Learning and Robotics providing analytics and custom development for specific business use cases. Combines Skills in mathematics with analytics with hands-on development of machine learning algorithms, deep learning, and data modeling to derive innovative solutions to enhance performance, productivity, and quality of deliverables in any industry.

• Experience in the application of Naïve Bayes, Analysis, Neural Networks/Deep Neural Networks, and Random Forest machine learning techniques.
• Creative thinking/strong ability to devise and propose innovative ways to look at problems by using business acumen, mathematical theories, data models, and statistical analysis.
• Discover patterns in data using algorithms and use experimental and iterative approach to validate findings.
• Advanced statistical and predictive modeling techniques to build, maintain, and improve on real-time decision systems.
• Work with product managers to productizing algorithms and solutions.
• In-depth knowledge of statistical procedures that are applied in both Supervised and Unsupervised Machine Learning problems.
• Machine learning techniques to marketing and merchandizing ideas.
• Advanced analytical teams to design, build, validate, and refresh data models.
• Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
• Ability to quickly gain an understanding of niche subject matter domains, and design and implement effective novel solutions to be used by other subject matter experts.
• Experience implementing industry standard analytics methods within specific domains and applying data science techniques to expand these methods, for example, using Natural Language Processing methods to aid in normalizing vendor names, implementing clustering algorithms, and deriving novel metrics.

TECHNICAL SKILLS
Highly analytical and detail-oriented data scientist with strong hands-on programming experience in solving business problems through data and technologies.

Machine Learning Methods
Applying classification, regression, prediction, dimensionality reduction, and clustering to problems, predictions and analytics that arise in retail, manufacturing, and market science.
Linear Regression, Logistic Regression, Random Forest, K-Nearest Neighbors, Deep Learning in Python.

Deep Learning Methods
Artificial Neural Networks, Gradient Descent variants (including ADAM), Regularization Methods, and Training Acceleration with Momentum Techniques
TensorFlow, PyTorch, Keras.

Artificial Intelligence
Text understanding, classification, pattern recognition, recommendation systems, targeting systems, ranking systems and analytics.

Statistical Analysis
A/B Testing, ANOVA, T-Test, Model Selection, Anomaly Detection, Case Diagnostics, Feature Selection in R or Python for analysis of data.

Analytics
Research, analysis, forecasting, and optimization to improve the quality of user-facing products, Predictive Analytics, Probabilistic Modeling, Approximation Inference.

Areas of Interest and Experience
Deep Learning, Representation Learning, Recommender Systems, Machine Learning for Robotics, Strategic Planning, Analysis.

Analytic Scripting Languages
Python, R, Matlab.

Programming Languages
Analytic programming using Python, NumPy, Pandas, Matplotlib, SciKit-Learn, TensorFlow, PyTorch, Keras.

Database, Query, Data Cleaning, and Normalization
PostgreSQL, MySQL, SQL Server, RDS, RedShift, MongoDB, DynamoDB, MS Excel, MS Access.

IDEs
RStudio, PyCharm, Visual Studio, Visual Studio Code, Jupyter Notebook, Sublime.

PROFESSIONAL EXPERIENCE
Senior Data Scientist
Credit Suisse
New York, NY
Sep 2020-Present

This Investment Portfolio project centers on Natural Language Processing and Time-Series Analysis for Investment Portfolio Management. The project involves creating a data model to analyze news in predictive analytics relating to stock trending over 6-month periods. This analysis is used to re-balance stock portfolios.

• Exploring various algorithmic trading theories and ideas.
• Applying data mining and optimization techniques in B2B and B2C industries and Machine Learning, Data/Text Mining, Statistical Analysis, and Predictive Modeling.
• Utilizing PySpark Python modules for machine learning and predictive analytics in Hadoop on AWS.
• Applying predictive modeling using state-of-the-art methods.
• Implementing advanced machine learning algorithms utilizing Caffe, TensorFlow, Scala, Spark, MLLib, R, and other tools and languages needed.
• Programming and scripting in R, Java, and Python.
• Developing Data Dictionary to create metadata reports for technical and business purposes.
• Building reporting dashboard on the statistical models to identify and track key metrics and risk indicators.
• Performing Boosting method on predicted model to optimize efficiency of the model.
• Extracting source data from Amazon Redshift on AWS Cloud platform.
• Parsing and manipulating raw, complex data streams to prepare for loading into an analytical tool.
• Exploring different regression and ensemble models in machine learning to perform forecasting.
• Developing new financial models and forecasts.
• Improving efficiency and accuracy by evaluating models in R.
• Utilizing TensorFlow to design deep learning models.
• Applied Machine Learning (ML), deep learning (DL) and natural language processing (NLP) in Artificial Intelligence (AI) applications.
• model training
• Defining the source to target data mappings, business rules, and data definitions.
• Performing an end-to-end Informatica ETL Testing for custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.

ML Engineer
Pima Realty
Tucson, AZ
Oct 2018-Aug 2020

I built a predictor that can estimate the price that a house will sell for based only on the house’s location in latitude and longitude. I cleaned the data set and selected the important features; specifically, I took only the sold price, square feet, and latitude and longitude columns. I converted the latitude and longitude values to radians as a way of normalizing the location data and attempted to use only the location to predict price as well as price per square foot. I also randomly split the data into training and testing; 80% for training and 20% for testing. I then used a KNN regressor with K = 10. This choice of K worked well for both the testing set and the test on outside data. I defined my own metric to evaluate the model: average percent difference of my predictions from the true price value. Using this metric, I found that predicting price per square foot worked better than predicting just the price. The model was deployed on REST API housed on an AWS EC2 instance.

• Performed EDA on data integrated from various public and private sources
• Established an input pipeline that included Normalization. Imputation, and de-noising.
• Random Train/Test Split.
• Applied KNN Regression.
• Applied Constructed Metric.
• Validated and Tested on Outside Data.
• Used Python for the Clustering Analysis and NetLog for simulation.
• Performed data exploration using statistical methods and visual packages from Python.
• Utilized pre-processed datasets that NetLog produced during the simulation phases and past datasets.
• Implemented a variety of Clustering models with the pre-processed data to classify population classes.
• Developed a segmentation solution using various Clustering analysis methods, including K-Means clustering, Gaussian mixture models, and DBSCAN.
• Performed exploratory data analysis on socioeconomic class data and plotted correlation between variables.
• Experimented with several classification methods, including decision trees, logistic regression, and KNN.
• Experimented with forecasting methods, including time-series analysis algorithms such as SARIMA.
• Extracted data from a SQL database using complex SQL queries.
• Customized solutions coded using Python utilizing the Tensorflow and Numpy libraries.
• Utilized advanced Machine Learning and Deep Learning models,

Data Scientist
Fanatics,
San Mateo CA
Mar 2016-Sep 2018

Fanatics is an online retailer of licensed sports merchandise for all major professional sports leagues, media brands, and a large number of collegiate teams. To increase their online retail KPIs such as click-through rate (CTR) and the number of items sold, I worked with the Online Sales Department to create a hybrid recommender engine to offer relevant suggestions to visitors. After deployment, the company saw an increase in all relevant KPIs, which resulted in higher profits.

• Used SQLAlchemy to perform queries and pull data from Amazon S3 MemSQL database into Pandas DataFrames in Python.
• Implemented a Singular Value Decomposition (SVD) collaborative filtering algorithm to recommend items to users.
• Explored a K-Nearest Neighbors (KNN) algorithm with Cosine Similarity to perform collaborative filtering.
• Created Flask app API that returns a software agnostic JSON file for software developers to implement in the site.
• Experimented with PySpark’s MLlib for building recommender engines on big data.
• Utilized Scikit-Learn for creating and training collaborative filtering algorithms.
• Evaluated model performance using Click Through Rate (CTR) and Mean Average Precision (MAP).
• Coordinated with many departments such as the Sales department, Data Engineering team, and Software Development team.
• Defined different metrics and indicators for item similarity in the content-based approach.
• Worked with multiple data imputation strategies to uncover unknown user-item ratings.
• Evaluated performance vs existing systems using A/B Testing and Canary Deployment.
• Preformed bi-weekly presentations to stakeholders to show progress toward project goals.
• Used Git version control to manage projects, assign tasks, and report issues.
• Coordinated with the UI/UX team to plan the implementation of recommendations.
• Used technology packages such as NumPy, SciPy, Scikit-Learn, PySpark, MLlib, Pandas, Matplotlib, Seaborn, and Flask.

Data Scientist
Screen Engine/ASI
Century City, CA
Aug 2014-Feb 2016

Screen Engine/ASI is a research and data analytics firm focused on maximizing market potential while assessing risk for clients in the entertainment and media industries.
I applied Sentiment Analysis using NLP techniques: I developed a Sentiment Analysis tool to classify quotes taken from scripts as they appeared on social media. Once quotes and verbal memes were identified as belonging to one property, further analysis was executed with a tool I built to filter twitter feeds and send the related tweets to the target audience groups (followers, fans, etc.). The contents of the input documents were analyzed using Google’s universal sentence encoder to embed the textual data into the appropriate vector space. Three competing models were trained using Gaussian classifiers, ANN, and LSTM to choose the best performance. LSTM was chosen because it yielded the best performance.
• Performed data scraping data using API provided on the web.
• Cleaned and balanced data using Pandas and Numpy.
• Used Google’s universal sentence encoder to embed text.
• Used PyTorch, Tensorflow, and Keras to build an LSTM architecture.
• Used Bag of Words, NLTK, and TF-IDF to analyze text.
• Implemented embedding using Word2Vec, Glove, and Doc2Vec.
• Applied Sentiment Analysis using Elmo, Bert, and ANN classifier.
• Delivered multiple projects in Java, JDBC, SQL and MySQL for intelligent applications.
• Used JavaScript, jQuery, Ajax, Google’s Maps API and fusion tables, HTML, and CSS to deploy online tools for teaching and taking exams.
• Provided detailed estimates for development efforts as needed.
• Conducted customer requirement gathering, functional/technical specifications, release schedule, and project reporting with direct client interaction.
• Designed integrated SDLC processes, applied standard project management methodologies, continuous improvement practices, and quality standards.

Data Analyst
Lumen Technologies
Monroe, LA
Aug 2012-Jul 2014
Constructed models to analyze churn, propensity to buy, segmentation, cross-sell/up-sell, etc.
• Performed advanced analytics using machine learning.
• Collaborated with cross-functional teams to help map exchange data in to a normalized model.
• Built a recommender system to identify optimal products and product groupings for sales offerings to new and existing customers.
• Conducted and interpreted multivariate analyses examples, including regressions with various distributions and duration models.
• Conducted analysis using R and Python on data derived from big data Hadoop systems using distributed processing paradigms, stream processing, and databases such as SQLand NoSQL.
• Updated, maintained, and validated large data sets for derivatives using SQL.
• Conducted root-cause analysis and preventative measures for data quality issues that occurred in day-to-day operations.
• Used advanced data mining, statistical analysis, machine learning and visualization techniques to create solutions to challenging real-world problems.
• Identify and analyze anomalous data (including metadata).
• Worked with diverse data sets, identified, and developed new sources of data and collaborated with product teams to ensure successful integration.

Data Analyst
East Point Media and Consult Ltd.
Lagos, Nigeria
Aug 2009-Jun 2012

East Point Media and Consult is a media strategy consultancy company.

• Prepared regular and ad-hoc financial reports for investments in real estate, media, and agriculture sectors.
• Assisted the Managing Director with creative solutions to various issues via careful analysis.
• Analyzed historical client and transaction data to identify trends that indicated areas of focus/concern.
• Interacted with the Nigerian Bureau of Statistics and analyzed national real estate and agriculture data periodically.
• Provided insights through data analysis resulting in driving more than 25% annual investment portfolio growth with at least 15% annual profit across all years.

EDUCATION
MSc in Electrical Engineering - NYU Poly, New York City, NY

ADDITIONAL TRAINING

FlatIron School - Data Science Immersive Program

"
,
Data Scientist,"
Professional Summary
*+ years of experience as a Data Scientist & Data Engineer

Data Scientist with experience in Artificial Intelligence, Machine Learning, Deep Learning, Data Mining, Predictive Analytics and Decision Science. Specialized knowledge to deal with exponentially growing data. Understands the underlying science of data and applies the same in a diverse set of problem statements in a variety of fields.

• Top Skills Python, R, SAS, AWS, GCP, Spark, Hadoop, SQL, TSQL, Airflow, Rshiny, Python Dash, Tableau
• Automated recurring reports using SQL and Python and visualized them on BI platform like Tableau.
• Experience in building deep learning models, using Natural Language Processing methods to aid in normalizing vendor names, implementing clustering algorithms, and deriving novel metrics.
• Experienced in Building and Deploying Machine Learning and Deep Learning models in AWS Segamaker and GPC Big query.
• Built and deployed Image recognition models using Keras and Tensorflow framework, Text comprehension, Classification, Pattern Recognition, Recommendation Systems, Targeting Systems, Ranking Systems
• Experience in implementation of the Stored Procedures, Triggers, Functions using T-SQL
• Skills to build a fully automated, highly elastic cloud orchestration framework on AWS and GCP and scheduling jobs in AirFlow.
• Predictive Modeling, Data Mining Methods, Factor Analysis, ANOVA, Hypothetical testing, normal distribution and other advanced statistical and econometric techniques.
• Proficient in Tableau and R-Shiny data visualization tools to analyze and obtain insights using large data sets. Created visually powerful and actionable interactive reports and dashboards.
• Experience in developing and analyzing data models. Involved in writing simple and complex SQL queries to extract data from a database for data analysis and testing
• Statistical advance programming language coding skills in R, Python, SAS and cloud platforms such as Azure ML and AWS ML.
• Experience using CUDA/GPU API for real-time image processing.
• Strong SQL programming skills working with functions, packages, and triggers.
• Developed predictive models using Decision Trees, Random Forests, Naïve Bayes, Logistic Regression, Cluster Analysis, and Neural Networks.
• Strong knowledge in all phases of the SDLC (Software Development Life Cycle).
• Knowledgeable in machine learning techniques and algorithms, such as k-NN, Naive Bayes, SVM, Decision Forests, Natural Language Processing (NLP) etc.
• Software development (full SDLC), Agile, and Scrum methodologies.

Skills Summary
Data Science Specialties: Natural Language Processing, Machine Learning, Internet of Things (IoT), Predictive Analytical thinking, Predictive Maintenance, Recommender Systems
Analytical Skills: Bayesian Analysis, Inference, Time-Series Analysis, Regression Analysis, Linear models, Multivariate analysis, Gradient Descent, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Predictive Analytics.
• Experience in stochastic optimization and regression with machine learning algorithms
• Experience formulating and solving discrete and continuous optimization problems.
• Able to research statistical machine learning, supervised learning, and classification methods
• Knowledgeable in implementing technical solutions using machine learning and other advanced technologies.
• Able to create new methods and solutions through a combination of foundational research and collaboration with ongoing initiatives.
• Strong mathematical and statistical modeling and computer programming skills in an innovative manner.
Analytics Tools: Classification and Regression Trees (CART), Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), Principal Component Analysis (PCA), Regression, Naïve Bayes,
Analytic Languages and Scripts: Python, PySpark, Spark SQL, Scala, Java, Ruby, LaTeX, R
Data Integration: SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS)
Languages: SAS, R, Perl, SQL, Python
Version Control: GitHub, Git, SVN
Libraries: ggplot, nltk, numpy, opencv, pandas, scikit-learn, scipy, spacy, Keras, Tensorflow
Computing Environments: Jupyter, Spyder IDE, Atom
Command Shell: iPython, Mac OS X
Data Query: Azure, Google, SQL and noSQL, various SQL and NoSQL databases and data warehouses.
• Experience with AWS cloud computing, Spark, Tableau.
• Capable of writing efficient code and working with large data sets.
Deep Learning: Machine perception, Data Mining, Machine Learning algorithms, Neural Networks, Tensor Flow, Keras
Soft Skills: Able to deliver presentations and highly technical reports; collaboration with stakeholders and cross-functional teams, advisement on how to leverage analytical insights. Development of clear analytical reports which directly address strategic goals.
• Able to identify and learn applicable new techniques independently as needed.
• Able to work comfortably and effectively within an interdisciplinary research environment.
• Experience with validation of machine learning ensemble classifiers.
• Significant enhancement of critical thinking and problem solving in application.
• Familiarity with project management, Agile Scrum methodologies and technical knowledge of how different IT functions integrate with one another.
Operating Systems: Windows 8 and 10, Linux Ubuntu

Professional Experience
Machine Learning Engineer Apr 2021 – Present
REI Sporting Goods / Seattle, WA
REI Sporting Goods is a large-scale provider of top-quality gear and apparel, expert advice, and rental equipment supporting life outside and outdoor experiences to enjoy alone or share with your friends and family.

I worked on a team responsible for developing and deploying risk-based decision tools and building knowledge-based systems to solve large scale computational problems to help the company better understand their customer demographics and related buying patterns.

• Developed Data Processing Pipeline using Apache Spark and Scala to process search queries in batches with joining data from other sources and APIs and make the final output available for search Embossing Process.
• Extracted topics from public documents using Gensim and TextRazor and linked them to entities in Knowledge Graph.
• Conceptualized and built PoC of Diagnostic Analysis and Predictive Analytics Engine for Oil Well Failure using E-M, MLE and MAP.
• Trained and deployed text classifiers with Word Embeddings, LSTM, BERT using TensorFlow, and Keras on AWS Sagemaker.
• Conducted Data Science team code reviews Python, TensorFlow, Keras).
• Wrote code in Python and R to extract useful data from retail trade information sources.
• Extracted data from a SQL database using complex SQL queries with Python’s SQLAlchemy and Pandas.
• Wrote SQL queries to merge data from multiple tables to obtain relationships between multiple types of customer data stores.
• Programmed customized solutions using Python utilizing the Tensorflow, Keras, and Numpy libraries.
• Utilized Spark, Scala, Hadoop, HBase, Kafka, Spark Streaming, and a variety of machine learning methods, including classifications, regressions, dimensionality reduction etc.
• Utilized Docker to handle deployment on heterogeneous platforms such as Windows, OSX, and AWS.
• Designed custom BI reporting dashboards in Python using Dash with Plotly.
• Worked in Git development environment.
• Worked in AWS Cloud computing environment (Spark (especially AWS EC2, AWS S3, Lambda)).
• Interfaced to an Amazon DynamoDB NoSQL database.

Senior Data Scientist Jan 2020 – Ap 2021
KROGER (8451) / Cincinnati, Ohio
Kroger is one of the largest retail chain corporation in United States park of Fortune 20. Headquartered in Cincinnati Ohio. Working with 8451 (part of Kroger) as a Data Scientist. Recommendations were created by simulating mine conditions and choosing which scenario would optimize ore producing KPIs. During testing, experienced mine dispatchers accepting these recommendations resulted in millions of dollars of more produced ore during a typical 12-hour shift.

• Employed Hadoop and GCP to ingest and curate the Client’s data.
• Worked on RNN model to perform sentiment analysis on customer tweets and reviews about products and models and scraped for thousands of reviews and delivered a multi class output (SoftMax) which ranked a Tweet as either positive, negative, positive with reservations or Unsatisfied customer.
• Cleaned data, performed EDA, imputed data for a predictive model which would predict delivery failures. Modelling was performed using SVM, Logisitic regression, KNN, Bernoulli, Naïve bayes and Artificial NN (RNN, CNN, ANN, LSTMS)
• Evaluated model performance for Binary classification models using confusion matrix, f1 scores, precision and recall
• Built an MVP for customer segmentation using unsupervised models, K means, grid search and DB scan.
• Executed data extraction and transformation and then loaded for time series modelling to predict SKU wise demand on a store level. Model built using ARIMA. Prophet models, near 90% accuracy on SKU (home and personal care).
• Developed code to store RMSE, R2 score for all-time series models in production. These scores are regularly updated and model performance is analyzed. This error analysis helps many teams immediately find insights if models need reassessment.
• Time series Models performed and stored Trend, Seasonality, and residual values in separate tables.
• Designed, wrote, and supported the code to automate the data processing pipeline for business-critical space and floor planning data for the entire Kroger enterprise.
• Developed PySpark applications using Data frames and Spark SQL API for faster processing of data.
• Data support and debugging of the aforementioned space and floor planning data.
• Designed visualizations for the Retail Operations data sets to inform the clients of the quality of the data they are receiving and to which they are applying data science methodologies
• Engaged with principal data scientist to cross-pollinate and leverage design patterns and practices for big data equerries which would filter out anomalies or input entries in data files.
• Develop, test and enhance data stores (tables, views, files) fitting to design and architecture of data solution.
• Developed Machine learning models such as KNN and K-means on incoming data, apply unsupervised Machine Learning models for segmentation.
• Investigate performance issues, identify optimization measures including but not limited to coalescing fragmented data, optimizing resource usage, re-partitioning, indexing, bucketing/distribution.
• Implemented PySpark and Spark SQL for faster testing and processing of data worked on migrating Oracle queries and Alteryx workflows into PySpark transformation

Data Scientist- Digital Flavors Oct 2018 – Jan 2020
Givaudan, Cincinnati Ohio
Givaudan is the world's largest company in the flavor and fragrance industries. Based out of Cincinnati Ohio. I was working as a data scientist in the Digital Flavors Division
I developed a churn-model that identified customers who were on the verge of leaving, using logistics regression. Through incentives of customers who were identified by the model to potentially cancel their account, cost savings were created by retention and the average life time of customers were increased significantly.

• Develop applications to standardize and ETL data; design large scale data analytics and reporting platforms.
• Develops and codes software programs, algorithms and user inter-faces to cleanse, integrate and evaluate large datasets from multiple disparate sources primarily on RStudio and developed on Shiny app
• Designed and built core data visualization capabilities. Tableau or Shiny apps built with Plotly
• Design and build core data visualization capabilities. Tableau or Shiny apps built with Plotly
• Identifies meaningful insights from large data and meta data sources, built a recommender app to map customer behavior and recommend/show next sample on the Aroma platform.
• Interacts with research teams to identify questions and issues. Built easy to use ANN and tree models (XGboost, Gradient desent) and deployed them using Rshiny. The dashboard was easy to use can was frequently used by non-IT staff to generate predictions for a population using training data.
• Responsible a A/B testing on new flavor extractions,
• Used AWS SQS to store multiple requests and AWS Cloud Watch to execute the lambda functions as well as integrated AWS SNS to send recommendations to the user via email.
• Worked with data science team and provided respective data as required on an ad-hoc request basis.
• Delivered portfolio risk dashboard as a package covering all aspects of the credit life cycle for retail unsecured loans.
• Created time series forecasts using Prophet for default rates of bank financial instruments.
• Worked with huge data sets from Big Data with Hadoop, HDFS, Map Reduce, and Spark.
• Information used included structured and semi-structured data elements collected from both internal and external sources.
• Unbalanced data issue was handled using Synthetic Minority Over Sampling, SMOTE. Missing data was handled using KNN imputation.
• Assisted both application engineering and data scientist teams in mutual agreements/provisions of data, deployment of production models etc.
• Python, MlLib, and a broad variety of machine learning methods including classifications, regressions and dimensionality reduction were incorporated.
• Use of Supervised, Unsupervised, Semi-Supervised classification and clustering of documents.
• Strong communication and problem-solving skills incorporated in a team environment.
• Contributed to security projects involving real-time object tracking and classification using OpenCV libraries.
• Interrogated analytical results to resolve algorithmic success, robustness and validity.
• Assisted in developing Spark/Scala, Python for regular expression (regex) projects in a Hadoop/Hive environment.

Consultant Machine Learning Engineer Jan 2017 – Sep 2018
MNG Health Cincinnati, OHIO, USA
MNG Health is a health care marketing company based out of Pittsburgh. As a data scientist my job was to build predictive models that can generate demand predictions and create custom marketing packages for HCPs
ANN autoencoders were used to model ECG readings and to detect anomalies from baseline. Data from arryhythmic events were combined with normal ECG data to simulate early warnings of symptoms of heart disease based upon error reconstruction metrics. Successful models were packaged and forwarded for deployment.

• Used predictive modelling techniques MNG’s Health extensive datasets to identify patterns by Time Series Analysis
• Applied hypothesis testing, regression analysis, linear models, non-linear models, forecasting, and machine learning
• Built a R shiny dashboard that allows to filter and select from over a million records, used by non-data science teams
• Final model predicts the probability and like-hood of prescription for hundreds of thousands of HCPs, Model validated by empirical data sets, allows laser strategy for marketing saving hundreds of manhours and thousands in savings.
• Feature engineering and data cleaning on different files, import, mutate and add new variables for machine learning
• Applied both Random Forest and Gradient boosting to cross validate results, identified top GINI split variables
• Project management with Scrum, all project deadlines meet with no backlogs, and clear distribution of work in team
• Thrived as a contributor, scientist and developer in an Agile development process.
• Use of Python/R or similar scripting languages to manipulate, analyze and visualize large data sets.
• Rapid model creation in Python using pandas, numpy, sklearn, and plot.ly for data visualization.
• Models implemented in SAS and interfaced with MSSQL databases and scheduled to update on a timely basis.
• Developed anomaly and outlier detection development in the creation of algorithms with CNNs.
• Machine learning classification of documents - Neural Network and Deep Learning language techniques, K-neighbors, K-means, Random Forest, Logistic Regression.
• Trained Data with Different Classification Models like Decision Trees, SVM and Random forest
• Managed BI group and associated cross-departmental collaborators in the design, development, and implementation of near-real-time cloud and traditional data systems to capture, cleanse, store, and process data
• Programmatic usage of SQL databases and search engines.
• Segmentation of medical related images using OpenCV libraries.
• Developed and deployed machine learning as a service on Microsoft Azure cloud service.
• Conducted and interpreted analyses using noisy data sets.

Data Scientist/Data Engineer Mar 2015 – Jan 2017
Unilever, Englewood Cliffs, NJ
Unilever is second largest FMCG producer, based out of London I was working as a data scientist in the Home and personal care department
Health & Life Insurance company that is privately held. Data from current and past patrons were used to warehouse data that allowed me to create models and make predictions of best medical plans suited for each potential customer via a recommender system. Insurance agents were aided by this recommender system that incorporated Principle Component Analysis and Singular Value Decomposition in making collaborative suggestions about prospective customers. Early A/B testing showed that insurance agents who used the recommender system were able to close deals 13% more often than those who had not adopted the system.

• Tested solutions on AWS using services such as SageMaker, EC2, and Snowball Edge.
• Measured, monitored, and analyzed statistical trends for route planning, and route optimization
• A/B testing with development team on app and web, apply ML (association) to help upsell and cross sell.
• Used Machine learning to predict demand and supply patterns and guide operations to lower fuel costs and time
• Implemented logistics, vehicle routing problem, traveling salesman problem, capacitated vehicle routing problem.
• Developed personalized product recommendation with machine learning algorithms that used Collaborative filtering to better meet the needs of existing customers and acquire new customers.
• Created machine learning algorithm and employed logistic regression, random forest, KNN, SVM, neural network, linear regression, and lasso regression and k-means.
• Developed optimization algorithms that can be used with data driven models, such as with supervised and unsupervised machine learning or reinforcement machine learning.
• Able to research statistical machine learning methods which may include forecasting, supervised learning, classification, and Bayesian methods.
• Able to advance the technical sophistication of solutions through the use of machine learning and other advanced technologies.
• Performed exploratory data analysis and data visualizations using R, and Tableau.
• Collaborated with data engineers to implement the ETL process, wrote and optimized SQL queries to perform data extraction and merging from Oracle.
• Used R, Python, and Spark to develop a variety of models and algorithms for analytic purposes.
• Performed data integrity checks, data cleaning, exploratory analysis and feature engineer using R and Python.

Data Scientist Jun 2012 – Feb 2015
BRITISH AMERICAN TOBACCO (Global Top Employers), HQ in London
British American Tobacco is the world's second largest tobacco manufacturer headquartered in London. I was working as a Data Analysts/scientist in their Marketing and logistics department. Promoted from Jr. Data Scientist to Data Scientist.
Data training sets were created, and classification of documents to relevant categories were used in the search for companies with low market visibility. NLP techniques and associated models were used to filter data from several sources on the web. Relevant documents led to the discovery and acquisitions of several companies structured as LBOs that yielded estimated averages of IRRs of 25% with expected exit times of 2-4 years.

• Create data quality products for monitoring and reporting, develop documentation for production and maintenance efficiency
• Establish and maintain an ongoing process for reviewing suspect data, determining root cause, and communicating remediation requirements
• Extracted payroll data from SQL and NoSQL, perform statistical analysis of performance and people analytic measures lead of analytics and rewards fora team that achieved10% in productivity in form of reduced breaks and more work hours and negotiate contract with modest increment in wages
• Use of a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction using NTLK, word2vec and TF/IDF
• Cleaned, parsed, and tokenized 1.6 million sentences using NLTK and scikit-learn.
• Developed, deployed, and maintained production NLP models with scalability in mind.
• OLAP cubes used in preparation for data mining, behavioral and attitudinal segmentation, predictive modeling, insight extraction, and data visualization
• Worked with complex applications such as R and SAS, to develop neural network, cluster analysis.
• Implementation of machine learning algorithms and concepts such as: K-means Clustering (varieties), gaussian distribution, decision tree etc.
• Analyzed data using data visualization tools and reported key features using statistic tools and supervised machine learning techniques to achieve project objectives.
• Analyzed large data sets and applied machine learning techniques and develop predictive models, statistical models.
• Used key indicators in Python for machine learning concepts like regression, boot strap aggregation and random forest.
• Use of inferential statistics and machine learning data pipelines.

CERTIFICATIONS
MS Business Analytics
BS Economics and Mathematics
Cloudera Certified Developer in Spark and Hadoop

Self-Training
Deploying Serverless Models in AWS Sagemaker
Snowflakes integration with Cloud Platforms
Data Scientist with R
Data Scientist with Python
Practical Deep Learning using keras

"
,
Data Scientist,"
Professional Summary
• **+ years’ overall experience covering Software/Information Technology/Web Development and Data Science.
• 8 consecutive years focused on Data Science.
• Extensive work in Natural Language Processing and Predictive Analytics using Machine Learning Algorithms, Visualization Tools, and Web Deployment Technologies.
• Used Neural Networks, Trees, Clustering Algorithms, and Statistical Models to propel systems which perform Sentiment Analysis, Fraud Detection, Client Segmentation, Predictive Maintenance, Demand Forecasting.
• Experience in Natural Language Processing (NLP), Machine Learning & Artificial Intelligence.
• Experience with AWS, Kubernetes, and Azure cloud computing.
• Spark (especially AWS EMR), Kibana, Node.js, Tableau.
• Expertise in Machine Learning, Deep Learning, Natural Language Processing, and Data Analytics Ml_Ops, Model Productionizing and Monitoring.
• Projects involving Sentiment Analysis, Fraud Detection, Predictive Analytics, Artificial Intelligence.
• Skilled Python programmer.
• Business understanding, Data understanding, Data preparation, Modeling, Evaluation and Deployment.
• Experienced in practical applications of data science to solve business problems and to produce actionable results.
• Able to incorporate visual analytics dashboards.
• Experience with a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction.
• Knowledge on Apache Spark and developing data processing and analysis algorithms using Python.
• Programming strength in Python, C, C++, Java, SQL, R, MATLAB, Mathematica, JavaScript.
• Use of libraries and frameworks in Machine Learning such as NumPy, SciPy, Pandas, Theano, Caffe, Sci-Kit Learn, Matplotlib, Seaborn, TensorFlow, Keras, PyTorch, NLTK, Gensim, Urllib, Beautiful Soup.
• Ability with algorithms, data query and process automation.
• Evaluation of datasets and complex data modelling.

Technical Skills
ANALYTICS - Data Analysis, Data Mining, Data Visualization, Statistical Analysis, Multivariate Analysis, Stochastic Optimization, Linear Regression, ANOVA, Hypothesis Testing, Forecasting, ARIMA, Sentiment Analysis, Predictive Analysis, Pattern Recognition, Classification, Behavioral Modeling
DATA EXTRACTION AND MANIPULATION - Hadoop HDFS, Hortonworks Hadoop, MapReduce, Cloudera Hadoop, Cloudera Impala, Google Cloud Platform, MS Azure Cloud, SQL, NoSQL, Data Warehouse, Data Lake, SWL, HiveQL, AWS (RedShift, Kinesis, EMR, EC2, Lambda)
NATURAL LANGUAGE PROCESSING - Document Tokenization, Token Embedding, Word Models, Word2Vec, FastText, Bag Of Words, TF/IDF, Bert, Elmo, LDA.
MACHINE LEARNING - Supervised Machine Learning Algorithms (Linear Regression, Logistic Regression, Support Vector Machines, Decision Trees and Random Forests, Naïve Bayes Classifiers, K Nearest Neighbors), Unsupervised Machine Learning Algorithms (K Means Clustering, Gaussian Mixtures, Hidden Markov Models, Auto Encoders), Imbalanced Learning (SMOTE, AdaSyn, NearMiss), Deep Learning Artificial Neural Networks, Machine Perception
PROGRAMMING LANGUAGES - Python, R, SQL, Java, MATLAB, Mathematica, C, C++, JavaScript, PHP
LIBRARIES - NumPy, SciPy, Pandas, Theano, Caffe, Matplotlib, Seaborn, Plotly, TensorFlow, Keras, NLTK, PyTorch, Gensim, Urllib, BeautifulSoup4, PySpark, PyMySQL, SQAlchemy, MongoDB, SQLite3, Flask, Deeplearning4j, EJML, DPLYR, GGPLOT2, Reshape2, TIDYR, PURRR, READR, Apache, Spark, MapReduce, WPF, Entity Framework Core, Node.js
DEVELOPMENT - Git, GitHub, GitLab, Bitbucket, SVN, Mercurial, Trello, PyCharm, IntelliJ, Visual Studio, Sublime, JIRA, TFS, Linux, Unix
APPLICATIONS - Machine Language Comprehension, Sentiment Analysis, Predictive Maintenance, Demand Forecasting, Fraud Detection, Client Segmentation, Marketing Analysis
LEADERSHIP - Push project goals, determine business use cases, and mentor/lead teams
QUALITY - Continuous improvement in project processes, workflows, automation and ongoing learning and achievement CLOUD Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud)

Professional Work Experience
Levi Strauss & Co., San Francisco, California
ML-Ops Engineer
June 2019 – Present
Levis is a manufacturing and merchandising store that supplies over 500 stores and multiple output channels. As lead ML-Ops Engineer, I designed and implemented a restocking solution using AWS Batch and Docker Containers. The Purpose of the restocking solution was to populate an app running on Objective C edge devices to inform and design the restocking of product according to predicted demand. The models were built using Sagemaker and Scheduled using Adobe Airflow. At Levi’s, I:

• Built a personalized in-session product recommendation engine.
• Wrote scripts in Python that automated text summarization and clustering.
• Implemented and configured a Next-Best offer prediction solution.
• Designed Microassortments for Next-Gen stores.
• Conducted Anomaly Detection and Root Cause Analysis exercises.
• Unified consumer profile with probabilistic record linkage.
• Enabled Visual search capability for similar and complementary products.
• Architected, built, maintained, and improved new and existing suites of algorithms and their underlying systems.
• Implemented end-to-end solutions for batch and real-time algorithms along with requisite tooling around monitoring, logging, automated testing, performance testing, and A/B testing.
• Worked closely with data scientists and analysts to create and deploy new product features on the ecommerce website, in-store portals, and the Levi’s mobile app.
• Established scalable, efficient, automated processes for data analyses, model development, validation, and implementation.
• Implemented deployment solutions using TensorFlow, Keras, Docker, and Elastic Kubernetes Service.
• Implemented Model Drift Monitoring and Retraining Strategies.
• Integrated solution into AWS, GCP, and Azure environments.
• Implemented and configured dedicated preprocessing, inference, and model validation scripting using a SageMaker model for batch transformation,

Sutter Health, Sacramento, California
Data Scientist/ NLP Engineer
June 2016 – June 2019
Worked with a natural language processing (NLP/data science team that implemented a medical response AI chatbot that directs users regarding Insurance issues, coverage, medical records, and billing questions to the correct department or provides general answers to patient’s non-health-related questions.

• Cleaned text data using different techniques.
• Performed EDA such as Bag of Words, K-means, DBSCAN, etc.
• Used embedders such as Universal Google Encoder, Doc2Vec, TFIDF, BERT, and ELMO to identify the best embedder that yields the best performing result.
• Performed Cosine Similarity method to match the user input to the most similar trained question and matched the trained question to the corresponding department.
• Deployed model using FLASK.
• Split the dataset into training, validation, and test data.
• Visualized and rescaled images.
• Created the model using Keras convolutional layers, max pooling layers, normalization, and drop-out layers using different activation functions.
• Flattened the CNN output and fed them to the dense layers.

HSBC Bank USA, New York City, NY
Data Scientist
April 2015 – June 2016
HSBC is a major player in the banking and financial business worldwide. I worked with their Cyber Security and Fraud departments to improve their credit/debit card fraud detection using Machine Learning Algorithms. My primary function was to design a Data-Driven model which detected all cases of fraud while limiting false-positives. Statistical Methods were used and as the datasets involved were highly imbalanced, innovative solutions were implemented to insure the highest confidence in the predictions and outlier detection.

• Performed Data Cleaning, features scaling, and features engineering using Pandas and Numpy packages in Python and built models using deep-learning frameworks.
• Performed data mining and developed statistical models using Python to provide tactical recommendations to business executives.
• Programmed a utility in Python that used multiple packages Scipy, Numpy, Pandas).
• Integrated R into micro-strategy to expose metrics determined by more sophisticated and detailed models than natively available in the tool.
• Extended existing semantic labeling model to perform Monte Carlo, Markov Chain and provide uncertainties, as well as semantic predictions using Bayesian approximation. A new metric was proposed to evaluate the quality of estimated uncertainties. The developed model was shown to outperform the baseline model when evaluated on the same dataset.
• Designed dashboards with Tableau and provided complex reports, including summaries, charts, and graphs to interpret findings to team and stakeholders.
• Worked with Data Engineers for database design for Data Science.
• Use Git to apply version control and tracked changes in files and coordinated work on the files amongst multiple team members.
• Implemented a Python-based distributed Random Forest.
• Used predictive modeling with tools in SAS, SPSS, R, and Python.

El Camino General Engineering IV, Palo Alto, California
ML Engineer/CV Specialist
March 2013 – April 2015
EC operates in a wide variety of industries and provides customers with one-source construction design/development/project management services for complete project lifecycles. I served as part of an Analytics and Inspection team that developed computer vision techniques to identify and classify cracks and lesions in concrete structures from customer footage using convolutional neural networks and pretrained models. The goal was to detect cracks or imperfections in concrete poured structures and floors.

• With the PyTorch Python API, the team built the architecture and trained the convolutional neural networks (CNN) on several hundred images.
• Exploited transfer learning with custom-built classifiers in PyTorch to speed up production time and improve results.
• Used a fully convolutional network (FCN) - pre-trained YOLO v3 algorithm - to speed up predictions.
• Took into consideration prediction time and overhead to make sure our predictions happened in real time.
• Regularized the data by applying transformations to the images using Pillow.
• Worked with large stores of video imaging data stored on AWS S3 buckets for training the model.
• Supplied our pickled model to the software development team to integrate into the drone software.
• Employed proper version control using Git with BitBucket to coordinate with fellow team members.
• Replaced proprietary software with custom-built algorithms for greater control over outcomes.

Education
Bachelor of Science - Computer Science - Cogswell University of Silicon Valley

"
,
VP of Engineering/Research,"
Aleksandar Marinkovic, Sc.D.
** ****** **** **** * Brookline, MA 02446 adoq6l@r.postjobfree.com 617-***-****

Experienced engineering manager who works to bridge the space between strategic vision and medical device development. Engineer-scientist, research and development innovation leader with more than 15 years of experience in medical devices, life sciences and biotechnology. Built and led cross-functional teams in all stages of development of medical devices and diagnostic instrumentation. Applied systems engineering in complex product development. Expert in design controls and medical device design verification testing. Expert in product development lifecycle management. Expert in design and prototyping of user-centered electro-mechanical devices and systems. Trained and researched in regenerative medicine and tissue engineering, physiology of lung and muscle, microfluidics, organs-on-chip.

Expertise in:
Building R&D Teams
Interviewing Candidates
Product Development
Software Development Leadership
Hardware Development Leadership
Product Development Lifecycle Management
Digital Transformation
Diagnostic Instrumentation
Medical Devices
System Architecture
Complex Systems Design
Embedded Systems
Algorithms
Data Engineering, Machine Learning
Wearable Sensors
Tissue Engineering and Regenerative Medicine
Minimally Invasive Surgery
Mechanisms of Lung Diseases

Select accomplishments:
●Built and led cross-functional teams developing medical devices and embedded software.
●Architected hardware and embedded software for diagnostic and therapeutic medical devices.
●Designed and implemented performance BLDC motor control systems for surgical devices.
●Developed advanced signal processing algorithms and designed user interfaces to maximize user experience.

Technical expertise and skills:
Medical device development:
Medical devices: Product development lifecycle management, System verification and validation (V&V); Minimally invasive surgery, Pulmonary function testing, Non-invasive monitoring of physiological signals
System architecture: Complex systems, Multi-domain architecture, Hardware-software integration, Electro-mechanical systems, Data analytics
Control systems: Real-time control, Feedback control, Digital control, System identification, Machine learning, BLDC Motor control (hardware and software), Fluidic control systems
Embedded systems: ARM MCUs, SoCs, Xilinx FPGA, Data acquisition and mixed signal processing, Digital filters, Algorithms
UI and UX: User-machine interfacing, Seamless patient-diagnostic device interaction, GUI design and integration
Electronics: Analog and mixed signal systems, Microcontrollers, Analog and digital circuits, Wearable sensors, Physical and chemical sensors, PCB design
Design and manufacturing: Design controls, Rapid prototyping, Managing product architecture, Failure modes and effects analysis, Engineering materials, Biocompatibility
Quality and regulatory: IEC 62304, ISO 13485, IEC 60601, FDA 510(k)
Software development:
Programming languages: C#, Visual Basic .Net, Matlab, Embedded C/C++, Python
Development environments: Visual Studio, Eclipse, Qt Creator
Requirement lifecycle management: Helix ALM (TestTrack), Polarion ALM
Source code management / Version control: GIT, Azure DevOps, Helix Core, Bitbucket
Issue tracking and testing: Jira, Helix ALM
Operating systems: Windows, Embedded Linux, RTOS

Experience:
Medtronic, Woburn, MA (Hardware/Software/Systems Engineering Manager) 2021 – now
●Drove technical projects and provided leadership in an innovative and fast-paced environment.
●Built and managed a team of software engineers, including task planning and code reviews.
●Managed a team of electrical, mechanical and R&D engineers, including task planning and design reviews.
●Took responsibility for the overall planning, execution and success of complex technical projects.
●Contributed technically to projects.
●Worked closely with product management and quality engineering teams to ensure we're building the best products.
Medtronic, Woburn, MA (Principal R&D Engineer) 2018 – 2021
●Led technology innovation in line with Medtronic GYN product roadmap. Designed system architecture for hysteroscopic tissue removal and fluid management systems.
●Supervised embedded software development for next generation hysteroscopic tissue removal and fluid management systems.
●Conceived new approaches and developed Technology Requirements.
●Worked with internal and external resources to develop prototypes that integrate candidate technologies and demonstrate their feasibility.
●Oversaw the development of technology demonstration prototypes and addressed technical challenges as those arose.
●Supported the creation of chartering documents to seek management approval for funding and execution of GYN projects.
●Supported the evaluation of development partners and acted as a key contact person for the duration of the project.
●Created RACIQ matrices to enable project development activities, and manage and coordinate geographically remote R&D teams; led initiation and execution of projects across interdisciplinary teams.
●Led ideation sessions; created IP and prepared invention disclosures in focus areas.
●Mentored senior and junior engineers.
●Implemented product development lifecycle management system to ensure traceability of all stages of the medical device development process.
●Led V&V activities, created test plans, designed test analogs for subsystems, and supervised writing and execution of test protocols.
●Developed and implemented digital control algorithms for BLDC motors and hysteroscopic fluid management system pumps.
●Designed RFID reader for validation of disposable tissue shavers and automatic setting of operational parameters.
Smith & Nephew, Andover, MA (Staff Systems Engineer, R&D) 2017 – 2018
●Led design of the BLDC motor assembly and controller for arthroscopic shaver system. Worked with OEM and accomplished the development of the next generation of BLDC motor capable of withstanding harsh operating conditions (exposure to saline and autoclave).
●Investigated and defined system features. Worked with other engineering disciplines to ensure proper implementation.
●Worked with marketing to define user requirements and performed feasibility studies.
●Developed system validation and verification protocols and performed testing.
●Developed control system algorithms and translated them into embedded software ensuring proper performance of shaver handpiece.
Morgan Scientific, Haverhill, MA (Director of Development/Engineering) 2016 – 2017
●Led the development of advanced signal processing algorithms for pulmonary function testing (PFT) instruments. Designed and wrote a signal processing module for the ComPAS PFT software suite.
●Developed a method to rapidly measure lung volumes with body plethysmography without the need for thermal equilibration of the device. The method vastly improved user experience, especially for children.
●Implemented instrument control, integrated fast gas sensors and developed calibration procedures.
Wyss Institute for Biologically Inspired Engineering, Boston, MA (Postdoctoral Fellow) 2015 – 2016
●Developed a tri-culture of primary human cells on the Organs-on-Chip microfluidic platform.
●Identified acute cytokine and lipid mediator responses in the cell culture model of lung fibrosis after ionizing radiation exposure.
Morgan Scientific, Haverhill, MA (Consultant) 2008 – 2016
●Designed and implemented the single- and multi-breath nitrogen washout PFTs.
●Developed advanced signal processing algorithms for PFT devices.
Matrigen Life Technologies, Brea, CA (Technology Developer) 2011 – 2013
●Supervised engineering of the ‘softwell’ technology that replicated a broad range of physiological tissue softness, from fat to contracted cardiac muscle, so one can routinely venture beyond the rigidity of tissue culture plastic and study tissue fibrosis development in pathophysiologically relevant mechanical milieu.
Massachusetts General Hospital, Boston, MA (Postdoctoral Fellow) 2013 – 2014
The Laboratory for Tissue Engineering and Organ Fabrication, The Center for Regenerative Medicine
●Designed a next generation of biomimetic scaffolds for liver tissue engineering.
●Engineered skeletal muscle tissue constructs.
Harvard University, Boston, MA (Graduate Student) 2007 – 2012
Harvard School of Public Health, Molecular and Integrative Physiological Sciences
●Developed a high-throughput, traction-based functional assay for studying contractility of single cells and used this tool to identify the mechanisms involved in matrix stiffness-induced activation of lung fibroblasts; suggested novel strategies for treating idiopathic pulmonary fibrosis.
●Developed ‘TractionsForAll 2015’ (https://tractionsforall.wixsite.com/tractionsforall), freely distributed software that computes tractions exerted by the adherent cells on soft hydrogel substrates.
Massachusetts Institute of Technology, Cambridge, MA (Graduate Student) 2005 – 2007
Department of Mechanical Engineering, d'Arbeloff Lab for Information Systems and Technology
●Developed a method for calibration of wearable photoplethysmograph sensor for continuous blood pressure monitoring.
Harvard School of Public Health, Boston, MA (Graduate Research Scholar) 2003 – 2005
Department of Environmental Health, Physiology Program
●Developed a real-time virtual loading system used to discover the relaxation effect of deep breathing on airway smooth muscle undergoing spastic response as in asthmatic attack.
Water Supply Company, Kragujevac, Serbia (Staff Engineer, Control Systems) 2002 – 2003
●Supervised control and monitoring of drinking water treatment and distribution.
ECM Industrial Electronics, Kragujevac, Serbia (Intern) 2001
●Developed digital PID controller automatically tuned by using relay feedback approach.

Education:
Postdoctoral Fellow, Wyss Institute for Biologically Inspired Engineering, Harvard University 2015 – 2016
Area: Organs-on-Chip, Biomimetic Microsystems, Disease Modeling, Microfluidics
T32 Postdoctoral Fellow, Massachusetts General Hospital 2013 – 2014
Area: Tissue Engineering, Regenerative Medicine, Translational Medicine, Microfluidics
Award: Kirschstein National Research Service Award in Tissue Engineering and Regenerative Medicine
Doctor of Science, Molecular and Integrative Physiological Sciences, Harvard University 2007 – 2012
Area: Bioengineering, Physiology, Mechanisms of Disease, Biostatistics, Environmental Health
Thesis: Interactions of Matrix Stiffness and Cytoskeletal Tension in Lung Fibroblast Proliferation and Fibrogenesis
Award: Ruth L. Kirschstein NRSA, graduate student fellowship
Master of Science in Mechanical Engineering, Massachusetts Institute of Technology (MIT) 2005 – 2007
Area: Control, Instrumentation and Robotics; Bioengineering
Thesis: Reconstructing the Blood Pressure Waveforms Using a Wearable Photoplethysmograph Sensor and Hydrostatic Pressure Variations Measured by Accelerometers
Bachelor of Science, Faculty of Mechanical Engineering, University of Kragujevac, Serbia 1996 – 2002
Area: Control and Applied Mechanics, Summa Cum Laude

Certificates:
Association for the Advancement of Medical Instrumentation (AAMI), Design Control Requirements – Integrating the Quality System Regulation and ANSI/AAMI/ISO 13485
Internet of Things: Business Implications and Opportunities, MIT Sloan School of Management, ID 19644928
Principles and Models for System Architecture, Massachusetts Institute of Technology, July 2018

Memberships:
MIT MechE Alliance Mentoring (I help MIT MechE students to achieve their goals and build career in product development and design of medical devices.)
International Council on Systems Engineering (INCOSE), Biomedical Engineering Society, American Thoracic Society, American Physiological Society, Biophysical Society
Peer reviewer for: Journal of Biomechanics (Acta Biomaterialia), Journal of Visualized Experiments (JoVE)

Patents:
1.Marinković A, “Surgical systems for controlling an angular position trajectory for tissue shavers and other rotating surgical instruments”, Pub. No.: US 2007/0167844 A1, Appl. No.: US16/410,025, Filed: May 13, 2019
2.Asada HH, Marinković A, Shaltis PA, Reisner AT, “Apparatus and method for blood pressure measurement by touch”, Pub. No.: US 2007/0167844 A1, Appl. No.: 11/605,935, Filed: Nov 29, 2006

Select publications:
1.Marinković A, Liu F, and Tschumperlin DJ, “Matrices of physiologic stiffness potently inactivate IPF fibroblasts”, Am J Respir Cell Mol Biol. 2013 Apr;48(4):422-30, PMID: 23258227
2.Mih JD, Marinković A, Liu F, Sharif A, and Tschumperlin DJ, “Matrix stiffness reverses the effect of actomyosin tension on cell proliferation”, J Cell Sci. 2012 Dec 15;125(Pt 24):5974-83, PMID: 23097048
3.Marinković A, Mih JD, Park J, Liu F, and Tschumperlin DJ, “Improved throughput traction microscopy reveals dominant role for matrix stiffness in fibroblast contractility and TGF-β responsiveness”, Am J Physiol Lung Cell Mol Physiol. 2012 Aug 1;303(3):L169-80, PMID: 22659883
4.Mih JD, Marinković A, Liu F, Sharif A, and Tschumperlin DJ, “A multiwell platform for studying stiffness-dependent biology”, PLoS One. 2011;6(5):e19929, PMID: 21637769
5.Luthy SK, Marinković A, Weiner DJ, “Resonant frequency does not predict high-frequency chest compression settings that maximize airflow or volume”, Pediatr Pulmonol. 2011 Jun;46(6):604-9, PMID: 21438176

"
,
Data Scientist,"
P: 720-***-****
G: adonqk@r.postjobfree.com
Professional Summary
Experience working as a Data Scientist/Data Analyst/Data Modeler with emphasis on data mapping, and data validation in a data warehousing environment.
Demonstrated skills applied to business intelligence (and BI technologies) tools such as data warehousing, reporting, querying tools, and data mining.
Worked on different types of Python modules such as Tensorflow, Pytorch, Theano, NLTK, and Gensim.
Experienced employing Python, MATLAB, SAS, Tableau, and SQL for data cleaning, data visualization, risk analysis, and predictive analytics.
Hands-on experience with Machine Learning, Regression Analysis, Clustering, Boosting, Classification, Principal Component Analysis, and Data Visualization Tools.
Adept at using SAS Enterprise Guide, R, Python, and Big Data related technologies including Hadoop, Hive, Pig, Sqoop, Cassandra, Oozie, Flume, Map-Reduce and Cloudera Manager for the design of business intelligence applications.
Ability to provide wing-to-wing analytic support including pulling data, preparing analysis, interpreting data, making strategic recommendations, and presenting to client/product teams.
Develop Logical and Physical Data models and organizing data as per business requirements.
Strong understanding of when to use a standard statistical model, and when to use deep learning techniques.
Strong programming skills in a variety of languages such as Python and SQL.
Familiarity with Crystal Reports, and SSRS - Query, Reporting, Analysis and Enterprise Information Management.
Excellent knowledge on creating reports on Pentaho Business Intelligence.
Experience in Databases using Oracle, XML, DB2, Teradata15/14, Netezza, server, Big Data and NoSQL.
Worked with engineering teams to integrate algorithms and data into Return Path solutions.
Worked closely with other data scientists to create data-driven products.
Strong experience in Statistical Modeling/Machine Learning and Visualization Tools.
Expert at Full SDLC processes involving Requirements Gathering, Source Data Analysis, Creating Data Models, and Source to target data mapping, DDL generation, and performance tuning for data models.
Extensively used the Agile methodology as the organization standard to implement data models.
Experienced with machine learning tools and libraries such as Tensorflow, PyTorch, Caffe2, R, and Spark.
Hands-on experience with NLP, mining of structured, semi-structured, and unstructured data.
Experienced and in-depth knowledge of the SAS Enterprise Miner, and Python programming language.
Contribution to several research projects that combine new data sources and computational tools
Use of mathematical and statistical modeling and computer programming skills in an innovative manner.
Technical Skills table

Programming Languages:
Python, R, C/C++, C#, Wolfram, SQL (MySQL / SQLite / PostgreSQL), LaTeX
Version Control:
Git, GitHub
Machine Learning Frameworks:
TensorFlow, Torch
Cloud Computing:
Amazon Web Services (AWS), Microsoft Azure, GCP
IDEs:
PyCharm, Jupyter
Python Libraries:
NumPy, SciPy, Pandas, scikit-learn, Keras, PyTorch, TensorFlow, XGBoost, NLTK, lifelines, Matplotlib, Seaborn, TabPy, BeautifulSoup
Machine Learning:
Statistical Modeling, Deep Learning, Artificial Intelligence (AI), Natural Language Processing (NLP), Sentiment Analysis, Survival Analysis, Time Series Analysis
Supervised Learning:
Linear Regression, Logistic Regression, ElasticNet Regression, Decision Trees (CART), Random Forests, XGBoost (Gradient Boosted Decision Trees), Support Vector Machines (SVM, SVR, SVC), k-Nearest Neighbors (kNN), Naïve Bayes
Unsupervised Learning:
Principal Component Analysis (PCA), k-Means Clustering, Gaussian Mixture Models (GMM), MeanShift, Hidden Markov Models (HMM)
Deep Learning:
Artificial Neural Networks, (ANN), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), LSTM Networks, Generative Adversarial Networks (GAN), batching and regularization techniques
Optimization:
Convex Optimization, Non-Convex Optimization, Linear Programming (LP), Dynamic Programming (DP), Monte Carlo Methods, Network Flows
Statistics:
Bayesian Statistics, Statistical Modeling, Statistical Hypothesis Testing, Factor Analysis, ANOVA, Experimental Design, Factorial Design

Experience
Senior ML-Ops Engineer
Form.Com, Denver, Colorado 07/2020 to Present
Headed the Data Science team at Form.Com. The team was in charge of developing a computer vision and evaluation application to identify products being sold in individual stores. I implemented a Convolutional Embedding system to identify brand name products and movements in the cooler after successive image captures.
Developed a custom dataset for fine-tuning a deep neural network.
Fine-tuned a variety of image models with object-detection heads.
Used both Single Shot Detection (SSD) and You Only Look Once (YOLO) object detection models.
Deployed finished model on edge devices using TensorFlow-Lite.
Built various statistical models involving Time Series analysis, Survival Analysis, Multivariate Regression, Linear Regression, Logistic Regression, and PCA for financial projection.
Led the development of the expected profit projection engine by applying machine learning with financial engineering and actuarial science.
Performed in-force management, including survival analysis, churn/retention analysis, and risk identification.
Used pre-trained models to visualize feature maps in the intermediate layers and perform transfer learning.
Used pre-trained models (VGG16, ResNets, Inceptions, DenseNet, U-Net, etc.) for transfer learning on small datasets.
Designed and implemented the enterprise Financial Value-at-Risk model.
Led various cross-department projects and worked closely with internal stakeholders such as business teams, product managers, and engineering teams.
Worked on customer segmentation using an unsupervised clustering learning technique.

NLP Engineer, Data Scientist
Buoy Health, Boston, Massachusetts 05/2018 to 07/2020
Designed a customer service-oriented chatbot to identify potential symptoms and to direct traffic to appropriate departments. Buoy Health required a symptom checker chatbot which leverages AI to deliver personalized and more accurate diagnoses. The company’s algorithm was trained on clinical data from 18,000 medical papers to mirror the literature referenced by physicians. Beginning with the symptoms provided by the user via natural language processing, the chatbot matches the symptoms to all possible conditions and then ask clarifying questions to narrow them down to the best selection. Symptom checker chatbots are not clinical decision support (CDS) tools and do not claim to assist with medical decision making (MDM). They are also exempt from FDA regulation. Implemented BERT embeddings and Neural Net Classification and similarity algorithms. Initial perplexity measurements show a Computer Understanding of over 90% with a solution matching accuracy of over 85%.
Worked in an environment using Python, NoSQL, Docker, AWS, and Kubernetes.
Worked with the Python packages NumPy, Pandas, SciPy, Matplotlib, Plotly, and FeatureTools for data analytics, cleaning, and feature engineering.
Used NLTK and Gensim for NLP processes such as Tokenization and for creating custom Word Embeddings.
Imported from Python’s TensorFlow package for building Neural Network models.
Implemented ELMO, Doc2Vec and BERT embeddings.
Employed numerous different models, including Convolutional and Recurrent Neural Networks, LSTM, and Transformers which all utilized Word Embeddings.
Models which were operationalized were deployed to a RESTful API using the Python Flask package and Docker containers.
Used Agile approaches, including Extreme Programming, Test-Driven Development, and Agile Scrum.

Data Scientist
Santander Bank, Boston, Massachusetts 06/2015 to 05/2018
Santander Bank is a diversified financial business that helps people and businesses prosper. I was part of a small team of data scientists that worked with the Security Department tasked with fraud detection. I applied an ensemble of classification and unsupervised models to identify fraudulent activity within the incoming transactions. Once deployed, we were able to reduce the number of incorrectly identified fraudulent charges and save our customers time and money.
Stratified imbalanced data to ensure fair representation of the minority data in all data sets used for cross validation of the model.
Consulted with regulatory and subject matter experts to gain clear understanding of information and variables within data streams.
Utilized cloud computing resources for model optimization/tuning of hyperparameters, and cross-validation of statistical data science models.
Extracted data from Hive database on Hadoop using Spark through PySpark.
Used R’s dplyr for data manipulation, as well as ggplot2 for data visualization and EDA.
Utilized Scikit-Learn, SciPy, Matplotlib, and Plotly for EDA and data visualization.
Built Artificial Neural Network models to detect anomalies using PyTorch and Scikit-Learn.
Used Scikit-Learn’s model selection framework to perform hyper-parameter tuning using GridSearchCV and RandomizedSearchCV algorithms.
Developed unsupervised K-Means and Gaussian Mixture Models (GMM) from scratch in NumPy to detect anomalies.
Employed a heterogeneous stacked ensemble of methods for the final decision on what transaction was fraudulent.
Deployed model using a Flask app stored in a Docker container.
Evaluated the performance of our model using a confusion matrix, accuracy, recall, precision, and F1 score. Took careful consideration of the recall score.
Utilized Git for version control on GitHub to collaborate work with the team members.

Senior Data Analyst
Carnival cruise lines, Miami, florida 10/2013 to 06/2015
Carnival Cruise Line is an international cruise line that operates with more than $20 billion of annual revenue. At Carnival, I developed model to forecast demand and predict bookings across the calendar year, which helped optimize pricing during peak seasons and holidays, as well as port allocations and itineraries for the fleet. In this demand forecasting project, I used an ARIMA Time Series Regression model to forecast year-round bookings for three ship classes. Apart from modeling additional ship classes, I also performed ETL tasks (statsmodels, SQL, AWS EC2), feature engineering, and matrix-to-matrix transformations.
Developed time series forecasting models in Python using statsmodels to forecast demand for cruise ships.
Helped balance the load of ports by forecasting demand.
Incorporated data mined and scraped from outside sources.
Worked closely with the DevOps team to integrate my solutions into their software.
Enhanced data collection procedures to include information that is relevant for building analytic systems.
Conducted ad-hoc analysis and presentation of results in a clear manner.
Created machine learning algorithms using Scikit-learn and Pandas.
Built predictive models to forecast demand.
Hands-on use of commercial data mining with tools created in R and Python.
Processed, cleansed, and verified the integrity of data used for analysis.
Developed dashboards for use by executives for ad hoc reporting using Tableau visualization tools.
Solved analytical problems, and effectively communicated methodologies and results.
Worked closely with internal stakeholders such as business teams, product managers, engineering teams, and partner teams.
Generalized feature extraction in the machine learning pipeline which improved efficiency throughout the system.
Performed univariate, bivariate, multivariate analysis and thereby created new features and tested their importance.

Customer Relations/Social Media Administrator
Les Brown Enterprises 2010-2012
Utilized social networking sites to boost rebranding efforts of LBE.
Created and maintained customer and client e-mail databases.
Financial analyst for LBE products and managed small travel budgets.
Working knowledge of the Spanish and Korean languages.
Strong written and verbal communications skills.

Education
BBA – Finance and Business Administration - Howard University

Certification
CompTia A+ Certificate

"
,
Data Scientist,"
Professional Summary
** ***** ** ********** ** Machine Learning, Deep Learning, Data Mining with large datasets of Structured, Unstructured Data, Data Acquisition, Data Validation, Predictive Modeling.
Design optimization methods with computational efficiency considerations.
Research statistical machine learning methods which include forecasting, supervised learning, classification, and Bayesian methods.
Conduct complex, advanced research projects across business disciplines.
Develop new, advanced cutting-edge techniques and algorithms.
Experience with AWS cloud computing, Google Cloud Platform, Spark (especially AWS EMR), Kibana, Node.js, Tableau, Looker.
Strong technical communication skills; both written, verbal.
Understand, articulate the “big picture”, simplify complex ideas.
Strong problem solving, structuring skills.

Identify, learn applicable new techniques independently as needed.
Create new s through a combination of foundational research, collaboration with ongoing initiatives.
Stochastic optimization, to utilization by commercial applications or open-source algorithms.
Formulating, solving discrete, continuous optimization problems.
Transfer, implement results, technology in hard-, software prototypes, demo systems relevant - the businesses
Survey relevant technologies, stay abreast of latest developments
Knowledge of remote sensing.
Capable of writing efficient code, working with large datasets
Excellent use of mathematical, statistical modeling, computer programming skills in an innovative manner.

Technical Skills Summary
Programming Languages
Python, R, SQL, Java, MATLAB, Mathematica, C#, C++, Javascript

Libraries
NumPy, SciPy, Pandas, Theano, Caffe, SciKit-learn, Matplotlib, Seaborn, Plotly, TensorFlow, Keras, NLTK, PyTorch, Gensim, Urllib, BeautifulSoup4, PySpark, PyMySQL, SQAlchemy, MongoDB, sqlite3, Flask, Deeplearning4j, EJML, dplyr, ggplot2, reshape2, tidyr, purrr, readr, Apache, Spark.

Development
Git, GitHub, GitLab, Bitbucket, SVN, Mercurial, Trello, PyCharm, IntelliJ, Visual Studio, Sublime, JIRA, TFS, Linux, Unix

Data Extraction and Manipulation
Hadoop HDFS, Google Cloud Platform, MS Azure Cloud, SQL, NoSQL, Data Warehouse, Data Lake, SWL, HiveQL, AWS (RedShift, Kinesis, EMR, EC2, Lambda)

Analytics
Data Analysis, Data Mining, Data Visualization, Statistical Analysis, Multivariate Analysis, Stochastic Optimization, Linear Regression, ANOVA, Hypothesis Testing, Forecasting, ARIMA, Sentiment Analysis, Predictive Analysis, Pattern Recognition, Classification, Behavioral Modeling

Machine Learning
Supervised Machine Learning Algorithms (Linear Regression, Logistic Regression, Support Vector Machines, Decision Trees and Random Forests, Naïve Bayes Classifiers, K Nearest Neighbors), Unsupervised Machine Learning Algorithms (K Means Clustering, Gaussian Mixtures, Hidden Markov Models, Auto Encoders), Imbalanced Learning (SMOTE, AdaSyn, NearMiss), Deep Learning Artificial Neural Networks, Machine Perception

Natural Language Processing
Document Tokenization, Token Embedding, Word Models, Word2Vec, FastText, Bag Of Words, TF/IDF, Bert, Elmo, LDA

Applications
Machine Language Comprehension, Sentiment Analysis, Predictive Maintenance, Demand Forecasting, Fraud Detection, Client Segmentation, Marketing Analysis

Cloud
Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud Platform)

Quality
Continuous improvement in project processes, workflows, automation and ongoing learning and achievement CLOUD Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud)

Leadership
Push project goals, determine business use cases, and mentor/lead teams

Professional Experience
Senior Data Scientist
Mobile Apps Co, Atlanta, GA,
November 2019-Present

Worked on a team to create an alert automation system for internal messages and logs by leveraging cutting-edge NLP techniques. A hand-labeled internal dataset combined with tweets from Twitter’s API was used to train a model for importance, relevance, and priority along with a sentiment analysis matrix. Results were then classified by priority and urgency. The final production model used a neural network based on BERT and allowed users to decide what types of messages they wanted to let through the filter through an adjustable threshold and re-training. User productivity was expected to increase by 19.7% as projected by the business.

Work tasks:

Accessed the Twitter API using a Python wrapper to extract pseudo-labeled data based on hashtags.
Cleaned and prepared text best data through normalization, tokenization, stemming, and lemmatization using BERT and NLTK.
Coded customized solutions using Python and the Tensorflow and Numpy libraries.
Tested on a variety of embedders, including bag of words, TD-IDF, Word2vec, and ELMO.
Utilized statistical classifiers, random forests, and logistic regressions to perform sentiment analysis.
Constructed an Artificial Neural networking machine learning solution for natural language processing.
Implemented a model utilizing BERT for embedding and classification and fine-tuned to specific data.
Productionized final model by hosting a web API and user-friendly intranet app powered by FLASK.

Senior Data Scientist
Synchrony, Stamford, CT
May 2018-November 2019

Synchrony is a consumer financial services company. Augmented credit decisions by incorporating deep learning approaches using Cox Proportional Hazard (CPH) with XGBoost backend. Worked on a large complex longitudinal dataset spanning 20 years of data stored in SQL warehouse. Deployed model on a Spark cluster. Our model was able to predict the credit limit and reduce probability of default.
This project was focused on customer segmentation based on machine learning and statistical modeling effort, including building predictive models and generating data products to support customer segmentation.
Work tasks:

Applied analytics concepts of probability, distribution, and statistical inference on given datasets to unearth interesting findings through use of comparison, T-test, F-test, R-squared, P-value etc.
Performed data mining using state-of-the-art methods and executed large efficient SQL queries.
Enhanced data collection procedures to include information relevant for building analytic systems.
Processed, cleaned, and verified the integrity of data used for analysis.
Conducted ad-hoc analysis and presentation of analytics in a clear manner.
Automated anomaly detection systems and constantly tracked performance.
Implemented data architecture and employed data modeling techniques.
Hands-on use of commercial data mining with tools created in R-Programming and Python.
Created machine learning algorithms using Scikit-learn and Pandas.
Built predictive models to forecast risks for credit and loan products.
Developed dashboards for use by executives for ad hoc reporting using the Tableau data visualization tool.
Developed clusters using information from prospect database for enabling marketing initiatives.

Data Scientist
Shell Oil, Houston, TX
November 2014-May 2018
City, State

Predictive Analytics can enhance oil field production and cuts costs by finding optimal well settings and forecasting equipment failures and potential problems. The data spanned several years tracking oil wells in every major North American basin. The data included information on drilling and operational data from thousands of wells and hundreds of miles of low-pressure pipelines. Analysis of the data revealed critical issues with field deployed equipment.
This project built “digital twins” — computer models replicating above and below ground well behavior for artificial lift equipment. Input from sensor readings was applied to specific field issues: 1) improving plunger timing to realize well potential; 2) predicting preventive equipment maintenance to prevent failure in rods and submersible pumps; 3) reducing overuse of chemicals in wells.

Work tasks:

Worked in Git development environment.
Applied SAS Enterprise suite, Python, and Big Data-related technologies, including knowledge in Hadoop, Hive, Sqoop, Oozie, Flume, and Map-Reduce
Applied Predictive Modeling, Data Mining Methods, Factor Analysis, ANOVA, Hypothetical Testing, and Normal Distribution.
Transformed business requirements into analytical models, designed algorithms, built models, and developed data mining and reporting solutions that scaled across the company’s massive volume of structured and unstructured data.
Applied Statistical NLP / Machine Learning, especially Supervised Learning- Document classification, information extraction, and named entity recognition in-context.
Worked with Proof of Concepts (POC's) and gap analysis and gathered necessary data for analysis from different sources, and prepared data for data exploration using data wrangling.
Designed Physical Data Architecture of New system engines.
Implemented neural network utilizing Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, neural networks, Principle Component Analysis, and Recommender Systems.
Worked the Software Development Life Cycle (SDLC), including Requirements Analysis, Design Specification and Testing per cycle in both Waterfall and Agile methodologies.
Programmed SQL Server and Python functions.
Developed Logical and Physical Data models and organized data per business requirements using Sybase Power Designer, ER Studio in both Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) applications.
Designed Star Schema and Snowflake schema for Data Warehouse and Operational Data Store (ODS) architecture.
Designed Data Modeling Online Applications and served as Solution Lead for Architecting Data Warehouse/Business Intelligence Applications.
Worked with languages such as Python and Scala and software packages such as Stata, SAS, and SPSS to develop neural network and cluster analysis.
Designed visualizations using Tableau software and published and presented dashboards and storylines on web and desktop platforms.
Developed Logical Data Architecture with adherence to Enterprise Architecture.
Used Ply in R and Pandas in Python for performing Exploratory data analysis.
Utilized data modeling tools such as Power Designer and ER Studio.
Programmed functions using Python, C++, Matlab, and R.
Used PyTorch, Tensorflow, and Scikit Learn.
Integrated with AWS, Azure, and GCP platforms.
Applied Normalization and De-Normalization techniques for optimum performance in relational and dimensional database environments.
Performed System Analysis, E-R/Dimensional Data Modeling, Database Design, and implemented RDBMS specific features.
Implemented Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio, SSIS, SSAS, and SSRS.
Performed and completed Data Analytics, Data Reporting, Ad-hoc Reporting, Graphs, Scales, PivotTables and Online Analytical Processing (OLAP) reporting.
Interacted with data from Hadoop for basic analysis and extraction of data in the infrastructure to provide data summarization.
Created visualization tools and dashboards with Tableau, ggplot2 and d3.js.
Worked with and extracted data from various database sources such as Oracle, SQL Server, and DB2.

Data Scientist
Schlumberger Geosolutions, Houston, TX
August 2012-November 2014

Conducting analysis of geological facies and lithofacies depositional environment through neural networks classification with petrophysical well log data. Extrapolating classified facies with the help of surface seismic acoustic impedance trends calibrated with drilled wells. Targeting pay zones for volumetric assessment and estimation for field commercial evaluation and development. Used multi-layer perceptron, Logistic Regression, and Spectral Clustering to perform predictive analytics on IoT data from drilling equipment.

Work tasks:

Provided information and data support and services relating to both structured and unstructured data working across all business teams within the Global Wells Organization.
Developed and maintained processes and supporting tools for information and data control.
Interfaced information and data control resources with partners, vendors, regulatory agencies, and other external bodies, keeping distribution contacts current.
Extracted data from well-logging systems (e.g., OpenWells, CasingWear, StressCheck, Well Cost, and Campos, among others) to build machine learning algorithms to solve various problems.
Used Logistic Regression to predict whether there would be deviation at any given well depth in a drilling operation.
Used XGBoost with IoT data to predict Torque and Drag to minimize well casing and formation damage.
Used NLP to do Sentiment analysis followed by LDA to generate topics from the sentiment categories.
Processed huge datasets (over billion data points and 2 TB in size) for data association pairing and provided insights into meaningful data association and trends.
Deployed machine learning models on Azure Stack (Disconnected on drilling rigs), while ingesting data from IoT sensors.
Used Python 3.0 (NumPy, SciPy, Pandas, SciKit-Learn, Seaborn, NLTK) and Spark 2.0 (PySpark, MLlib) to develop a variety of models and algorithms for analytic purposes.

Education
Bachelor of Science - Physics - Uniersidad de Las Americas, Puebla, Mexico

Additional Education
University of Notre Dame, South Bend, Indiana

"
,
Data Scientist,"
Nicholas Kim
Data Scientist
P: 980-***-****
G: adonl3@r.postjobfree.com
PROFESSIONAL SUMMARY
Data Scientist with 7+ years’ experience processing and analyzing data across a variety of industries. Leverages various mathematical, statistical, and Machine Learning tools to collaboratively synthesize business insights and drive innovative solutions for productivity, efficiency, and revenue.

•Experience applying statistical models on big data sets using cloud-based cluster computing assets with AWS, Azure, and other Unix-based architectures.
•Experience applying Bayesian Techniques, Advanced Analytics, Neural Networks and Deep Neural Networks, Support Vector Machines (SVMs), and Decision Trees with Random Forest ensemble.
•Experience implementing industry standard analytics within specific domains and applying data science techniques to expand these methods using Natural Language Processing, implementing clustering algorithms, and deriving insight.
•In-depth knowledge of statistical procedures that are applied in both Supervised and Unsupervised Machine Learning problems.
•Machine Learning techniques to promote marketing and merchandising ideas.
•Proven creative thinker with a strong ability to devise and propose novel ways to look at and approach problems using a combination of business acumen and mathematical methods.
•Identification of patterns in data and using experimental and iterative approaches to validate findings.
•Advanced predictive modeling techniques to build, maintain, and improve on real-time decision systems.
•Contributed to advanced analytical teams to design, build, validate, and re-train models.
•Excellent communication skills (verbal and written) to communicate with clients, stakeholders, and team members.
•Ability to quickly gain an understanding of niche subject matter domains, and design and implement effective novel solutions to be used by other subject matter experts.

TECHNICAL SKILLS
•Analytic Development: Python, R, Spark, SQL.
•Python Packages: NumPy, Pandas, Scikit-learn, TensorFlow, Keras, PyTorch, Fastai, SciPy, Matplotlib, Seaborn, Numba.
•Programming Tools: Jupyter, RStudio, Github, Git.
•Cloud Computing: Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP)
Machine Learning, Natural Language Processing & Understanding, Machine Intelligence, Machine Learning algorithms.
•Analysis Methods: Forecasting, Predictive, Statistical, Sentiment, Exploratory and Bayesian Analysis. Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Clustering.
•Applied Data Science: Natural Language Processing, Machine Learning, Social Analytics, Predictive Maintenance, Chatbots, Interactive Dashboards.
•Artificial Intelligence: Classification and Regression Trees (CART), Support Vector Machine, Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, Regression, Naïve Bayes.
•Natural Language Processing: Text analysis, classification, chatbots.
•Deep Learning: Machine Perception, Data Mining, Machine Learning, Neural Networks, TensorFlow, Keras, PyTorch, Transfer Learning.
•Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series analysis.
•Soft Skills: Excellent communication and presentation skills. Ability to work well with stakeholders to discern needs. Leadership, mentoring.
•Other Programming Languages & Skills: APIs, C++, Java, Linux, Kubernetes, Back-End, Databases.

WORK EXPERIENCE
Bank of America, Charlotte, NC February 2020 - Present
Senior Data Scientist

At Bank of America, I worked as a Natural Language Processing expert and model architect where I built, trained, and tested multiple Natural Language Processing models which classified user descriptions and wrote SQL code based on user questions. The goal of the project was to centralize and search for Splunk dashboards within the Bank of America network, and to create an A.I. assistant to automate the coding process to extract information from these dashboards.

•Used Python and SQL to collect, explore, analyze the structured/unstructured data.
•Used Python, NLTK, and Tensorflow to tokenize and pad comments/tweets and vectorize.
•Vectorized the documents using Bag of Words, TF-IDF, Word2Vec, and GloVe to test the performance it had on each model.
•Created and trained an Artificial Neural Network with TensorFlow on the tokenized documents/articles/SQL/user inputs.
•Performed Named Entity Recognition (NER) by utilizing ANNs, RNNs, LSTMs, and Transformers.
•Involved in model deployment using Flask with a REST API deployed on internal Bank of America systems.
•Wrote extensive SQL queries to extract data from the MySQL database hosted on Bank of America internal servers.
•Built a deep learning model for text classification and analysis.
•Performed classification on text data using NLP fundamental concepts including tokenization, stemming, lemmatization, and padding.
•Performed EDA using Pandas library in Python to inspect and clean the data.
•Visualized the data using matplotlib and seaborn.
•Explored using word embedding techniques such as Word2Vec, GloVe, and Bert.
•Built an ETL pipeline that could read data from multiple macros, processed it using self-made preprocessing functions, and stored the processed data on a separate internal server.
•Automated ETL tasks and scheduling using self-built data pull-request functions.

Dominion Energy, Richmond, VA June 2017 – February 2020
Data Scientist / ML Ops Engineer

Worked as a Data Scientist for a large American power and energy company headquartered in Richmond, Virginia that supplies electricity and natural gas to various states. Member of a small team of data scientists and analysts where we created numerous demand forecasting models from Dominion’s historical data hosted on Hadoop HDFS and Hive to estimate short-term demand peaks for optimizing economic load dispatch. Models were built using Time Series analysis using algorithms like ARIMA, SARIMA, ARIMAX, and Facebook Prophet.

•Endeavored multiple approaches for predicting day ahead energy demand with Python, including exponential smoothing, ARIMA, Prophet, TBATS, and RNNs (LSTM).
•Successfully built a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) using PyFlux, to model the uncertainty of Dominion’s other time series, ensuring a ‘safety’ stock of generating units.
•Incorporated geographical and socio-economic data scraped from outside resources to improve accuracy.
•Incessantly validated models using a train-validate-test split to ensure forecasting was sufficient to elevate optimal output of the number of generation facilities to meet system load.
•Prevented over-fitting with the use of a validation set while training.
•Built a meta-model to ensemble the predictions of several different models.
•Performed feature engineering with the use of NumPy, Pandas, and FeatureTools to engineer time-series features.
•Coordinated with facility engineers to understand the problem and ensure our predictions were beneficial.
•Participated in daily standups working under an Agile KanBan environment.
•Queried Hive by utilizing Spark through the use of Python’s PySpark Library.

Cargill, Minneapolis, MN June 2015 – June 2017
Computer Vision Engineer

Cargill is an American privately held international food conglomerate; major businesses are trading, purchasing and distributing grain and other agricultural commodities. Our team used CNNs with Computer Vision to build the Machine Learning model to detect unhealthy hydrophytes. Our model helped regulators work more efficiently by detecting unhealthy hydrophytes in hydroponic farming automatically, and increased their harvesting rate which increased their revenue.

•Performed statistical analysis and built statistical models in R and Python using various supervised and unsupervised Machine Learning algorithms like Regression, Decision Trees, Random Forests, Support Vector Machines, K-Means Clustering, and dimensionality reduction.
•Used MLlib, Spark's Machine Learning library, to build and evaluate different models.
•Defined the list codes and code conversions between the source systems and the data mart enterprise metadata library with any changes or updates.
•Developed Ridge regression model to predict energy consumption of customers. Evaluated model using Mean Absolute Percent Error (MAPE).
•Developed and enhanced statistical models by leveraging best-in-class modeling techniques.
•Developed a predictive model and validated Neural Network Classification model for predicting the feature label.
•Implemented logistic regression to model customer default and identified factors that were good predictors.
•Designed a model to predict if a customer would respond to marketing campaign based on customer information.
•Developed Random Forest and logistic regression models to observe this classification. Fine-tuned models to obtain more recall than accuracy. Tradeoff between False Positives and False Negatives.
•Evaluated and optimized performance of models by tuning parameters with K-Fold Cross Validation.

Hilton Hotels, McLean, VA April 2014 – June 2015
Data Analyst

Worked with NLP to classify text with data draw from a big data system. The text categorization involved labeling natural language texts with relevant categories from a predefined set. One goal was to target users by automated classification. In this way we could create cohorts to improve marketing. The NLP text analysis monitored, tracked, and classified user discussion about product and/or service in online discussion. The Machine Learning classifier was trained to identify whether a cohort was a promoter or a detractor. Overall, the project improved marketing ROI and customer satisfaction. Also incorporated a Churn Analysis model to examine repeat business/dropoff.

•Worked the entire production cycle to extract and display metadata from various assets and helped develop a report display that was easy to grasp and gain insights from.
•Collaborated with both the Research and Engineering teams to productionize the application.
•Assisted various teams in bringing prototyped assets into production.
•Applied data mining techniques and optimization techniques standard to B2B and B2C industries, and applied Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.
•Utilized MapReduce/PySpark Python modules for Machine Learning and predictive analytics on AWS.
•Implemented assets and scripts for various projects using R, Java, and Python.
•Built sustainable rapport with senior leaders.
•Developed and maintained Data Dictionary to create metadata reports for technical and business purposes.
•Built and maintained dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.
•Kept up to date with latest NLP methodologies by reading 10 to 15 articles and whitepapers per week.
•Extracted source data from Oracle tables, MS SQL Server, sequential files, and Excel sheets.
•Parsed and manipulated raw, complex data streams to prepare for loading into an analytical tool.
•Involved in defining the source to target data mappings, business rules, and data definitions.
•Project environment was AWS and Linux.

EDUCATION
Bachelor of Arts - Data Science - University of California, Berkeley

"
,
Big Data Engineer,"
Experience working in Hadoop-as-a-Service (HAAS) environments with SyncSort-DMX-H, subversion (SVN), and SQL and NoSQL databases
Experience with Hadoop Big Data infrastructure for batch data processing and real-time data processing.
Design and build scalable Hadoop distributed data solutions using native, Cloudera and Hortonworks, Spark, Databricks, Delta Lake on Databricks, and Hive.
Experienced in Ansible, Jenkins, and PySpark.
Write Hadoop streaming applications with Spark Streaming and Kafka.
Handling of large datasets using partitions, Spark in-memory capabilities, broadcasts, joins, transformations in the ingestion process.
Experienced in Amazon Web Services (AWS), and cloud services such as EMR, EC2, S3, EBS and IAM entities, roles, and users.
Importing real-time logs to Hadoop Distributed File System (HDFS) using Flume.
Performance tuning of Spark jobs in Hadoop for setting batch interval time, level of parallelism, and memory tuning, and changing the configuration properties, and using broadcast variables.
Administration of Hadoop cluster(CDM); review of log files of all daemons.
Skilled in phases of data processing (collecting, aggregating, moving from various sources) using Apache Flume and Kafka.
Experience in Hadoop ecosystems with database and ETL.
Design and build big data architecture for unique projects, ensuring development and delivery of the highest quality, on-time and on budget.
Significant contribution to the development of big data roadmaps.
Creates and maintains environment configuration documentation for all pre-production environments
Able to drive architectural improvement and standardization of the environments.
Provides clear and effective testing procedures and systems to ensure an efficient and effective process.
Expertise in Storm for reliable real-time data processing capabilities to Enterprise Hadoop.
Extending HIVE and PIG core functionality by using custom User Defined Function's (UDF), User Defined Table-Generating Functions (UDTF) and User Defined Aggregating Functions (UDAF) for Hive and Pig.
Good Knowledge on Spark framework on both batch and real-time data processing.
Hands-on experience processing data using Spark Streaming API with Scala.
Clearly documents big data systems, procedures, governance and policies.



Apache
Apache Drill, Apache Kafka, Apache MAVEN, Apache Oozie, Apache Pig, Apache Hue, Apache Sqoop, Apache Flume, Apache Hadoop, Apache HBase, Apache HCatalog, Apache Ant, Apache Cassandra, Apache Lucene, Apache SOLR, Apache Airflow, Apache Camel, Apache Mesos, Apache Tez, Apache ZooKeeper
BI Visualization
Kibana, Tableau
Programming
C++, Visual Basic, JavaScript, R

Development
Drupal, UX Design
File Types
XML, Ajax, JSON, Avro, Parquet, ORC
APIs
Spark API, REST API, SOAP API
Development
Agile, Kanban, Scrum, Continuous Integration, TDD, Unit Testing, Functional Testing, Design Thinking, Lean, Six Sigma
Security & Forensics
SQL Injection, Data FTK imager, XXS
Soft Skills
Communication, Collaboration, Customer Service, Help Desk, Mentoring, Reviewing
Big Data
RDDs, UDFs, Data Frames, Datasets, Pipelines, Data Lakes, Data Warehouse, Data Analysis
Hadoop
Hadoop, HDFS, Hadoop YARN, Hortonworks, Cloudera, Impala
Spark & Hive
Apache Spark, Spark Streaming, Spark MLlib, GraphX, Apache Hive, Hive QL
Database
Redshift, ArangoDB, Cassandra, HBase, MongoDB, SQL, NoSQL, MySQL, RDBMS, Access, Oracle
File Management
HDFS, Snappy, Gzip, DAS, NAS, SAN
Cloud Services & Distributions
AWS, Azure, Anaconda Cloud, Elasticsearch, Solr, Lucene, Cloudera, Databricks, Delta Lake on Databricks, Hortonworks, Elastic. Cloud Foundry, Elastic Cloud
Software
Microsoft Office Suite, Peoplesoft, WEKA, PandaDoc
Operating Systems
Linux/UNIX, Windows
Virtualization & Network
SQL Injection, Data FTK imager, WAN/LAN, TCP/IP, Routing, VMware, VirtualBox, OSI Model

SENIOR DATA ENGINEER Feb 2021 – Present
FINRA, Washington, DC
Assigned to a cross-functional team that worked around 15 revolving members applying an Agile methodology with the mandate to develop deep learning patterns that ingested market trading data from multiple exchanges to analyze and produce exceptions and provide alerts where trading patterns showed signs of possible manipulation.
Worked with team members and stakeholders to understand the market and financial terminologies, systems, data structures, entities and relationships between entities, expected data formats and applicable operations on data.
Work with development team to build out patterns that monitored opening and closing market trading activities to determine anomalous behaviors and generate exceptions as well as alerts on a daily and monthly scale.
Stored specific consolidated and formatted market audit data in S3 bucket to be read into an AWS EMR cluster, and ran Python code against the data to perform operations and manipulations.
Applied functions and conversions on the data through various steps such as prep, running Tsfresh to automatically extract data from the data science models and output data to temporary HIVE tables and finally back to S3.
Configured solution so only the specific required amount of data is pulled in for the process and the data is all within this (AWS) environment and does not leave. Additions are made in subsequent iterations if end-user has requests not met in first iteration or the data scientist level has some improvements on the model that need to be adapted.
Executed Hadoop/Spark jobs on AWS EMR using program data stored in S3 Buckets.
Used Databricks running Spark Scala for event processing and transformations on event data.
Created reports using Jupyter Notebook.
Applied Duo Single Sign-On cloud-hosted SAML identity provider (IdP) for two-factor authentication.
Handled task tracking, backlog management and meeting preparation with the combination of Confluence docs and MIRO board.

SENIOR BIG DATA ENGINEER Jul 2020 – Feb 2021
Health Grades, Denver, CO
Building and working on data streaming pipelines to handle the creation and mastering of new and existing patients records to consolidate patient information across healthcare providers, sharing this information securely across multiple orgs (such as insurance, settlement companies, etc.) while maintaining confidentiality and privacy. The data is then used in the implementation of the CRM platform being built to offer value addition to the overall customer/patient healthcare experience.
Worked in a Databricks Delta Lake environment on AWS using Spark.
Ingested data using flat files and API’s using Kafka from MuleSoft and Salesforce.
Developed the schema and configurations for mapping the raw input format (typically CSV) into a common record format (CRF).
Created the schema and configuration to convert and map the CRF into an interchange format (CIF-Common Interchange Format) for interacting with another system called Reltio (Mastering data management tool).
Developed the code which handles data type conversions, data value mappings and checking for required fields.
Developed the programs using Spark using Python (Pyspark).
Mapped the ingested data to the delta lake table schema and loaded into its table in delta lake and snowflake.
Coded functionality to handle merging records in Reltio by adding an additional field in the CRF.
Modified the pipeline for streaming messages to receive the new incoming records for merging, handling missing (NULL) values and triggering a corresponding merge in salesforce records on receipt of a merge event.
Worked on the task for propagate down any updates or creation of new Persons records in salesforce to be reflected in the delta lake tables.
Created an entirely new Common Record Format schema for the Salesforce person record, adding all the necessary mapping configurations.
Added functions to transform select fields being returned from Reltio into a recommended JSON body structure for the Kafka topic to be published onto the salesforce Kafka topic.
Supported the QA and MuleSoft teams that work on testing and troubleshooting spark job run failures.
Created and managed Kafka topics and producers for the streaming data.

SENIOR BIG DATA ENGINEER February 2019 – July 2020
McGraw-Hill Education, Boston, MA
Open Learning data migration – all McGraw-Hill data was stored in old legacy systems and that data needed to be consolidated and migrated into the new Open Learning ecosystem. This involved building streaming pipelines to stream this data into DataMarts.
Worked in a Databricks environment on AWS using Spark.
Migrated data stored in old legacy systems that used batch ETL jobs to load data to streaming pipelines that stream real-time events and store them in DataMart.
Used Databricks, Jenkins, CircleCI, IntelliJ IDEA, third-party tools in software development.
Transferred Data from Data Lake comprised of AWS buckets where the raw events are stored into DataMart which is loaded via Spark streaming pipelines.
Used Databricks running Spark Scala for event processing and transformations on event data.
Loaded the transformed data into staging tables for data analysis with database functions.
Followed Agile Scrum processes for Software Development Lifecycle (SDLC) with 2-week Sprints and daily 30-minute standups (Scrums).
All relevant documentation created and recorded in Confluence pages.
Tasks, sprints, stories and backlog management tracked using Jira Agile development software.
Major contributions included design, code, configuration and documentation for components that manage data ingestion, real time streaming, batch processing, data extraction and transformation.
Building Zero-Down-Time Spark Scala pipelines that ingest data from McGraw-Hill software products.
Built data ingestion and processing pipelines with Spark on Databricks.
Built complex SQL functions to fetched data and returned results used in reports and other API’s.

SENIOR BIG DATA ENGINEER August 2017 – February 2019
Home Depot, Atlanta, GA
Imported data from AWS S3 and into Spark RDD and performed transformations and actions on RDD’s.
Utilized Amazon Web Services (AWS) Cloud services like EC2, S3, EBD, RDS and VPC.
Used Spark-SQL to Load JSON data and created Schema RDD and loaded it into Hive Tables and handled Structured data using SparkSQL.
Involved in HBASE setup and storing data into HBASE, which will be used for analysis.
Loaded data into spark RDD and do in memory data computation to generate the Output response.
Implemented Spark using scala and SparkSQL for faster testing and processing of data.
Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive.
Designed and created Hive external tables using shared meta-store instead of Derby with partitioning, dynamic partitioning and buckets.
Created solutions to transform data from various sources and load it into platforms such as Hadoop, Snowflake to create a data lake.
Configured SQL database to store Hive metadata.
Installed and configured Apache Hadoop and Hive environment on the prototype server.
Loaded unstructured data into Hadoop File System (HDFS).
Excellent understanding of Hadoop Architecture and underlying Hadoop framework including Storage Management.
Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce, Hive, HDFS, Spark, Kafka and Apache NiFi.
Experience in importing and exporting data using Sqoop 1.99.7 from HDFS to RDBMS and vice-versa.
Experience in analyzing data using HiveQL 2.1.1 and custom MapReduce programs in Java.
Hands on experience in Linux Shell Scripting. Worked with Big Data distribution Cloudera.
Developed complete end-to-end Big Data processing in Hadoop eco system.
Developed spark code using Scala and Spark-SQL/Streaming for faster processing of data.
Created Hive Schemas using performance techniques like partitioning and bucketing.
Imported data from different sources like HDFS/Hbase 0.94.27 to Spark RDD.
Developed MapReduce program to extract and transform the data sets and resultant dataset were loaded to Hbase and vice-versa using Kafka 2.0.x.

BIG DATA ENGINEER May 2016 – August 2017
Foot Locker, New York, NY
Responsible for that day-to-day operation and support of Hadoop environments.
Collaborate with Corporate IT function around integrating Hadoop ecosystems with critical enterprise systems.
Responsible for development and management of all cluster related testing activities.
Responsible for creating roadmaps for ongoing cluster deployment and growth.
Function as expert consulting resources around Hadoop integration points with any ETL and BI teams.
Setup, installed, and monitored 3-node enterprise Hadoop cluster on Ubuntu Linux
Analyzed and interpreted transactions behaviors and clickstream data with Hadoop and HDP to predict what customers might buy in the future.
Implemented Hadoop data pipeline to identify customer behavioral patterns, improving UX on e-commerce website.
Develop MapReduce jobs in Java for log analysis analytics and data cleaning.
Perform big data processing using Hadoop, MapReduce, Sqoop, Oozie and Impala.
Import data from MySQL to HDFS, using Sqoop to load data.
Analyze Hadoop clusters using big data analytic tools including Hive, and MapReduce.
Conduct in-depth research on Hive to analyze partitioned and bucketed data.
Leveraged Sqoop to import data from RDBMS into HDFS.
Integrated Hadoop into traditional ETL, accelerating the extraction, transformation, and loading of massive structured and unstructured data.
Loaded unstructured data (Log files, XML data) into HDFS using flume.
Used Hive to analyze partitioned and bucketed data and compute various metrics for reporting.

BIG DATA ENGINEER February 2015 – May 2016
KOMODO Health, San Francisco, CA
Building large-scale and complex data processing pipelines.
Solid experience with Apache Spark data structures, critical features and performance tuning.
Cluster and configuration management systems, like Docker and Kubernetes
Amazon AWS services (EC2, S3, RDS etc)
Analyze Hadoop clusters using big data analytic tools including Hive, and MapReduce.
Developed Spark code using Scala and Spark-SQL/Streaming for faster processing of data.
Used Spark-SQL to Load JSON data and created Schema RDD and loaded it into Hive Tables and handled Structured data using SparkSQL.
Used Spark API over Hortonworks Hadoop YARN to perform analytics on data in Hive.
Worked on analyzing Hadoop cluster and different big data analytic tools including MapReduce, Hive, HDFS, Spark, Kafka and Apache NiFi.
Responsible for development and management of all cluster related testing activities.
Excellent understanding of Hadoop Architecture and underlying Hadoop framework including Storage Management.

HADOOP SYSTEM ADMINISTRATOR November 2013 – February 2015
Cargill, Minnetonka, Minnesota
Helped the team to increase cluster size from 16 nodes to 52 nodes. The configuration for additional data nodes managed by using Puppet.
Installed, Configured and deployed a 30 node Cloudera Hadoop cluster for development and production
Worked on setting up high availability for major production cluster and designed for automatic failover.
Performance tune Hadoop cluster to achieve higher performance.
Configured Hive meta store with MySQL, which stores the metadata of Hive tables
Configured Flume for efficiently collecting, aggregating and moving large amounts of log data.
Enabled Kerberos for Hadoop cluster Authentication and integrate with Active Directory for managing users and application groups. Used Ganglia and Nagios for monitoring the cluster around the clock.
Wrote Nagios plugins to monitor Hadoop NameNode Health status, number of Task trackers running, number of Data nodes running.
Designed and implemented a distributed network monitoring solution based on Nagios and Ganglia using puppet.
Developed multiple MapReduce jobs in java for data cleaning and preprocessing.
Developed HIVE queries and UDFs to analyze in HDFS and Performed Analyzing/Transforming data with Hive and Pig.
Performed various configurations, which includes, networking and IPTable, resolving hostnames, user accounts and file permissions, http, ftp, SSH keyless login. Moved data from HDFS to RDBMS and vice-versa using SQOOP.
Worked with the Linux administration team to prepare and configure the systems to support Hadoop deployment
Created volume groups, logical volumes and partitions on the Linux servers and mounted file systems on the created partitions.
Upgraded the Hadoop cluster from cdh3 to cdh4. Designed and allocated HDFS quotas for multiple groups.
Implemented Fair scheduler on the job tracker to allocate the fair amount of resources to small jobs.
Implemented Kerberos for authenticating all the services in Hadoop Cluster.
Deployed Network file system for Name Node Metadata backup.
Configured and deployed Hive metastore using MySQL and thrift server.
Environment: Hadoop, HDFS, Map Reduce, Hive Pig, Sqoop, Oozie, HBase, Linux, Java, Xml.

LINUX SYSTEMS ADMINISTRATOR March 2010 – November 2013
Carousel Industries, Exeter, RI
Performed system administration technical work tasks on Linux-based systems for multiple clients and provided technical support via telephone/email to thousands of users.
Configured DNS and DHCP on clients’ networks.
Created database tables with various constraints for clients accessing FTP.
Built, installed, and configured servers from scratch with OS of RedHat Linux.
Performed Red Hat Linux Kickstart installations on RedHat 4.x/5.x, performed Red Hat Linux Kernel Tuning, memory upgrades.
Installed, configured, and performed troubleshooting of Solaris, Linux RHEL, HP-UX, and AIX operating systems
Applied OS patches and upgrade.
Upgraded administrative tools and utilities and configured and added new services.

Vincent Nwobodo
SENIOR HADOOP ENGINEER
PROJECT SUMMARY
ANALYSIS
DASHBOARD
DATA CLEANING
TRANSFORMING
ARCHITECTURE
SCRIPTING
FINRA // Washington, DC
SENIOR DATA ENGINEER

HEALTH GRADES // Denver, CO
SENIOR BIG DATA ENGINEER

MCGRAW-HILL EDUCATION // Boston, MA
SENIOR BIG DATA ENGINEER

HOME DEPOT // Atlanta, GA
SENIOR BIG DATA ENGINEER

FOOT LOCKER // New York, NY
BIG DATA ENGINEER

KOMODO Health // San Francisco, CA
BIG DATA ENGINEER

CARGILL // Minnetonka, MN
HADOOP SYSTEM ADMINISTRATOR

CARGILL // Minnetonka, MN
HADOOP SYSTEM ADMINISTRATOR
Big Data Engineering with strong understanding of Hadoop big data architectures, data movement technologies, database partitioning, database optimization, and building communication channels between structured and unstructured databases; cloud technologies and tools.

8 Years Big Data Engineering
3+ Years in IT Admin
EDUCATION
MASTERS DEGREE IN COMPUTER INFORMATION SYSTEMS
University of Michigan, Flint
Flint, Michigan

BACHELOR OF SCIENCE IN INFORMATION SYSTEMS
American University
Nigeria

TRAINING
AWS Associate
(720) 545- 2585
adomch@r.postjobfree.com
Boston, MA
PROFESSIONAL SKILLS
LEADERSHIP
PROJECT MGMNT
COMMUNICATION
PROFESSIONAL SUMMARY
SKILLS SUMMARY
PROFESSIONAL EXPERIENCE

"
,
Data Scientist,"
PROFESSIONAL PROFILE
Data Scientist and Machine Learning Engineer with 8 plus years in using advanced mathematical techniques to create actionable solutions to business and scientific problems. Extensive experience in using object-oriented programming to perform statistical modeling, data mining, machine learning, artificial neural networks and various optimization algorithms. A highly responsible data scientist who is punctual and vigilant in sustaining effective communication, writing efficient and well documented code, creating intuitive visualizations, and providing practical results.
TECHNICAL SKILLS
Python Packages: PyTorch, NumPy, Pandas, Scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn.
Programming Languages: Python, R, MATLAB, Linux, Latex Data Systems SQL, MySQL, NoSQL, AWS (RDS, RedShift, Kinesis, EC2, EMR, S3), MS Azure, Spark, Hive, Hadoop.
IDEs: Spyder, Jupyter, PyCharm, Rstudio, Eclipse.
Development Tools: GitHub, Git, IPython notebook, Trello, SVN.
Statistical Methods: Bayesian Statistics, Hypothesis Testing, Factor Analysis, Stochastic Modeling, Factorial Design, ANOVA Optimization Techniques Linear Programming, Dynamic Programming, Convex Optimization, Non-Convex Optimization, Monte Carlo Methods, Network Flows.
Machine Learning Frameworks: TensorFlow, Torch, Keras, Caffe Python Libraries NumPy, Pandas, SciPy, Matplotlib, scikit-learn, Caffe, NLTK, StatsModels, Seaborn, Selenium.
Deep Learning: Keras, TensorFlow, PyBrain,
Analysis Methods: Unsupervised Learning K-means Clustering, Hierarchical Clustering, Centroid Clustering, Principle Component Analysis, Gaussian Mixture Models, Singular Value Decomposition Supervised Learning Naive Bayes, Time Series Analysis, Survival Analysis, Linear Regression, Logistic Regression, ElasticNet Regression, Multivariate Regression.
Applied Data Science: Natural Language Processing, Deep Learning, Transfer Learning, Auto encoding/decoding.
Soft Skills: Quick learner. Goal-oriented. Write well-documented code, Able to manage time and prioritize tasks. Practice ego-free collaboration and communication. Great at presenting complex results to non-technical audience.

PROFESSIONAL EXPERIENCE
January 2020-Present
Location: New York, NY (Remote)
Company: Pfizer
Position: Lead Data Scientist
Pfizer is an industry leader in conducting clinical trials for pharmaceuticals. However, each trial can take years to complete and nearly 31% of trial participants withdraw before its conclusion, costing Pfizer millions of dollars. In order to stay ahead of the curve. Pfizer has a robust research effort. One of the areas of research is cell mutation and protein folding. Genetic mutations can change the protein stability through alteration in protein conformation and dynamics. To investigate this, the unfolded state of proteins was described using a statistical coil model and the folding free energy changes were obtained using Molecular Dynamics simulation and MMPBSA method. Different machine learning algorithms such as Regression, Random
Forest (RF), Gradient Boosting (GB) and Artificial Neural Network (ANN) were implemented for this purpose. The experimental values for stability changes were used to check the performance of the models.
Used unsupervised learning techniques such as K-means clustering and Gaussian
Mixture Models to cluster customers into different risk groups based on health
parameters provided through wearable technology regarding their activities and health goals
Multiple statistical modeling approaches were applied to determining the usefulness of the wearable technology data for various insurance products.
Survival modeling techniques, such as Poisson regression, hidden Markov models, and Cox proportional hazards, were used to model time to different events utilizing
wearable data (time to death for life insurance, time to next hospital visit, time to next accident, time to critical illness, etc.).
Data required extensive cleaning and preparation for machine learning modeling, as some observations were censored without any clear notification.
We solved a binary classification problem (transferring to lower risk group or not with given financial incentive) with a logistic regression.
An artificial neural network was utilized with Keras/TensorFlow in Python to solve binary classification problem for premiums and their intersection with the discriminant.
We used modifiers, including L1 regularization, dropout, and Nesterov momentum to enhance the neural network and optimize generalization

January 2017-December 2019
Location: Atlanta, GA
Company: IBM
Position: NLP Engineer

Worked with IBM’s L1 support team to automate assistance for users with software issues and crashes. Used anomaly detection algorithm to find periods of increased error rates with an NLP solution to determine what categories the anomalous logs belong to. Utilized Watson Discovery to find articles relevant to the logs and direct users to them. Incorporated feedback for continuous model training.
Tokenization using NLTK.
Stemming.
Filtering stop words.
Creating individual functions within a class for each prompt to the user K-Means
clustering.
Cosine similarity.
Flask deployment.
Used Keras, Torch and other frameworks including Theano and TensorFlow for neural network generation and optimization.
Utilized Python libraries such as Pandas, NumPy and Plotly to preprocess and clean text data.
Trained classification models on text classes using transfer learning techniques.
Used a Convolutional Neural Network which was trained on text classification. Anomaly detection and Root Cause Analysis.
Unified consumer profile with probabilistic record linkage.
Visual search for similar and complementary products.
Architect, build, maintain, and improve new and existing suite of algorithms and their underlying systems.
Implemented deployment solutions using Tensoflow, Keras, Docker and Elastic
Kubernetes Service.
Implemented Model Drift Monitoring and Retraining Strategies.
AWS, GCP, Azure.
Sagemaker Model transformation into dedicated preprocessing, inference, and model validation scripting.
ECR, EMR, Azure Kubernetes Service Experience.

June 2015-January 2017
Location: Greenville, SC
Company: Michelin
Position: ML Engineer/Supply Chain Solutions
Consultant

Michelin is one of the world’s leading companies and the largest single employer in South Carolina. As a Supply Chain Solutions consultant, my responsibilities included creating demand forecasting analysis of the whole automotive industry in the United States. This was done by creating Sales prediction using time series analysis: I developed a tool for predicting sales of a company based on its previous sales records. I used time series analysis techniques with
Tensorflow and Keras. The goal was to provide the company analysis, insight and suggestions for the future. Since time series analysis can be easily applied to different use cases, this model can be used in many other enterprises. Data was scraped from online sources using SQL queries among other tools. Python’s statsmodels package and Arima model were used in this project. The franchises were analyzed both individually and in groups. The model successfully identified branches that are doing well as well as those that are not performing as expected:
Data scraping and preprocessing.
Classification of the branches based on their size.
Training and testing a time series model to forecast the future sales of each group.
Identifying branches that perform the best and the worst among all branches.
Using statsmodels to decompose the time series into trend, seasonality and residual data.
Using Dicky-Fuller test to prove that the residual has stationary data.
Using the Arima model to model the stationary data.
Training and testing a time series model to forecast the future sales of the individual branches.
Modeling the trend, seasonality and the stationary data, combining them to provide the forecast for future.
Providing insight and suggestions to the managerial staff for the future.
Developed and delivered an Android radio application for Honda Motor Co using
Android, Android SDK, Java, C++, XML, JSON, Gradle, eclipse, GIT, SourceTree.
Responsible for planning, monitoring and execution of deployment and product
releases.
Participation in status meetings, progress report to track progress, risk management,
defect triage, defect tracking and resolution.

January 2014-May 2015
Location: Greenville SC
Company: Cryovac-SealedAir
Position: Computer Vision Specialist
Cryovac is a food packaging and packaging machinery company. Cryovac is the second largest employer in South Carolina. As part of the Computer Vision initiative, my responsibilities
included creating a computer vision-based algorithm to identify unproperly sealed items and establish a checklist for future computer vision aided QA systems. The work was in early days and was meant to be a proof of concept only.

With the PyTorch Python API, the team built the architecture and trained the
convolutional neural networks (CNN).
Exploited transfer learning with custom-built classifiers in PyTorch to speed up
production time and improve results.
Fine-tuned Vgg16 and other models to adapt their pre-trained weights to our use case.
Used a fully convolutional network (FCN) - pre-trained YOLO algorithm - to speed up predictions.
Took into consideration prediction time and overhead to make sure our predictions
happened in real time.
Regularized the data by applying transformations to the images using Pillow.
Worked with large stores of video imaging data stored on AWS S3 buckets for training the model.
Supplied pickled model to the software development team to integrate into QA
machinery.

EDUCATION
BS in Physics International IK University

MS in Condensed Matter Physics BeHESHTI University, Tehran

PhD in Computational BioPhysics, CLEMSON, USA

"
,
NLP Scientist/Engineer,"
Xing Jie (Jimmy) Zhong
*** * ****** **, ***** Bend, IN 46615 • 720-***-**** • adoj4f@r.postjobfree.com https://www.linkedin.com/in/xing-jie-zhong

PROFESSIONAL SUMMARY
●Research experience in Natural Language Processing, with significant focus on Machine Translation, and extensive experience with transformer architecture.
●4 years of experience in designing and building machine learning algorithms and neural networks.
●8 years of software development experience. 4 years of non-academic, non-internship software development experience.

EDUCATION
University of Notre Dame Notre Dame, IN
M.S. Computer Science and Engineering expected December 2021
Advisor: David Chiang
GPA: 3.9/4.0

University of Colorado at Boulder Boulder, CO
B.S. Electrical and Computer Engineering May 2014
Minor: Computer Science
GPA: 3.8/4.0 (Magna Cum Laude)

SKILLS:
Programming Languages: Python (proficient), C++ (intermediate), C (intermediate), Java (intermediate), HTML (intermediate), Pascal (beginner)
Data Science/Machine Learning Packages: Pytorch (proficient), Tensorflow (proficient), MxNet (beginner), Keras (proficient), Numpy (intermediate), Opencv (intermediate), ScikitLearn (beginner)
Cloud Platforms: AWS (intermediate)
Languages: Mandarin (Bilingual biliterate)

RESEARCH EXPERIENCE
Amazon Web Services - AI services New York, NY
Machine Translation Science Team - Applied Scientist Intern May 2021 – Aug 2021
●Explored state-of-the-art character based models and designed experiments. Curated and cleaned multi-domain datasets.
●Proposed simplification to existing MT pipeline inspired from CANINE. This resulted in ~2 bleu improvement on noisy datasets and 10 improvement on robustness.
●Set-up environments to train neural networks on the cloud using AWS.

University of Notre Dame Notre Dame, IN
Graduate Research Assistant, ND Artificial Intelligence Lab Aug 2018 – Present
●Researched, conceptualized and implemented various encoding methods to incorporate non-parallel data to existing machine translation neural networks.
●Developed dictionary attachment architecture to improve neural machine translation models.
●Programmed multi-task model to translate text and classify definitions simultaneously.
●Implemented transformers to predict word embeddings.

University of Colorado at Boulder Boulder, CO
Undergraduate Research Assistant, Correll Lab (Sep 2013 – Sep 2014)
●Prototyped soft robotics actuator and soft circuits using solid works and 3D printer.
●Produced first functional fully-embedded elastomer circuits.

Harvard University Cambridge, MA
Undergraduate Research Assistant, Microrobotics Lab (Jun 2012 – Aug 2012)
●Developed both mechanical and electrical design of a successful new robot prototype.
●Fabricated mechanical structures using a laser cutter and produced circuits using Etched Copper.
●Presented research results at the end of summer to the REU program and lab.

University of Colorado at Boulder Boulder, CO
Undergraduate Research Assistant, Programming Development Lab (Feb 2012 – May 2013)
●Ran test and collected data of a BLAS optimizing compiler.
●Wrote scripts to parse and process data files in Matlab.
●Compiled data into graphs of computer performance and noise.
●Attended SIAM CSE 2013 conference for poster presentation.

PROFESSIONAL EXPERIENCE
Keysight Technologies Colorado Springs, CO
R&D Test Development Engineer Jun 2015 –Jul 2018
●Successfully ported legacy code from Pascal into Labview.
●Trained technicians to run, test, and debug various product issues.
●Collected and processed product and system data for the validation of new systems.

Level 3 Communications Lone Tree, CO
Java Middleware Developer Sept 2014 - Jun 2015
●Configured and upgraded existing software systems and implemented new features.
●Work within a large team using agile methods to deploy new software features and debug system-level issues.

PUBLICATIONS
Look it up: Bilingual and Monolingual dictionaries improve neural machine translation. https://www.statmt.org/wmt20/pdf/2020.wmt-1.65.pdf

LEADERSHIP EXPERIENCES
Vice President of Graduate Student Government – University of Notre Dame 2021 - 2022
Social Committee Chair for Graduate Student Government - University of Notre Dame 2020 - 2021
President of Eta Kappa Nu (EE honor society) - University of Colorado Boulder 2013 - 2014
Secretary of Tau Beta Pi (Engineering honor society) - University of Colorado Boulder 2011 – 2013

"
,
Business Analyst Data,"
Snehal PGBABI (Business Vora Analyst & Business Intelligence) MS (Biotechnology and Endocrinology)
EXPERIENCE
EDUCATION
PROJECTS
DATA ANALYTICS SKILLS
CONTACT
PERSONAL STRENGTHS
TOOLS
• Machine Learning
• Predictive Modelling
• Regression Analysis
• Time Series Forecasting
• Web Scraping and Sentiment
Analysis
• Ensemble Techniques
• Model Deployment (R shiny)
• Neural Networking
• R, Python
• SAS, KNIME
• SQL
• Tableau
• Google Analytics
• Advanced Excel.
• Logical Thinking
• Problem Solver
• Executing in a Fast-Paced
Environment
• Self starter
• Team player and Fast learner
• Motivated
• adn6dt@r.postjobfree.com
• 315-***-****
• Chicago, IL
• linkedin.com/in/snehalvora
Entrepreneur -Learning Ladderz (Mumbai, India) Sept 2014 – Sept 2019
• Owned and managed children's circulating toy and books library
• Complete set-up of fully operational circulating library where kids could exchange toys and books for a monthly fee-based model.
• Analyzed data of subscribed clients including patterns of toys & books chosen to develop marketing strategies and promotions on social media platforms. Intern - Haffkine Institute (Mumbai, India ) June 2003 – July 2003 On-job training under the Annual Biotechnology Program conducted by the Department of Clinical Pathology, Virology and Zoonoses.
Intern -Microbiology and Pathology Lab (Mumbai, India) April 2002 – May 2002 Highly enthusiastic with strong aptitude for analysis of complex data to build successful business models. Passionate and team oriented problem solver with degree in Business Analytics and Business Intelligence. Hands on experience with data from Healthcare, Sales, Inventory & Marketing, Retail, Finance & Insurance industry. Seeking for the position of a Data Scientist, offering statistical modeling techniques, advanced computer skills, expertise in biotechnology and analyzing large complex and multi-dimensional data set using a variety of tools. PGBABI (Post Graduate in Business Analytics and Business Intelligence) 2019 - 2020 University of Texas at Austin,Red Mc
MS (Master of Science in Biotechnology and Endocrinology) 2004 – 2006 University of Mumbai, India
BS (Bachelor of Science in Biotechnology and Zoology) 2001 - 2004 University of Mumbai, India
Model creation and Recommending ways to increase Life Expectancy July 2020 Health-care data sourced from U.N. and W.H.O. websites. Predictive analysis, Time series Analysis, Regression analysis was performed to build machine learning models to predict Life Expectancy of 193 countries with accuracy of 94%. Visualization of trends in life expectancy across years and across countries done in Tableau. Found the top 5 variables that influence life expectancy with a quantifiable score.
Recommending ways to increase revenue of a Coffee Chain May 2020 Marketing data analysis of Point of Sale (POS) Data for a coffee chain restaurant. Used Techniques like Market Basket Analysis, Churn Rate Prediction, RFM Analysis. Build visualizations achieve revenue growth by recommending taking off certain items from the menu and suggesting popular combo meals for customers.
Creation of Credit Risk Default Model for Bank and model deployment (R-shiny)April 2020 Developed a credit risk default model using data from financial bank. Using Logistic Regression, Univariate & Bivariate Analysis, Model Performance Measures, Build a forecasting model to predict monthly gas production March 2020
• Objective: Analyzing and Predicting Australian gas production.
• Tools: R, Tableau and Excel

"
,
"Biochemist, biophysicist","
Gary Angles
US citizen, Bilingual

**** ******* ** ***
San Diego, CA 92111
505-***-****
adn01w@r.postjobfree.com
adn01w@r.postjobfree.com

Chemistry, biochemistry & biophysics with molecular dynamics
Molecular dynamics, AMBER, Gromacs, Gaussian G16; Cell Cultures, HPLC, Protein Separation, SDS page, Western Blot; c++, c#, python, php, CUDA, Machine learning. Speak Spanish, English, Incan Languages, Beginner French; Licensed soccer coach and skydiver.

Education
PHD 2019 New Mexico Institute of Mining and Technology, Socorro, NM
· Major: Chemistry, computational chemistry, with biochemistry& biophysics.
MS 2013 New Mexico Highlands University, Las Vegas, NM
· Major: Chemistry, with biochemistry, organic, inorganic synthesis, and crystallography.
GRADUATE STUDIES 2005-2007 University of New Mexico, Albuquerque, NM
· Area of study: Graduate program chemical engineering.
BS 1997 New Mexico Highlands University
· Major: Chemistry, with computer science and software engineering.

Experience
Nanome Inc, San Diego, CA Jan 2021 – Mar 2021 Applications Scientist (Contract)
· Contributed to the testing and development of virtual reality (VR) applications.
· Assisted biopharma clients customize, troubleshoot the VR software to their research.
· Generated leads & sales in scientific networks, and biopharma.
UCSD, San Diego, CA Nov 2019 – Nov 2020 Postdoc
· Developed & achieved prediction of protein effects of biomaterials mechanical properties.
· Achieved and established methods for biomaterial properties measurement.
Los Alamos National Labs, Los Alamos, NM Feb 2018 – Nov 2019 Research Assistant
· Achieved theoretical prediction to design new explosive triggering compounds, properties, and yields.
· Achieved, with a team, the development of IED & chemical weapons detection methods and devices.
New Mexico Institute of Mining and Technology, Socorro, NM Jan 2014 – Sep 2019 Research Associate
· Developed a simplified method to characterize diffusion permeation through biomaterials macro models.
· Proposed and achieved corrections needed to EPR measurement methods that were unquestioned.
New Mexico Highlands University, Las Vegas, NM Jan 2011 – May 2013 Research Assistant
· Biochemistry: Worked on cell cultures, protein expression, HPLC.
· X-ray diffractometry of small molecules and proteins.
· Synthesis of organic & inorganic compounds and characterization.

Publications
· Romanov AS, Angles GF, Antipin MY, Timofeeva TV, Acta Cryst. (2012) C68, m69-m72.
· Mohapatra SK, Angles G, Marder SR, J of Organometallic Chem. (2012) Volumes 706–707, 140–143.
· Romanov AS, Angles GF, Timofeeva TV, Corsini M, Fabrizi de Biani F, CrystEngComm (2015) 17, 7564-7573.
· Angles G, Dotson R, Bueche K, Pias SC, Advances in experimental medicine and biology, ISOTT (2016) 977, 9-14.
· Dotson R, Smith C, Bueche K, Angles G, Pias S, Influence of cholesterol on the oxygen permeability of membranes: insight from atomistic simulations, Biophysical Journal (2017) 112, June 6, 2336-2347.
· Nguyen TD, Veauthier JM, Chavez DE, Angles G, Nelson TR, Lapsheva E, Schelter EJ, Correlating Mechanical Sensitivity with Spin Transition in the Explosive Spin Crossover Complex [Fe(Htrz)3]n[ClO4]2n, JACS (2019)142, 10, 4842-4852
· Kaseman D, Nelson TR, Angles G, et.al., Chemical Analysis of Fluorobenzenes via Multinuclear Detection in the Strong Heteronuclear J-Coupling Regime, Applied Sciences (2020) 10, 11, 3836.
· Kaseman D, Nelson TR, Angles G et.al., J-Coupled Spectroscopy prediction of nerve gas agents at Earth’s Magnetic Field, Journal of Magnetic Resonance (2020) Submitted.
· Angles G, Pias S, Markov chain Monte Carlo solution for steady state permeation through lipid bilayers, Biophysical Journal (2020) Under review.
· Angles G, Pias S, Atomistic simulations modify interpretation of spin-label oximetry data. Part 1: intensified water–lipid interfacial resistances, Biophysical Journal (2021) Submitted.
· Angles G, Pias S, Discerning Membrane Steady-State Oxygen Flux by Monte Carlo Markov Chain Modeling, Advances in Experimental Medicine and Biology (2021) 126*-***-***.
· Qi Wang, Dotson R, Angles G, Pias S, 3 Simulation Study of Breast Cancer Lipid Changes Affecting Membrane Oxygen Permeability: Effects of Chain Length and Cholesterol, Oxygen Transport to Tissue XLII (2021) 1269, 15.

Presentations
· Angles G, Romanov AS, Timofeeva TV, “Synthesis, Characterization and Structures of Metallocene compounds” American Crystallographic Association in Boston, MA. July 28, 2012.
· Angles G, Pias SC, ""Cross Validation of simulation and EPR oximetry approaches: membrane cholesterol reduces oxygen flux"" NM IMBRE biotechnology annual meeting, Santa Fe, NM March, 2015; and at Biophysical Society 60th annual meeting, Los Angeles, CA. February 27, 2016.
· Angles G, Pias SC, ""Journey to the center of a cell membrane: cavities that propel diffusion; cavity analysis of cellular membranes"" NM IMBRE biotechnology annual meeting, Santa Fe, NM March, 2016.
· Angles G, Pias SC, “Lower than Expected Membrane Permeability to Oxygen, Especially When Cholesterol Content Is High” International society of oxygen transport to tissue (ISOTT), Chicago, IL July 2016.
· Angles G, Pias SC, "" Molecular Simulations to Explain Experimental Oxygen Probe Behavior"" NM IMBRE biotechnology annual meeting, Santa Fe, NM March, 2017.
· Angles G, Pias SC, “Tempocholine membrane probes measure oxygen in hydrophobic regions: insight from molecular” International society of oxygen transport to tissue (ISOTT), Halle/Saale, Germany, August, 2017.
· Angles G, Pias SC, “Rapid and easy calculations of oxygen transit across model cellular membranes” NM IMBRE, August, 2018.
· Angles G, Pias SC, ""Cross Validation of simulation and EPR oximetry approaches: membrane cholesterol reduces oxygen flux"" Biophysical Society 2019th annual meeting, Baltimore, MA. March 2, 2019.
· Angles G, Pias SC, “Discerning Membrane Steady-State Oxygen Flux by Monte Carlo
Markov Chain Modeling” International society of oxygen transport to tissue (ISOTT), Albuquerque, NM. July, 2019.

Lab experience
New Mexico Institute of Mining and Technology, Socorro, NM:
· Biochemistry Laboratory. Co-Instructor (January 2015 – May 2017).
· General Chemistry, Co-Instructor (August 2013-2016).
New Mexico Highlands University, Las Vegas, NM:
· General Chemistry Laboratory. Teaching Assistant (August 2010 – May 2012).
· Chemistry Laboratory 7, Teaching Assistant (August 2011 – December 2011).
· Instrumental Analysis Laboratory. Teaching Assistant (January 2011 – May 2011).
· Biochemistry Laboratory. Teaching Assistant (January 2011 – May 2011).

Grants/Awards
· Graduate Assistant Chemistry 21382 Research fund (2010-2012), New Mexico Highlands University, PREM LMITA, CMDITR.
· Graduate Assistant Chemistry, Teacher Assistant grant (2010-2012), New Mexico Highlands University, NSF.
· Graduate Assistant Chemistry, Teacher Assistant grant (2014-2017), New Mexico Institute of Mining and Technology.
· NIH under NIGMS grant P20GM103451 & NM INBRE.
· FASEB postdoctoral preparation institute, August 10-11, 2017 at National Harbor, MD.
· Investment club, scholarship award, spring 2018 semester.
· New Mexico Tech. Chemistry Dept. research excellence award, researcher of the year, May 2018.
· NISBRE, NIH, NIGMS seventh biennial national IDeA symposium of biomedical research excellence, (June 24-26, 2018) Wardman Park Marriott, Washington, DC.
· International Society of Oxygen Transport to Tissue, Duane Bruley award to research excellence, July 31, 2019.

Workshops
· Computational biophysics workshop, Berkeley, Energy Biosciences Building University of California Berkeley, Berkeley, CA (Klaus Schulten & Emad Tajkhorshid).
· FASEB postdoctoral preparation institute, August 10-11, 2017 at National Harbor, MD: Professional development, CV clinic, interview training, funding opportunities, networking skills.
· NISBRE, NIH, NIGMS seventh biennial national IDeA symposium of biomedical research excellence, (June 24-26, 2018) Wardman Park Marriott, Washington, DC.
· LAMMPS & computational chemistry of materials workshop sponsored by Sandia National Labs (August, 2019) Albuquerque, NM.

References
· Christopher Lee, UCSD, adn01w@r.postjobfree.com, 703-***-****
· Ali Behzadan, UCSD, adn01w@r.postjobfree.com
· Padmini Rangamani, UCSD, adn01w@r.postjobfree.com, 858-***-****
· Tammie R Nelson, Los Alamos National Labs, adn01w@r.postjobfree.com
· Sally C Pias, New Mexico Institute of Mining and Technology, adn01w@r.postjobfree.com, 575-***-****
· Arcadius V Krivoshein, University of Houston Clear lake, adn01w@r.postjobfree.com, 281-***-****
· Tatiana Timofeeva, New Mexico Highlands University, adn01w@r.postjobfree.com, 505-***-****

Additional information and skills
Nuclear Magnetic Resonance (NMR), GCMS, IR, UV-Vis, MALDI-TOF, Pipetting, dilution.
HPLC; compound and protein lyophilization.
C#, Unity

"
,
Data Scientist Machine Learning engineer,"
Aqueel Premjee
Houston, TX ***** 832-***-**** adn0gs@r.postjobfree.com
Summary
Data Scientist with 20+ years of experience focused on machine learning, supervised, unsupervised, reinforced, deep learning, performance metrics, parameter fine tuning, descriptive and inferential statistics. Experienced in data mining, including web-crawling, and machine learning models to uncover insights and drove $3M+ in business growth. Hands-on experience with Python, R, and SQL. Seeking a challenging role to leverage my strong analytical, data-oriented skills in data mining/data wrangling and use Machine Learning to generate models that predict the outcome of unseen data. Areas of Expertise
● Python, R, SQL
● Machine Learning
● Deep Learning
● Predictive modeling
● Quantitative analysis
● Statistical Analysis
● Python, Pandas, Keras, Pytorch, NumPy, Scikit-learn, matplotlib
● Database design multipurpose
● Data wrangling
● Physics - Mechanics - Electromagnetism -Waves
Skills
● Strong communication skills
● Avid and swift learner
● Robust math skills
● Extensive knowledge of industrial control systems and technology
● Creative problem solving
● Teamwork
Experience
ALDINE ISD, HOUSTON TEXAS 2015 – present
● Developing, customizing the Learning Management System (Schoology) to create synchronous/asynchronous Massive Open Online Course (MOOC) content in Physics Page 1 of 3
Aqueel Premjee adn0gs@r.postjobfree.com
TEAM LEAD, DATA SCIENTIST CORAXIS CORPORATION (COX AUTOMOTIVE) 2011 - 2015
● Team Lead for data mining project to predict recommendation in the retail domain
● Created bot detection program in Python using decision tree analysis
● Created machine learning models with R-Caret/Python-Scikit-learn to make testable predictions
● Developed multivariate Gaussian anomaly detection algorithm in R/Python to identify suspicious patterns in online traffic
● Real Estate price extrapolations using sklearn and Pytorch
● Conducted cluster analysis to generate segmented profiles of users using KNN
● Optimized online user data model
DATA SCIENTIST HAYDARI STEEL INDUSTRIES 2006 – 2011
● Implemented regression algorithms used in the Cold rolling process using sklearn
● Improved quality through prediction of strip quality using machine learning models for processing parameters in mill, annealing and pickling
● Classification with Support Vector Machines for the Hot rolling process
● Minimized downtimes and production loss through fault prediction using cobble analysis with digital twins/assistants and vision technology for tracking
● Gradient descent process for optimizing Stockyard levels
● Track quality and optimize storage time through real-time tracking of quality and tonnage in storage and stockpiles to reduce overloading, lost stockpiles
DATABASE ADMINISTRATOR LOTUS TEXTILE INDUSTRY AND ACADEMY 1995 - 2006
● Designed entire data model for clients, employees, production items and production machinery
● Configured all servers and databases
● Set up systems/automated feeds from sensor outputs
● Responsible for data extraction, backups/restores
● Responsible for user provisioning
● Responsible for data authentication and data integrity.
● Performance monitoring for hardware and software INSTRUCTOR 1989 - 1995
● Roanoke Valley Governor's School Mathematics, Computer Science 1993 - 1995
● Virginia College Electronics 1992 – 1993
● ITT Technical Institute Physics and Electronics 1989 – 1992 Education
● 22 graduate units 1988 San Diego State University
● B.S. Physics 1986 Loyola Marymount University 2
Aqueel Premjee adn0gs@r.postjobfree.com
● Associate in Business Administration 1983 The American College in London Certificates
● Relational database and SQL and SQL server – 3-part course - Coursera
● Data Science with Python – University of Michigan
● Machine Learning – Alberta Machine Learning Institute
● Machine Learning – Stanford University
● Applied Machine Learning in Python - University of Michigan
● Data Analysis using Python – University of Pennsylvania
● Statistical Inference – John Hopkins University
● Python Data Representations – Rice University
● SQL for Data Science – UC Davis
● Probability and Data with R – Duke University
3

"
,
Data Science,"
Summary: * Years in Data Science/ML, ** Years in Information Technology

Creative Data Scientist and Software Engineer focused on machine learning. Extensive background in project management, leadership, and financial reporting. Well versed in various machine learning techniques, such as Linear and Logistic Regression, Decision Trees, and Neural Network Architectures. Comfortable with deployment and integration on cloud technologies such as AWS and Azure. Mathematics and logical thinking to tackle the everyday obstacles. Thrives under pressure, quick learner, and a self-starter always in the mood for a new challenge.

Professional Profile
oExperience applying Naïve Bayes, Regression, and Classification techniques as well as Neural Networks, Deep Neural Networks, Decision Trees, and Random Forest.
oFamiliarity using statistical models on large data sets using cloud computing services such as AWS, Azure, and GCP.
oApplying statistical and predictive modeling methods to build and design reliable systems for real-time analysis and decision-making.
oExpertise in developing creative solutions to business use cases through data analysis, statistical modeling, and innovative thinking.
oPerforming EDA to find patterns in business data and communicate findings to the business using visualization tool such as Matplotlib, and Seaborn, and Plotly.
oLeading teams to productionize statistical or machine learning models and create APIs or data pipelines for the benefit of business leaders and product managers.
oExperience using supervised and unsupervised techniques.
oImplementation of predictive analytics for sales to provide forecasting and improve decision-making using techniques such as ARIMA, ETS, and Prophet.
oExcellent communication and presentation skills with experience in explaining complex model and ideas to team members and non-technical stakeholders.
oLeading teams to prepare clean data pipelines and design, build, validate, and refresh machine learning models.
oApplying statistical analysis and machine learning techniques to live data streams from big data sources using PySpark and batch processing techniques.
Professional Skills
Analytic Development
Python, R, Spark, SQL
Python Packages
Numpy, Pandas, Scikit-learn, TensorFlow, Keras, PyTorch, Fastai, SciPy, Matplotlib, Seaborn, Numba
Programming Tools
Jupyter, RStudio, Github, Git
Cloud Computing
Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP)
Machine Learning
Natural Language Processing & Understanding, Machine Intelligence, Machine Learning algorithms
Analysis Methods
Forecasting, Predictive, Statistical, Sentiment, Exploratory and Bayesian Analysis. Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Clustering
Applied Data Science
Natural Language Processing, Machine Learning, Social Analytics, Predictive Maintenance, Chatbots, Interactive Dashboards.

Artificial Intelligence
Classification and Regression Trees (CART), Support Vector Machine, Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, Regression, Naïve Bayes
Natural Language Processing
Text analysis, classification, chatbots.
Deep Learning
Machine Perception, Data Mining, Machine Learning, Neural Networks, TensorFlow, Keras, PyTorch, Transfer Learning
Data Modeling
Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series analysis
Soft Skills
Excellent communication and presentation skills. Ability to work well with stakeholders to discern needs. Leadership, mentoring
Other Programming Languages & Skills
APIs, C++, Eclipse, Java, Linux, C#, Docker, Node.js, React.js, Spring, XML, Bootstrap, Django, Flask, CSS, Express.js, Front-End, HTML, Kubernetes, Back-End, Databases, Finance, GitHub

Professional Experience
NLP Engineer, Data Scientist
Buoy Health May 2019 – Present
Boston Massachusetts

Chat bot Sentiment Analysis with BERT- – Designed a customer service oriented chatbot to identify potential symptoms and to direct traffic to appropriate departments. Buoy Health required symptom checker chatbot which leverages AI to deliver personalized and more accurate diagnoses. The company’s algorithm was trained on clinical data from 18,000 medical papers to mirror the literature referenced by physicians. Beginning with the symptoms provided by the user via natural language processing, the chatbot matches the symptoms to all possible conditions and then ask clarifying questions to narrow them down to the best selection. Symptom checker chatbots are not clinical decision support (CDS) tools and do not claim to assist with medical decision making (MDM). They are also exempt from FDA regulation. Implemented BERT embeddings and Neural Net Classification and similarity algorithms. Initial perplexity measurements show a Computer Understanding of over 90% with a solution matching accuracy of over 85%.

•Worked in an environment using Python, NoSQL, Docker, AWS, and Kubernetes.
•Worked with the Python packages NumPy, Pandas, SciPy, Matplotlib, Plotly, and FeatureTools for data analytics, cleaning, and feature engineering.
•Used NLTK and Gensim for NLP processes such as Tokenization and for creating custom Word Embeddings.
•Imported from Python’s Tensorflow package for building Neural Network models.
•Implemented ELMO, Doc2Vec and BERT embeddings.
•Employed numerous different models, including Convolutional and Recurrent Neural Networks, LSTM, and Transformers which all utilized Word Embeddings.
•Models which were operationalized were deployed to a RESTful API using the Python Flask package and Docker containers.
•Used Agile approaches, including Extreme Programming, Test-Driven Development, and Agile Scrum.

Machine Learning Engineer/Computer Vision Specialist
Aware Sep 2017 – April 2019
Bedford MA

Age, Emotion, Gender, Face Recognition with CNNs – Develop a machine vision facility for the improvement of closed-circuit camera networks. The project consisted of a network path prediction component using linear programing and forecasting techniques as well as a computer vision system using vectorized features. The system utilized a facial embedding algorithm that compared favorably with Convolutional Neural Networks at the time but had the advantage of not requiring retraining to identify new users. Using the data set from Data Vision containing 14,000 images.

Designed Vectorizing function to embed facial features.
Created specialized algorithm to store and compare vectorized features and verifications.
Implemented Convolutional Neural Networks using PyTorch and Python.
Performed data cleaning on images and tabular data.
Performed image augmentation techniques to introduce rotational, motion and scale invariance.
Designed Statistical evaluation techniques to test the model performance.
Deployed using Flask and pickle.
Developed, evaluated, and trained a custom convolutional neural network (CNN) using frameworks such as Tensorflow and Keras in Python.
Leveraging of model checkpoints and early stopping as well as optimizers such as Adam to expedite the model training process.
Image resizing and interpolation into a standard size as well as generation of rotational and other invariances using the Skimage library.
Utilization of the CV2 library in order to read and render videos from historic and live data.


Data Scientist
Compass March 2015 – July 2017
Boston, MA

Forecasting and Analytics – Main project targeting the price prediction of houses in greater Boston area. Following a correlation analysis, few attributes in the dataset seemed to correlate with the price attribute. In order to train the model with more comprehensive data, the Boston Police reports were added as attributes. First, the all the incidents per zip code were counted, since in the original dataset there was the zip code attribute. Then a function to loop through the original data was created to append a column that contained the number of crimes reported on the zip code of that row. A negative correlation between house price and areas with high crime rates was shown. The model was then trained using the attributes, number of bathrooms, number of bedrooms, number of crimes and square feet. To test the model, the same data was extracted from a website used to buy/sell property called Zillow. The prediction of the model was 85% accurate in comparison with the price listed on the website. Additional forecasting for sales and overall trends was done in parallel with this work.

Built and integrated logistic and linear regression models, balancing various internal requirements of covariance and variable criteria.
Discovered patterns in data using algorithms and use experimental and iterative approach to validate findings.
Creative thinking/strong ability to devise and propose innovative ways to look at problems by using business acumen, mathematical theories, data models, and statistical analysis.
Advanced statistical and predictive modeling techniques to build, maintain, and improve on real-time decision systems using ARIMA, ETS, and Prophet.
Used decision trees and random forests to grade the validity of the variables used in the regression models. Implemented additional tools such as bagging and boosting (AdaBoost, XGBoost) in order to strengthen these models.
Communicated results and recommendations to business stakeholders on a weekly basis. Implemented feedback and features based on the evolving needs of the business in a rapidly changing social landscape.
Model evolution performed between competing groups, where the best model was selected for further refinement.
Final model deployed in a Flask app on AWS to be called via a REST API.

Data Scientist
Americold Jan 2013– Feb 2015
Atlanta, GA (remote)

Failure Prediction Model – AmeriCold Logistics LLC is a major temperature controlled warehousing and transportation company based in Atlanta, Georgia, United States. It is in the business of modern commercialized temperature-controlled warehousing for the storage of perishable goods, one of the forms of Food Preservation. As pressurized gas and air system are fundamental to the continued success of the enterprise, a machine learning-based solution was designed and implemented to predict failures and program preventative maintenance. An internal dataset of component failures in various air pressure system(APS) activated or powered equipment was compiled in order to identify potential failure and reduce the cost of maintenance. The base line of this project was that when predicting if a component needs maintenance or not, a false positive only costs the company $10, in contrast of a false negative that costs $500. Several Classification algorithms were tried, from Naïve Bayes to Logistic Regression. In the end we implemented a cox proportional hazards model to predict the expected failure time of seven critical systems. Savings to the enterprise were estimated to be upwards of $400,000 and incalculable earnings in good will and customer cargo integrity and good will.

•Performed Data Preprocessing on Censor Generated and IoT Data.
•Pre-processed data using PCA and feature elimination while still maintaining a classification accuracy of more than 99% by the trained models.
•Implemented SVM, for faster training, less resource-intensive reasons.
•Implemented various neural networks such as convolutional and recurrent for large number of features.
•Developed K-means and Density-Bases Spatial Clustering of Application with Noise and mixture methods such as multi-variate Gaussian mixture model for this process.
•Project led to the increased of performance, accuracy, precision, and recall rate.

Courses & Certificates
Face Recognition Web APP with Machine Learning in Flask

Free.AI

Machine Learning, Data Science and Deep Learning with Python

Sundog Education

Machine Learning Practical Workout

Super Data Science

Python 3 Programming Specialization

University of Michigan

Google IT Automation with Python

Google

Homeland Security and Cybersecurity

University of Colorado

Big Data Modeling and Management Systems

University of California San Diego

Machine Learning for All

University of London

Server-side Development with NodeJS, Express and MongoDB

The Hong Kong University of Science and Technology

Programming Foundations with JavaScript, HTML and CSS

Duke University

AWS Fundamentals: Going Cloud-Native

Amazon Web Services

Java Programming: Solving Problems with Software

Duke University

Java Programming: Arrays, Lists, and Structured Data

Duke University

Getting Started with Google Kubernetes Engine

Google Cloud

Palo Alto Networks Academy Cybersecurity Foundation

Palo Alto Networks Academy

Front-End Web UI Frameworks and Tools: Bootstrap 4

The Hong Kong University of Science and Technology

Multiplatform Mobile App Development with React Native

The Hong Kong University of Science and Technology

Full-Stack Web-Development with React

The Hong Kong University of Science and Technology

Continuous Delivery & DevOps

University of Virginia

Digital Electronics & Logic Design

10X Training Technologies

Complete React Developer in 2020 with Redux, Hooks, GraphQL

Zero to Mastery


Education

Master’s Degree
Information Technology
Endicott College GPA 3.96 - Beverly, Massachusetts
Specialized in Machine Learning, Deep Learning, Data Science & Analytics, Software Engineering and Cloud Computing

Bachelor’s Degree
Accounting
Unifacex, GPA 3.5

Languages

English: Fluent
Portuguese: Fluent
Spanish: Fluent

Relevant Experience

Projects can be found on GitHub Repository – github.com/jonathanOak

"
,
Data Scientist Cobol,"
Shannon Reuben Serrao
Blacksburg, VA 540-***-**** adntap@r.postjobfree.com github.com/shannonserrao F1-OPT Technical Skills
DATA SCIENCE: Pandas NumPy Scikit-Learn Signal Processing Image Processing SciPy PROGRAMMING LANGUAGES: Python SQL C++/C MATLAB Fortran Bash Mathematica COBOL MACHINE LEARNING: Time-Series modelling(ARIMA) Regression NLP (GloVe, FastText) Classification Clustering Deep Learning (Pytorch) Random Forest K-Means TensorFlow, Keras Distributed Computing Apache Spark SVM AWS Gradient Descent Naive Bayes Neural Networks (CNN, RNN, LSTM) Price prediction Big Data DATA VISUALIZATION WEB: Matplotlib Bokeh Altair Seaborn Flask HTML MATHEMATICS AND STATISITICS: Hypothesis Testing Statistical Modeling Monte Carlo Methods Linear Algebra DOMAIN KNOWLEDGE: Finance Epidemiology Churn Analysis Projects and Achievements
LEASETOME WEBAPP RECOMMENDER SYSTEM github.com/shannonserrao/L2M_Capstone
• Data Engineering using Graph QL to pull housing data from third party APIs; Built a recommender/visualization webapp targeted at commercial retail space customers for LeaseToMe. TEXT BASED VISUAL, QUESTION ANSWERING IN IMAGES github.com/shannonserrao/textVQA_challenge
• Developed a vanilla VQA model with a multidisciplinary team, improving accuracy by 38%; maximized file storage by performing data cleaning while engineering and refining OCR characters in image and conversion to fastText and GloVe embedding vectors; used CNN(Pytorch) to implement OCR attention to answer image-based questions AN ANALYSIS OF STACK OVERFLOW SURVEY github.com/shannonserrao/Stack_Exchange
• Improved Stack Overflow customer satisfaction with multi-class classification, regression and clustering based analysis COVID NETWORK MODELING github.com/shannonserrao/COVID_SEIR
• Created a small-world lattice model to simulate COVID outbreak in a community as part of the pandemic prevention program at Virginia Tech; predicted spread by restructuring real-world data and recommended preventive measures; re- structured real-world graph data to predict/test preventative measures IMAGE ANALYSIS OF ECOLOGICAL MODELS github.com/shannonserrao/May-Leonard_lattice_Simulation
• Built a Monte-Carlo model to analyze May-Leonard ecology; utilized spiral image data to determine spiral stabilization criterion; detected temporal and spatial correlation signal and pattern recognition of spiral structures in image data using Pandas and SciPy; mentored three undergraduate students for three years AWARDS: WAN-ZIA SCHOLARSHIP FOR EXCELLENCE IN SCHOLARSHIP AND ACTIVE ENGAGEMENT OF RESEARCH Work Experience
HSBC-GLT PUNE June 2008 – August 2009
Software Engineer
• Processed vast amounts of financial ledger information to write COBOL codes for Financial Straight Through Processing on mainframe computers
Education
Ph.D in Physics Virginia PolyTech Institute & State University December 2020 MS in Physics Indian Institute of Technology Roorkee June 2013 BE in Electronics and Telecommunication Mumbai University (Don Bosco Institute of Technology) June 2008

"
,
Data Analyst Scientist,"
HIMANSHU JAT
**** ********* ***, ***-****, Philadelphia PA 19104 • 267-***-**** • adnpsl@r.postjobfree.com • www.himanshujat.com PROFESSIONAL SUMMARY
• 3+ years of hands-on experience with Python, R, SQL, Tableau, Big Data, AWS and Data Science
• Analyzed data using Python, R, SQL by querying structured and unstructured databases
• Proficient in designing machine learning models with Python programming language
• Extensive experience of working and developing in UNIX environment such as Ubuntu
• Theoretical and practical understanding of Linear Algebra, Statistics, Neural Networks and Algorithm Development
• Working experience with Machine Learning, Deep Learning and Artificial Intelligence SKILLS
Programming Languages: Python, R, DOMO
Databases: SQL, MongoDB, Microsoft SQL server, MySQL, PostgreSQL, SQlite3 Libraries: Pandas, Numpy, Scipy, Scikit-learn, Flask, Django, Tensorflow, Keras, PySpark, OpenCV, Weka Data Science: Feature Engineering, Dimensionality Reduction, Outlier Detection, Hyperparameter Tuning Machine Learning: Supervised and Unsupervised ML, Regression, Classification, Clustering, CNN, RNN, Matlab Statistical Techniques: PCA, Hypothesis Testing, Time Series Analysis, F-test, ANOVA, A/B testing, MS Excel Data Visualization: Tableau, Power BI, Matplotlib, Seaborn, Plotly, Google Analytics Cloud Platforms: Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure Soft Skills: Excellent Communication, Problem Solving, Research, High Quality, Integrity Skills, Organization, Critical Thinking, Passion, Collaborate, Presentation Skills PROFESSIONAL EXPERIENCE
Verif-y Inc, Philadelphia, PA June 2020 – December 2020 Data Analyst Intern
• Developed a chatbot and integrated this to the company’s website, which enabled the company to handle several customer inquiries in a short period of time. This resulted in maintaining high quality customer service while generating approx. ~$50K per annum through staff reduction.
• Enabled the company to gather relevant business insights through chatbots, which improved their data analysis.
• Created a credit card scanning with 95% accuracy for Western Union for better fraud detection and security.
• Performed an anomaly detection using credentials ranking to identify important data points and critical incidents essential in developing new process improvements that drive customer success.
• Designed and implemented OCR with 99% of accuracy for automatic document parsing, which streamlined internal processes. Enabled the company to cut costs by 20% for document examinations. Upwork Inc, Santa Clara, CA January 2018 – September 2019 Data Scientist Freelancer
• Developed a data science pipeline for a client to improve their service quality impacting customer retention, which generated a total of $250K.
• Established an image clustering portal for photographers to consolidate images effectively and efficiently. Enabled photographers to save ~20 hours of work per shoot, which is equivalent to $35K savings per annum.
• Utilized in-depth machine learning expertise to accurately identify the potential causes of employee attrition.
• Created 300+ video tutorials and a data science book that spread awareness on multidisciplinary technical learnings. EDUCATION
MS in Data Science, Drexel University, Philadelphia, PA Graduated in July 2021 GPA 3.9 BE in Computer Science, Acropolis Institute of Technology and Research, India Graduated in June 2019 GPA 3.7 Machine Learning, Stanford University (Coursera) Certified in February 2018 100% Data Science Certification, TopGrads Certified in June 2021 100% ACADEMIC PROJECTS
Amazon Go – Customer enters a store, shops and just GO! (2021) Successfully replicated the Amazon Go with over 90% accuracy using a single camera on each shelf, which will save $40K per state for reducing the number of cameras. Created charts and graphs using Tableau for visualizing the results. Stock Price Prediction – Predicting Stock Prices using Sentiment Analysis of Tweets and News Articles (2020) Developed a system with 70% accuracy that analyzes the sentiment of news articles and Twitter tweets to predict whether the stock prices of Fortune 500 companies will go up or down. Used PowerBI to plot word clouds of frequently used words in the text. FasTest – Disease Prediction Using Retinal Scans (2019) Predict the risk of cardiovascular disease by retinal scans using a mobile phone camera and an external lens. FasTest was the top 100 ideas of India in the TCS EngiNX competition.

"
,
Illinois University Software Developer,"
Jordan Park
Computer Scientist
Well-grounded and dedicated software developer with a strong desire
for high achievement. Passionate in creating quality software and ﬁnds the trials along the way to be an exciting learning experience. Former expertise in music orients his ﬁnesse in both creativity and team-work. Seeking growth opportunities to expand technical skills and to try new technologies.
adno11@r.postjobfree.com
630-***-****
Winﬁeld, IL 60190
linkedin.com/in/JordanKPark
github.com/JordanKPark
EXPERIENCE/PROJECTS
Sum Sudoku (C#) (04/2019 – 06/2019)
Based on the classic game of Sudoku, this C# Windows Form Application employs the use of OOP principles such as classes and objects. Relies on time, mouse, and keyboard event handling. Playing ﬁeld created using dynamically allocated GUI components. Frequent Flyer Redeemer (Java) (09/2018 – 10/2018) Collaborated with a group of 3 to build a Java application that a travelling agent could present to a client who has accumulated frequent ﬂyer miles. Employs GUI elements like combo-boxes, buttons, and panels and uses event handlers to receive user input. Physical Therapist Clinical Website (HTML, PHP, MySQL) (01/2018 – 05/2018) Led a team of four to create a user-friendly designed website for a database course using HTML, PHP, and MySQL that allowed the user to login and create an appointment that delivered detailed routines based on the user input. Managed the creation and maintenance of the database that encompassed the patient, therapist, routine, and appointment information and the generation of future data.
ASCII Slot Machine (C++) (11/2016 – 01/2017)
A timeless gambling where a user may wager their e- money, this C++ Windows Console Application utilizes data structures such as lists and vectors. Automatic Text-Based Baseball (C++) (03/2016 – 05/2016) An automatically simulated baseball game that applies .csv Excel input ﬁles and struct/classes to build teams and players with stats. Applies sound, time, and ASCII graphics, and hidden cheat in code to bring one of my ﬁrst apps developed in C++ to life. EDUCATION
08/2017 – 12/2019
Bachelor's Degree in Computer Science: Software Development Northern Illinois University
Dekalb, IL
Object-Oriented Programming Data Structure Algorithm Analysis Principles of Operating Systems Database Management Unix and network programming Intro to Software Engineering Programming in C#/.NET Android Mobile Device Programming Computer Architecture and Systems Organization Programming in IBM Assembler 08/2015 – 05/2017
Associate Degree in Liberal Arts
College of DuPage
Glen Ellyn, IL
Phi Theta Kappa: High Honor's Society
TECHNICAL SKILLS
Languages C/C++, C# and .NET, PHP, HTML,
Python, CSS, IBM Assembler, Java
Database
Management
MySQL, MS Access
Operating
Systems
Windows, Linux, UNIX, Android Tools Vista TN3720 IBM, VIM, Notepad++, VS Code, Oracle VM, PuTTY, GitHub, Git,
Bitbucket,
IDE's MS Visual Studio, Eclipse, Dr. Java,
PyCharm, Android Studio
Other Unity, Unreal Engine 4, GarageBand, Pro
Tools, Excel
Related Courses:
High Honors:

"
,
Associate Director/Principal Investigator,"
Hien-Anh NGUYEN, PhD
adnoe3@r.postjobfree.com
linkedin.com/in/nguyenhienanh
Associate Director: cell-based assays and high-throughput small molecule screening Principal Scientist: 7 years of experience in pre-clinical biologic development process
• Associate Director at SAMDI Tech Inc., USA 2020-2021 My mission consists of building a brand new cell culture facility from scratch that offers both i) conventional cell-based assays and ii) unprecedented mass spectrometry-amenable cell-based assays for a high-throughput label-free screening of millions of small molecules in a complex biological context as cultured cells within several weeks. Achievements: A fully functional new CRO offering >20 cell-based assay services; initial high-throughput screening of 100,000 compounds
• Principal Investigator/ Chief Scientific Officer at Enzyme by Design Inc., USA 2017-2020 During 3.5 years at EbD, I successfully competed and was awarded with three Phase I and one Phase II SBIR/STTR small business grants. I led the in-house EbD's protein engineering and discovery programs as well as the sourcing of multiple CROs/CDMO for drug manufacturing and pre-clinical drug evaluation programs for IND enabling studies. Beyond the technical leading role of a CSO, I led the management team to complete interviews with KOLs, physicians, nurses and hospital staffs in leading cancer centers throughout the US to confirm the clinical & commercial needs for developing our biologics drug candidates. Achievements: $3.5M in funding; >130 customer discovery interviews; a throughout 2-year plan for drug CMC and IND enabling studies
• Scientific consultant at Tango Biosciences LLC., USA 2018-2019 My job consists of setting up a brand new CHO culture facility and providing scientific consulting services on mammalian protein production of large-scale quantity and of high purity levels for Tango Biosciences to feed in its phase-display affinity selection downstream.
Achievements: A fully functional new CHO facility capable of transiently expressing multiple difficult protein targets
• Senior Scientist at University of Illinois at Chicago, USA 2013-2020 I initiated and led multiple fundamental and translational projects aimed at gaining an increase in understanding of the relationship between protein structure and function towards oncology bio-better drugs. I spent 3 years to master crystallography skills from gene/protein engineering to crystallization and structure refinement, identifying 2 lead drug candidates. I then spent the next 4 years to characterize these drugs in vitro test tubes and in culture, together with in vivo PD/PK/ADME evaluation. Overall, along with a solid and board technical skillset, I naturally refined my project management, grant writing and supervision skills. I enriched my experience in critically evaluating the scientific literature and adopted novel technologies which facilitate drug discovery. Achievements: 1 issued international patent and 1 pending PCT patent applications, 7 peer-reviewed publications.
• Bio-analyst and Pathway constructor at Institute Curie, Paris, FRANCE 2012-2013 Project: “From Atlas of Cancer Signaling Networks to druggable target design” https://acsn.curie.fr I was in charge of creating an open access & comprehensive signaling map of the Epithelial-to-Mesenchymal Transition process. I was able to deliver the map after 10 months (a process normally requires three to four years of full time work). The map of more than 5,000 genes and proteins with Google’s navigation principle is online at https://acsn.curie.fr/navicell/maps/emtcellmotility/master/index.html Overall, I developed Computational pathway analysis and Bioinformatics skills, along with competences in quickly evaluating and summarizing big data, in making conclusions that clarify next steps towards achieving project goals. I also experienced working in close collaboration with physicians from the Rene Huguenin hospital. Achievements: 1 peer-reviewed publication recommended by F1000Prime http://f1000.com/prime/725660252; 1 open-access database.
• Research Associate: PhD fellow at CEA – Institute of Structural Biology, Grenoble, FRANCE 2008-2012 Project: “Discovery of a new bacterial protein kinase family: functioning mechanism & cellular roles” I worked with a new enzyme, target for antibiotic development, but of unknown function. My mission was to characterize this enzyme biochemically and to find its real cellular functions in vivo. Overall, I obtained experience working in a European research project of multiple partners. I developed sough molecular biology and biochemistry skills as well as deep knowledge in bacterial protein kinases including 3.5-year experience in radioactivity kinase assay. Achievements: 1 peer-reviewed article; 1 invited talk and multiple poster presentations
• Research Assistant: Master 2 at AgroSup Dijon – Bourgogne University, FRANCE 2007-2008 Project: “Influence of culture and chilling kinetics on the stability of lyophilized sourdough starters” I optimized the lyophilization process for an industrial bacterium responsible for malolactic fermentation in wine-making. Overall, I obtained experience working in food industry along with skills in Microbiology and cell culture. 2008-2012 PhD in Biochemistry at Grenoble University, France Mention: summa cum laude (with distinction)
2007-2008 Master in Biotechnology – Food Science, Sensory and Consumer Research at Bourgogne University, France Mention: magna cum laude (3rd ranking in class of 30 students) 2002-2007 Engineer in Biotechnology – Food Technology at Hanoi Polytechnic University, Vietnam Mention: summa cum laude (1st ranking in promotion of >4800 students), Vietnamese grade 9.0/10 ~ U.S.A GPA 4.0 2002 Bachelor of Science in Chemistry, Excellence high school attached to Natural Science University, Vietnam Mention: magna cum laude – great honor
1/2
5)
THE 5)# LOGO AND
LOGO SYSTEM
!PPROV
,OGO W
EDUCATION
Languages Vietnamese (mother tongue), French (bilingual – acquired citizenship), English (fluent) Music Pianist at Hanoi Conservatory of music for 11 years (1991-2002) Technique High throughput small molecule drug screening: from compound management and automated liquid handling to compound screening campaigns using SAMDI mass spectrometry or fluorescent readouts. Matrix PlateMate, TTP Labtech Mosquito, Multidrop, Nanodrop, MALDI Ab Sciex Tof/Tof, PHERAStar FS and FSX Biochemistry: from protein engineering and large-scale production (both bacterial and mammalian (ExpiCHO) expression systems) to physical and enzymatic characterization. TFF, HPLC, FPLC (AKTA), SEC, DLS, Biacore/SPR, CD, TSA, SDS-PAGE, Western Blot, ELISA, multiple in-vitro enzymatic assays Crystallography: from protein crystallization to X-ray data processing and structure determination. Home-source X-ray; APS and ESRF synchrotrons, XDS and HKL3000 packages; Pymol, Chimera Bioinformatics: from molecular signaling map building (>5,000 proteins and genes) to pathway analysis. Cytoscape 3.0, CellDesigner 4.0, BiNome 3.0 softwares; Python, R languages (not developer but advanced user) Molecular biology: from inactivation of chromosomal gene to cloning and gene engineering. PCR, gene deletion/insertion/point mutations, chromosomal gene knock-out, CRISPR/Cas9 genome editing Microbiology: bioprocess using bacterial or yeast fermentation. E. coli (different strains), Bacillus subtilis, Oenococcus oenie, Lactobacillus acidophilus, Saccharomyces cerevisiae Cellular biology: setting up mammalian tissue culture facilities for 2 startups, master >20 cell-based assays Cultured >20 cancer cell lines, advanced user of BioTek and BMG microplate readers, CellCyte label-free imaging Animal: working with mice, rats, dogs for PK/PD and preclinical toxicological assessment towards IND applications i.p and i.v injection, cardiac puncture, tissue dissection, IVIS-live imaging, pharmacokinetic and pharmacodynamics Supervision Principal Investigator in 4 NCI SBIR grants (Phase I + Phase II), leading a team with 1 senior Postdoc and 4 technicians Supervised 1 MD/PhD student, 2 PhD students, 1 senior + 7 junior technicians and 3 undergraduate students Trained multiple teams on IVIS-live imaging and other techniques in working with rodents. Management Project management: Maintain 2 main jobs (45% and 55% FTE) and volunteering work between 2017-2020 CRO management: Conceive SOWs, manage Champions Oncology, Jackson Lab, Charles River Lab on multiple projects Business management: Co-founder; and Business developer for three different startups; Quote proposals for clients First author in 5 peer-reviewed articles
1. 2018, Cancer Res, 78, 1549-60, «A Novel L-Asparaginase with low L-Glutaminase coactivity Is highly efficacious against both T- and B-cell Acute Lymphoblastic Leukemias In Vivo»
2. 2017, J Mol Biol, 429, 3056-74, «Expanding the kinome world: a new family of protein kinase widely conserved in bacteria» 3. 2017, Sci Reports, 7:41643, «The differential ability of asparagine and glutamine in promoting the closed/active enzyme conformation rationalizes the Wolinella succinogenes L-asparaginase substrate specificity» 4. 2016, J Biol Chem, 291, 17664-76, «Design and characterization of Erwinia Chrysanthemi L-Asparaginase variants with diminished L-Glutaminase activity» 5. 2016, Biochemistry, 55, 1246-53, «Structural insight into substrate selectivity of Erwinia chrysanthemi L-asparaginase» Co-author in 6 peer-reviewed articles, 2 more en route in 2021 1. 2021, Blood Adv, 5, 1963-76 «The spleen as a sanctuary site for residual leukemic cells following ABT-199 monotherapy in ETP-ALL» 2. 2020, Curr Microbiol, 77, 4063-71, «UbK is Involved in the resistance of Bacillus subtilis to oxidative stress» 3. 2017, Sci Reports, 7:10224, «Discovery of human-like L-asparaginases with potential clinical use by directed evolution» 4. 2015, Oncogenesis 4, e160, «Atlas of Cancer Signaling Network: navigating cancer biology with Google Maps» 5. 2014, J Biol Chem, 289, 33175-86, «Identification and structural analysis of an L-asparaginase enzyme from guinea pig with putative tumor cell-killing properties»
6. 2014, J Med Chem, 57, 9480-94, «Structure-guided development of deoxycytidine kinase inhibitors with nanomolar affinity and improved metabolic stability»
1. Int. Patent. Appl. PCT/US2017/020090 “L-Asparaginase Variants and Fusion Proteins with Reduced L-Glutaminase Activity and Enhanced Stability”. https://patents.google.com/patent/WO2017151707A1/en 2. Int. Patent. Appl. PCT/US2018/046196 “Truncated Guinea Pig L-Asparaginase Variants and Methods of Use”. https://patents.google.com/patent/WO2019032952A1/en 2019 Among 40 HALO scientist awardees under the age of 40 Among 25 AnitaB PitCHER semi-finalists
2018 ASPIRE award nominee, “Smart Technologies for Healthy Societies” theme, for APEC scientists under the age of 40 AACR Cancer Research Early Career Award nominee
2017 Among 7 Finalists in UIC Startup Challenge (Presenter, Apr 10th) 2nd place in Fast Pitch Fan Favorite at IBIO-Index (Presenter, Apr 21st) 2016-2017 Among 5 Professional Entrepreneurship Awardees funded by Women In Bio organization 2008-2012 Among 20 Irtelis Awardees for PhD Research, fully funded fellowship by the Life Sciences Program of CEA (Commissariat à l'Energie Atomique et aux Energies Alternatives)
2007-2008 Among 25 Evarist Galois Awardees for Master Research, fully funded fellowship by the Embassy of France in Vietnam 2007 2nd prize in “National Students & Research Competition” of the Vietnam Ministry of Science and Technology 1st prize in “Students & Sciences” of Hanoi Polytechnic University 1st laureate of the 47th Engineering promotion of Hanoi Polytechnic University (Valedictorian speech) 2006 Honors Fellowship from Odon Valley, Rencontre du Vietnam Organization for outstanding scholar activity Honors Fellowship from Former Polytechnic Students Organization for top 10 engineers (attributed every 5 years) 2004 Agence Universitaire Francophonie Fellowship for French culture and languistics 2002-2007 5-year scholarship for top 5 engineer candidates of Hanoi Polytechnic University 199*-****-**-year scholarship for top 10 pianist students of Hanoi Conservatory of music Hien-Anh NGUYEN
+1-309-***-**-**
linkedin.com/in/nguyenhienanh
SKILLS
PUBLICATIONS
REWARDS and DISTINCTION
PATENTS
2/2

"
,
Network Support,"
CURRICULUM VITAE

CHARLES F. AUSTIN
** ********* ***** ******, ** 27501 - 209-***-**** - adnn4r@r.postjobfree.com

EDUCATION

University of Colorado, College of Letters, Arts & Sciences Department of Computer Science
Doctor of Computer Science, D.Sc. (Concentration in Information Security)

School of Ministry, Calvary Chapel Modesto, Modesto, CA
Diploma in Higher Education Leadership

University of Missouri, School of Computing and Engineering, Kansas City, MO
Master of Computer Science, MCS Summa cum Laude

University of Missouri, Henry W. Bloch School of Management, Kansas City, MO
Bachelor of Business Management, BBA Magna cum Laude

Electronic Computer Programming Institute (ECPI), Kansas City, MO
Diploma (Programming) Dean’s List

EMPLOYMENT HISTORY

My Computer Career, Holly Springs, NC 3/2018 - Present
Senior Network Engineer/IT Manager
Assist in providing computer/network support relating to software and hardware problems reported by users.
Maintain an adequate level of knowledge of operating system and application software being used to provide high levels of support to users.
Maintaining, repairing, and upgrading networks and computer systems. Diagnosing and fixing problems, avoiding potential issues with networks and infrastructure.
Monitor and evaluate efficacy of software/hardware usage, providing items to be covered in training of users, assisting them in productivity efficiencies.
Install new software applications or hardware on the LAN, coordinating assistance from third parties when necessary.
Add and maintain users on the network; assigning application access, ensuring security, and maintaining their configurations are within standards.
Assist in installation of workstations and printers on the LAN.
Participate in Team meetings, providing input and suggestions, and prepare minutes of discussion items.
Assist in gathering bid prices on equipment and supplies as needed.
Provide documentation as needed which defines upcoming needs of the network which would require the purchase of additional hardware and/or software.
Monitor and report licenses on applications to ensure compliance as needed.
Support, monitor, address, and make a recommendation with regard to internet traffic latency and bandwidth, in addition, using network performance metrics to identify issues in all cities.
Utilize technology to provide staff with a fast, accurate and secure method of gaining access to information so they can service the member in the fastest and most efficient manner possible.
Ensure that all necessary patches are applied to all servers.
Provide a secure environment for systems, servers, and overall technology.
Monitor load balance on servers and make recommendations accordingly.
Assist webmaster as needed.
Safeguard and keep completely private, except where explicitly stated, all company intellectual property, including but not limited to passwords, email, documents, hardware, software, infrastructure, etc.
Design a cyber range in a virtual environment to add to curriculum simulating Blue-Red teams

TVU Networks, Raleigh, NC, 2/2017 – 3/2018
Senior Technical Writer / Senior Network Engineer

• Setup and maintain database system to catalog and distribute user manuals and training material
• Authored repair, user and maintenance manuals, white papers, videos and web content
• Setup and administering LAMP/Moodle server hosting TVU’s on-line LMS
• Project management including network forensic analysis, forensic tools, penetration testing, methodologies and disaster recovery.

Turlock Christian Schools, Turlock, CA 4/2016 - 2/2017
Director of Technology/Sr. Network Engineer

• Oversaw all technology operations for the school incorporating new technology programs
• Devised and established IT policies and systems to support the STEM initiatives
• Analyzed the requirements of all educational departments to determine their technology needs
• Purchased efficient and most effective technological equipment and software to support STEM programs
• Managed Google Amin, Ren web, Sage, Quick books, Follett, Final Site, Single Wire, Informacast, Apex Learning and others
• Identified the need for upgrades, configurations and new systems and managed deployment
• Controlled budget exceeding 1.8 million dollars and report on expenditure to the school board
• Assisted in building relationships with vendors and creating cost-efficient contracts reducing overall budget by 22.8%
• Managed the deployment of over 1200 mobile devices
• Coordinated the efforts for a new campus facility including infrastructure, network, IP devices, Learning Centers and mobile devices
• Instituted new information security policies concerning chrome book and student internet activities
• Managed Active Directory and all server operation including VSphere and Hyper-V environments

Institute of Technology, Modesto, CA 11/2011 - 4/2016
Technical Division Director/MSSA Program Director/Technology Instructor

• Initiated the institution’s first accredited Microsoft Systems and Security Analyst (MSSA) program
• Designed curriculum and piloted to other campuses
• Responsible for programmatic accreditation to ACCSC
• Responsible for vouchers and certification including MTA, MOS and CompTIA
• Author of text book titled “Cloud Computing” used in curriculum
• Author of text book titled “Mobile Technology” used in curriculum
• Courses instructed: End to End Technology, Cloud Computing, Mobile Operating Systems, Networking

Concepts, Linux Servers, Microsoft Servers and Network Security
• Supervisory role overseeing regulatory affairs, curriculum, and faculty for the Technical Division
• Responsible for student retention and attribution and maintained 100%
• Lead Technical Division growth equaling 195% for all programs

Austin IT Solutions INC. 6/2001 to Present
CEO/System Engineer/Network Engineer

• Setup and managed a network support company including sales, deployment, service, accounting, procurement, shipping and receiving
• Established IT department supporting 200+ clients including all hardware platforms, printers, servers, bridges, routers, switches, T1, T3, modems, firewalls, cabling, Internet, as well as the overall portfolio of software applications.
• Project management on a wide array of IT projects for corporations, municipalities, lawyers and small businesses including computer forensic analysis, forensic tools, and penetration testing and disaster recovery.

DOX Electronics INC., Rochester, NY 4/1995 to 6/2001
System Engineer/Network Installer

• Installed and maintained Ethernet and token ring networks and physical disaster recovery venues.

COMMUNITY AND VOLUNTEER INVOLVMENT

KEQP, Modesto, CA 2006 - 2012
Volunteer Radio Station Programmer

• Programmed radio broadcast of shows for a digital low power radio station.
• Responsible for downloading files from vendor website and FTP sites converting files and loading files into appropriate time slots

SCW of Modesto, Modesto, CA 2001 - 2015
Executive Director/ Executive Board Member

• Serve as Head of Staff, providing direction and supervision to both program and support staff
• Implement and direct fund and resource development, including identification of funding prospects, grant shaping, and collaboration with foundations and judicatories
• Guide SCW in responding to emerging issues in California and those in need
• Collaborate and lead in effecting positive communication within and among all SCW structures, programs, partners, and with member non denominations
• Develop collegian, fiscal, and partnership relationships with non-denominational leadership.
• Serving those in our community who need help with food, clothing, shelter, employment and encouragement

City of Modesto, Modesto, CA 2016 to 2017
Advisory Board Member – Housing Rehabilitation Loan Committee (HRLC)

Calvary Chapel Manteca, Manteca, CA 2012 - 2017
Department Head/Board Member
Leadership role overseeing IT, media production and Missions Ministry

RESEARCH AND DEVELOPMENT ACTIVITY

NASA Ames Research (Nebula), http://nebula.nasa.gov/services/
Contributing Scientist
Researched the feasibility of Nebula to facilitate the sharing of large complex data sets. We designed it to be an agency-wide program and one of three flagship initiatives highlighted in NASA's Open Government Plan for Health Care.

NASA Ames Research (QuAIL)
Contributing Scientist
Researched and contributed development of the Quantum Artificial Intelligence Laboratory (QuAIL) located at the NASA Advanced Supercomputing (NAS) facility at NASA’s Ames Research Center in Moffett Field, California.

The Internet Society (ISOC)
Council member
Active council contribution on important Internet policies and voting rights in all chapters of ISOC including New York, Chicago, San Francisco, Denver, Montreal, Winnipeg, Stuttgart and Munich for over twenty years.

National Institute of Standards and Technology (NIST)
Contributing Scientist
Researched in collaboration with the Computer Security Resource Center developing the Lightweight Cryptography Project because the majority of current cryptographic algorithms are designed for desktop/server environments many of these algorithms do not fit into the constrained resources.

Ubuntu Phone project, http://www.ubuntu.com/phone/features
Scopes developer
Developed Ubuntu’s scopes that allows user to have multiple home screens with different kinds of content without having to go through individual apps.

Ubuntu Studio 17.10 Artful Aardvark project, https://ubuntustudio.org/tag/artful/
Contributing Scientist
Developer and Beta testing (yam) Ubuntu Studio, the official flavor of Ubuntu that is the most widely used multimedia orientated GNU/Linux distribution in the world.

SCIENTIFIC AND PROFESSIONAL PRESENTATIONS

SANS Institute, https://www.sans.org/
Keynote Speaker
Fellow Dissertations at multiple venues including SANS Institute Security Conferences in California, Colorado, Washington, New York and New Jersey

The Internet Society (ISOC)
Keynote Speaker
Presented at the Computer History Museum in Mountain House “The Internet of Things (IOT)”

The Greater Washington DC Chapter of the Internet Society (ISOC),
Elected Board Chair

Qualys Continuous Security, https://www.qualys.com/
Fellows Speaker, The ABCs of The Pragmatic CISO
• Assets – discover, assess, categorize, and secure assets across clouds and networks
• Borderless-ness – applies controls that remain effective across borders.
• Compliance – achieves continuous and demonstrable compliance with PCI, HIPAA and NIST.
NASA Ames Research Center
Contributing Scientist
• Center of Excellence Information Technology Strategic Plan Workshop

Penguin Books
Authored several published books as well as curriculum
• Cloud Computing – text book
• Mobile Operating Systems – text book
• Security for Internet Users – EBook
• Curriculum for the Microsoft Systems and Security Analyst program

Currently working on new project “Information Security 101” publication in 2021

SANS Institute, https://www.sans.org/
• Authored a published book directed to IT professionals, CEO and CIO titled “Cloud Computing Security Risks”

CURRENT CERTIFICATION

Aruba – Aruba Certified Mobility Associate
AWS – AWS Technical Professional Accreditation, AWS Certified Solutions Architect
CISSP– Certified Information System Security Professional
CISA – Certified Information System Auditor
CISM – Certified Information Security Manager
CCDH – Certified Developer for Apache Hadoop – cloudera
CNE – Certified Network Engineer
CompTIA: CTT+ Classroom Trainer, Linux+, Net+, Security+,
Cisco: CCNA Security – Cisco Certified Network Associate Security, CCNA – Cisco Certified Network Associate
EnCase – Certified Examiner
Google – Apps Certified Development Specialist
IBM – Certified Solution Advisor – Cloud Computing Architecture V1
ITIL – ITIL Expert – IT Service Management
Linux Professional Institute – LPIC-1 Certification
Microsoft: MCSE Business Intelligence, MCSE Private Cloud, MCSE Desktop Infrastructure, MCSE Server Infrastructure, Certified Master 2008r2 Active Directory,
Novell – Linux Administrator – Suse Linux Enterprise 11, Certified Web Designer
ORACLE – Oracle Database 11g Administrator Certified Professional
OSCP – Offensive Security Certified Professional
RHCSS – Red Hat Certified Security Specialist
Salesforce – Certified Software Developer
SUN – Certified Network Administrator Solaris 8
Ubuntu – Certified Professional
VMware – Certified Professional 5 Data Center Virtualization, vSphere 5

PROFESSIONAL RECOMMENDATIONS

Wayne Turnbow
Consultant, Software Assessment and Selection Team, AAC Utility Partners
I have been impressed by Charles’ enthusiasm and drive to have his students do their very best. He provides opportunities to show his students what the real world expects and will demand of his students. He strives to show that they can be better and to strive to be their best.

Steve Carter
Author / Writing at Cengage Learning
Charles and I worked on several projects while at the Institute of Technology. He was a valuable asset in setting up a new technical program for IOT that was eventually adopted by the corporation for implementation. Charles is a gifted teacher and very detailed administrator. He is open to new ideas, and brings new ideas forward with thoughtful and insightful suggestions. He was a pleasure to work with and would highly recommend him.

Dave Abbey
Media/Tech Director
Charles was always reliable and very competent, and an absolute pleasure to have on board!

Ken Michael
VP Dox Electronics Inc.
I’ve have known and worked with Charles for over 10 years and have found him to be an honest hard working Network Engineer with great knowledge and practical experience.

CONTINUING EDUCATION

Microsoft Academy – continuing education currently focused on HyperV and Azure

CBT Nuggets - including Power Shell 3, Ruby on Rails, Azure, Open Stack, VMware, Moodle and Articulate

Khan Academy – HyperV, Python and SQL

LYNDA.com - continuing education currently studying vSphere 6, Virtualization, vMotion and other VMware products with focus type 1 hypervisor tools

PROFESSIONAL GROWTH AND SERVICE

City of Modesto, California 2016, 2017
Housing Rehabilitation Loan Committee (HRLC)

Institute of Technology 2011-2015
League Soft Ball Coach

Internet Society ISOC
Chapter Memberships

IEEE Society
Contributing Member

HONORS AND AWARDS

100% Retention, Institute of Technology, Modesto, CA

Graduated Magna cum Laude, University of Missouri, Kansas City, MO

NASA Certificate of Appreciation - Center of Excellence Information Technology Strategic Plan Workshop

Dean’s Honor List, ECPI, Kansas City, MO

Letter of Accomplishment, ECPI, Kansas City, MO

Graduated Summa cum Laude, University of Missouri, Kansas City, MO

Academic Scholarship, University of Missouri, Kansas City, MO

IEEE - MEDAL OF HONOR, for an exceptional contribution or an extraordinary career in IEEE fields of interest, sponsored by the IEEE Foundation, to CHARLES AUSTIN(LFIEEE)—Institute Professor, Institute of Technology, Modesto, California, USA “For leadership and contributions across many fields of science and engineering.”

"
,
"Data scientist, Machine learning engineer, Applied Mathematics","
LOHIER K. JEAN
Machine Learning Student
React / React native Software Developer
**** ** ****, **** ********** FL, United States
adnnme@r.postjobfree.com
+1 (754) 779 – 0655
PROFESSIONAL PROFILE
Full stack software developer with strong abilities and experience in using Python to implement machine learning solutions. Skilled in using visualization frameworks such as Matplotlib, Seaborn and GGplot. Capable of using libraries such as Pandas, NumPy, and sciPy for data processing and to carry out statistical analysis. Capable of working with relational and non-relational databases to organize data for modelling purposes. Willing to learn new database and machine learning frameworks in order to help deliver scalable and powerful products. Knowledgeable of classical machine learning algorithms and deep learning concepts ; Passionate about Statistics and advanced Mathematics. CORE COMPETENCIES
Python • Weka • Java • JavaScript • Git • GitHub • NodeJS • Firebase • MongoDB • Jupyter Notebook • MYSQL • AWS • JIRA • Bit Bucket • Complex Problem Solving • Attention to Detail • Communication & Collaboration • Problem solving • Technical Writing • Public Speaking • Attention to Detail EDUCATION
MASTER OF SCIENCE IN COMPUTER SCIENCE FLORIDA ATLANTIC UNIVERSITY FLORIDA EXPECTED SUMMER 2020 HONORS AND AWARDS: Pathway’s scholarship (Summer 2020) BACHELOR OF SCIENCE IN MATHEMATICS FLORIDA ATLANTIC UNIVERSITY FLORIDA 2018 – 2020 HONORS AND AWARDS: President’s List (Spring/Fall 2019) • Dean’s List (Spring/Fall 2019) ASSOCIATE’S DEGREE IN MATHEMATICS BROWARD COLLEGE FLORIDA 2016 – 2018 HONORS AND AWARDS: President’s List (2018) • Highest Honors CAREER SUMMARY
MACHINE LEARNING INTERN E SOURCE COLORADO 2020 – TO DATE KEY ACHIEVEMENTS
• Used python and pandas to create an efficient auto-ML workflow.
• Used Amazon S3 to implement a backend server allowing users to store, compare and retrieve xgboost models.
• Generated extensive documentation and testing to improve reproducibility of the data pipelines.
• Created a scalable and cost-efficient framework to synchronize survey analysis with other internal data processes. TUTOR STEM CENTER AT BROWARD COLLEGE FLORIDA 2018 – 2021 KEY ACHIEVEMENTS
• Taught students a variety of data science topics inclusive of applied statistics and data modelling.
• Trained students to apply machine learning concepts to solve programming challenges in Python and R RESEARCH INTERN TELEMETRIC MEDICAL APPLICATIONS FLORIDA 2019 – 2020 KEY ACHIEVEMENTS
• Collaborated with Dr. Marc Hill to implement the RSA and Diffie - Hellman encryption protocols using C++.
• Succeeded in implementing time-efficient encryption algorithms by using memoization, recursion and number theoretical properties of Elliptic curves.

"
,
Data Science,"
adlzlw@r.postjobfree.com
732-***-****


TECHNICAL SKILLS

COMMUNICATION SKILLS verbal, written, presentations

LEADERSHIP supports project goals, business use case and mentors team

QUALITY continuous improvement in project processes, workflows, automation and ongoing learning and achievement

CLOUD Analytics in cloud-based platforms (AWS, MS Azure, Google Cloud)

ANALYTICS Data Analysis, Data Mining, Statistical Analysis, Multivariate Analysis, Stochastic Optimization, Linear Regression, ANOVA, Hypothesis Testing, Forecasting, ARIMA, Sentiment Analysis, Predictive Analysis, Pattern Recognition, Classification, Behavioral Modeling

PROGRAMMING LANGUAGES Python, R, SQL, Scala, Java, MATLAB, C, SAS, F#

LIBRARIES NumPy, SciPy, Pandas, Theano, Caffe, SciKit-learn Matplotlib, Seaborn, TensorFlow, Keras, NLTK, PyTorch, Gensim, Urllib, BeautifulSoup4, MxNet, Deeplearning4j, EJML, dplyr, ggplot2, reshape2, tidyr, purrr, readr

DEVELOPMENT Git, GitHub,Bitbucket, SVN, Mercurial, PyCharm, Sublime, JIRA, TFS, Trello, Linux, Unix

DATA EXTRATION AND MANIPULATION: Hadoop HDFS, Hortonworks Hadoop, MapR, Cloudera Hadoop, Cloudera Impala, Google Cloud Platform, MS Azure Cloud, both SQL and noSQL, Data Warehouse, Data Lake, SWL, HiveQL, AWS (RedShift, Kinesis, EMR, EC2)

MACHINE LEARNING Supervised Machine Learning Algorithms (Linear Regression, Logistic Regression, Support Vector Machines, Decision Trees and Random Forests, Naïve Bayes Classifiers, K Nearest Neighbors), Unsupervised Machine Learning Algorithms (K Means Clustering, Gaussian Mixtures, Hidden Markov Models, D), Imbalanced Learning (SMOTE, AdaSyn, NearMiss), Deep Learning Artificial Neural Networks, Machine Perception

APPLICATIONS: Recommender Systems, Predictive Maintenance, Forecasting, Fraud Prevention and Detection, Targeting Systems, Ranking Systems, Deep Learning, Strategic Planning, Digital Intelligence,


WORK EXPERIENCE

BANK OF NY MELLON November 2019 - Present
Data Scientist Florham Park, NJ
Worked with the Identity Management Team within the Information Security Division to develop self-service tools for internal employees. Worked to establish cloud controls for identity governance and assess risks associated with cloud service providers.

Programmed solutions using Python libraries such as numpy and pandas.
Use machine learning and statistical modeling techniques to develop and evaluate algorithms to improve performance, quality, data management and accuracy
Managed version control set up for the phantom platform using Git.
Setted up a playbook for events and classification containers.
Developed an app called risk hub and moved it to production deployment using Django.
Contributed to design and prototyping of medium to high complexity machine learning systems
Work with product managers to formulate the data analytics problem
In charge of cleaning and debugging datasets and the codebase before applications reach QA.
Used RASA to build a POC web app to show chat bot usage
Provided recommendations for controls for implementation of IAM on the cloud.
Perform analysis of user profiles and current application entitlements based on user profiles, organization, departments and groups.
Built a recommending system to auto-provisioning applications and platform access to new employees/contractors so they are productive as soon as they onboarded
Entitlement Analytics
Data Analysis and Reporting
Developed a Recommendation Engine of Entitlements and Applications
Performed Evaluation of on-prem and cloud controls.
Benchmarked on-prem identity management system vs cloud identity management systems
Responsible of presenting findings to stakeholders.
Selected and built dashboards to for internal usage
Familiar with Machine Learning modeling using python and frameworks like Tensorflow
Implemented and cleans datasets for network accesses based on user profiles


Randalls Food and Stores May 2018-November 2019
Data Scientist Houston, TX

Worked on a sales forecasting project for a using an artificial neural network developed in PyTorch along with Facebook’s Prophet model. I performed data cleaning in Python on a large dataset including several years’ worth of data across different departments in dozens of stores and produced highly accurate forecasts for each store and department.
Created a model using Facebook Prophet to produce highly accurate predictions of a weekly sales
Evaluated model performance on large dataset (multiple years of daily data for dozens of departments per store and dozens of stores)
Deployed model created highly accurate 6-month forecasts up to 6 months in advance for every store and department.
Worked in a Cloudera Hadoop environment using Python, SQL, and Tableau
HDFS (Cloudera): Pulled data from Hadoop cluster.
Worked within the Enterprise Applications team as a Data Scientist.
Used Python, Pandas, NumPy, and SciPy for exploratory data analysis, data wrangling and, feature engineering.
Used Tableau and TabPy for visualization of analyses.
Worked along with Business Analyst, Data Analyst, and Data Engineers.
Consulted with various departments within the company including, SIU and Safety.
Managed and matched claim numbers into fraud cases.
Cleaned fraud data to be joined with the claims data (~73k observations)
Research and Assess the Fraud Predictive Analytics scenario in terms of predicting final outcomes for new claims
Create a Tableau Dashboard that will help SIU in present their Annual Report
Tried kernel density estimation in lower dimensional space as a feature to predict fraud.
Testing Anomaly Detection Models such as Expectation Maximization, Elliptical Envelope, and Isolation Forest.
Multivariate analysis of safety programs from the last 10 years.
Used regression to determine the correlation of participation in the safety program with outcome of claims.
Hypothesis testing and statistical analysis was done to determine statistically significant changes in claims after participating in the safety program.
Presented findings of impact testing.
Workers Compensation fraud detection
Prepared data for exploratory analysis
Engineering actuarial formulas
Collaborated with other Data Scientist with use cases that included workplace accident prediction and sentiment analysis.
Technologies: Cloudera Hadoop, Python, SQL, and Tableau, Hadoop HDFS, Pandas, NumPy, and SciPy, TabPy, Data Modeling, Multivariate analysis, Regression Analysis, Hypothesis Testing, Exploratory Analysis, Sentiment Analysis, Predictive Analytics.


Omnicare Inc May 2017-April 2018
Data Scientist/NLP Engineer Stafford, TX

Worked with NLP to classify text with data draw from a big data system. The text categorization involved labeling natural language texts with relevant categories from a predefined set. One goal was to target users by automated classification. In this way we could create cohorts to improve marketing. The NLP text analysis monitored, tracked and classified user discussion about product and/or service in online discussion. The machine learning classifier was trained to identify whether a cohort was a promoter or a detractor. Overall the project improved marketing ROI and customer satisfaction.
Oversaw the entire production cycle to extract and display metadata from various assets developing a report display that is easy to grasp and gain insights.
Performs NLP preprocessing in Python using libraries such as NLTK.
Collaborated with both the Research and Engineering teams to productionize the application.
Assisted various teams in bringing prototyped assets into production.
Expertise in applying data mining techniques and optimization techniques in B2B and B2C industries and proficient in Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.
Utilized MapReduce/PySpark Python modules for machine learning & predictive analytics on AWS.
Implemented assets and scripts for various projects using R,Java and Python
Built sustainable rapport with senior leaders.
Developing and maintaining Data Dictionary to create metadata reports for technical and business purposes.
Build and maintain dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.
Keeping up to date with latest NLP methodologies by reading 10 to 15 articles and whitepapers per week.
Extracting the source data from Oracle tables, MS SQL Server, sequential files and Excel sheets.
Parse and manipulate raw, complex data streams to prepare for loading into an analytical tool.
Involved in defining the source to target data mappings, business rules, and data definitions.
Project environment was AWS and Linux.
Technologies Used: Python, R, Java, Kubernetes, Docker, ELK Stack (ElasticSearch, Logstash, Kibana), AWS Comprehend

TGS-Nopec February 2014-April 2017
Data Scientist Houston, TX
TGS-Nopec is a publicly traded company listed in the Norwegian Stock Exchange with global headquarters in Houston. The primary occupation of the company is to perform exploration studies for the oil and gas industry. Their principal products are data and insights for the oil and energy industries that include: multi-client geophysical data, multi-client geological data, imaging services and reservoir solutions, data & analytics, machine-learning solutions, well performance insights, etc. At TGS I performed a number of statistical studies including well performance and drilling optimization using deep neural nets.
Application of data mining techniques and optimization techniques in B2B and B2C industries and Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.
Utilized PySpark Python modules for machine learning & predictive analytics in Hadoop on AWS.
Predictive modeling using state-of-the-art methods.
Implemented advanced machine learning algorithms utilizing caffe, TensorFlow, Scala, Spark, MLLib, R and other tools and languages needed.
Programming, and scripting in R, Java and Python.
Developed Data Dictionary to create metadata reports for tec hnical and business purpose.
Built reporting dashboard on the statistical models to identify and track key metrics and risk indicators.
Performed Boosting method on predicted model for the improve efficiency of the model.
Extracted source data from Amazon Redshift on AWS cloud platform.
Parsed and manipulated raw, complex data streams to prepare for loading into an analytical tool.
Explored different regression and ensemble models in machine learning to perform forecasting
Developed new financial models and forecasts.
Improved efficiency and accuracy by evaluating models in R.
Involved in defining the source to target data mappings, business rules, and data definitions.
Performing an end to end Informatica ETL Testing for these custom tables by writing complex SQL Queries on the source database and comparing the results against the target database.

TMD Staffing June 2012-January 2014
Data Analyst Katie, TX

Applied Machine Learning, Data/Text Mining, Statistical Analysis and Predictive Modeling.
Implemented Event Task for executing an application automatically.
Involved in defining the source to target data mappings, business rules, and data definitions.
Assist in continual monitoring, analysis and improvement of AWS Hadoop Data Lake environment
Built and maintained dashboard and reporting based on the statistical models to identify and track key metrics and performance indicators.
Involved in fixing bugs and minor enhancements for the front-end modules.
Performed data mining and developed statistical models using Python to provide tactical recommendations to the business executives.
Integrated R into micro-strategy to expose metrics determined by more sophisticated and detailed models than natively available in the tool.
Participated in feature engineering such as feature intersection generating, feature normalize and label encoding with SciKit-Learn preprocessing.
Worked on outlier identification with Gaussian Mixture Models using Pandas, NumPy and matplotlib.
Adopted feature engineering techniques with 200+ predictors in order to find the most important features for the models. Tested the models with classification methods, such as Random Forest, Logistics Regression and Gradient Boosting Machine, and performed hyper-parameter tuning to optimize the models.

University of Port Harcourt Sept 2009-January 2012
IT Associate Rivers State, Nigeria
.
Assisted users in initiating services.
Experience with Microsoft Exchange Migration
Open Directory and Dot Net framework.
Implemented Event Task for executing an application automatically.
Involved in defining the source to target data mappings, business rules, and data definitions.
Assisted in basic computer repair/reformat.

EDUCATION

Higher National Diploma (Bachelor Equivalent): Petroleum Engineering Technology
University of Port Hartcourt, Rivers State, Nigeria

Master of Science: Business Analytics
Merrimack College, North Andover, Massachusetts

Certificate, Petroleum Data Technology
Lone Star College, Cypress, TX

Chidi O.
DATA SCIENTIST
Phone: (000-***-**** Email: adlzlw@r.postjobfree.com
SUMMARY
ABOUT ME
9 Years in Data Science
12 years in Information Technology
Expertise in Machine Learning, Deep Learning, Convoluted Neural nets
Projects involving NLP, NLU, Text Mining, Predictive Analytics, Artificial Intelligence
Techniques big data structure and unstructured
Extensive exposure on analytics project life cycle CRISP-DM (Cross Industry Standard Process for Data Mining) and web applications using SCRUM methodologies.
Use machine learning to advance systems such as product recommendations, search ranking and relevance, image attribution, demand routing, fit recommendations, inventory forecasting, threat modeling, etc.
Business understanding, Data understanding, Data preparation, Modeling, Evaluation and Deployment.
Experienced in practical application of data science to business problems to produce actionable results.
Experience in Natural Language Processing (NLP), Machine Learning & Artificial Intelligence.
Experience with AWS cloud computing, Spark (especially AWS EMR), Kibana, Node.js, Tableau, Looker.
Able to incorporate visual analytics dashboards.
Experience with a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction
Knowledge on Apache Spark and developing data processing and analysis algorithms using Python.
Programming in Java, Python and SQL queries.
Use of libraries and fraemworks in Machine Learning such as NumPy, SciPy, Pandas, Theano, Caffe, SciKit-learn Matplotlib, Seaborn, Theano, TensorFlow, Keras, NLTK, PyTorch Gensim, Urllib, Beautiful Soup).
Experience working in industrial or manufacturing environments around Operations Analytics, Supply Chain Analytics, and Pricing Analytics.
Ability with algorithms, data query and process automation.
Evaluation of datasets and complex data modelling.
.

"
,
Data Analyst Scientist,"
Summary

Knowledgeable and result-oriented professional with 6+ years of experience in data analytics and data science.
Good experience in Software Development Life Cycle (Analysis, Design, Development, Testing, Deployment and Support) in Agile and Waterfall methodologies.
Experience in using various packages in Python like Pandas, NumPy, SciPy, Matplotlib, and Scikit-Learn.
Experience in Text Analytics, generating data visualizations, manipulate data for data loads, extracts, statistical analysis, modeling, and data munging using Python and proficient R user with knowledge of statistical programming languages SAS.
Experience in Implementing procedures for extracting Excel sheet data into the mainframe environment by connecting to the database using SQL.
Good experience and knowledge of Amazon Web Services (AWS): EC2, S3 and IAM.
Good working Experience in various data analysis platforms including Jupyter Notebook and Spider.
Good knowledge of implementing Decision Trees, Linear and Logistic Regression, supervised and Unsupervised Learning principal Component Analysis.
Experience in building, publishing customized interactive reports and dashboards with customized parameters and user-filters using Tableau, Power BI.
Experience in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using SSIS.
Experience in creating and documenting Metadata for OLTP and OLAP when designing a system.
Good knowledge of transforming business resources and requirements into manageable data formats and analytical models, designing algorithms, building models, and reporting solutions that scale across a massive volume of structured and unstructured data.
Experience in Normalization and De-Normalization techniques for optimum performance in relational and dimensional database environments.
Working knowledge of relational and non-relational databases such as MySQL, SQL Server, Teradata, and MS Access.
Good knowledge of Data Warehousing principles Fact Tables, Dimensional Tables, Dimensional Data Modeling - Star Schema and Snowflake Schema.
Experience in project management tools like Jira and Maintain version control of code using tool such as GIT.
Good working with Operating Systems like Linux and Windows environments.
Effective team player with strong communication and interpersonal skills, possess a strong ability to adapt and learn new technologies and new business lines rapidly.

Skills

Methodologies:
SDLC, Agile, Waterfall
Programming Language:
Python, R, SQL
IDEs:
Jupyter Notebook, Spyder
Machine Learning:
Linear regression, logistic regression, decision trees, supervised Learning,
Unsupervised Learning
Cloud:
AWS
Python Packages:
NumPy, Pandas, Matplotlib, SciPy, Scikit-Learn
Visualization Tools:
Tableau, Power BI, SAS
ETL Tools:
SSIS, SSRS
Database:
SQL Server, MySQL, Teradata
Other Tools:
JIRA, Git, OLAP, KPI, MS Office
Operating System:
Windows, Linux
Education

Master’s In Computer Science
California State University, Fullerton, CA

Experience

Goldman Sachs Group, MA Mar 2020 - Current
Roles: Data Scientist
Responsibilities:

Followed Agile Methodology for application Implementation and Testing.
Worked on R and Python for programming for improvement of model. Upgraded the entire models for improvement of the product.
Working with open-source tools Jupyter Notebook and Spyder for statistical analysis and building the machine learning.
Responsible for Built classification models include Linear regression, Logistic Regression, and Decision Tree.
Involved in writing T-SQL, working on SSIS, SSRS, Data Cleansing, Data Scrubbing and Data Migration.
Responsible for Improved sales and logistic data quality by data cleaning using NumPy, SciPy, Pandas, and Scikit-Learn in Python.
Created SQL reports, data extraction and data loading scripts for different databases and schemas.
Developed Tableau data visualization using Scatter Plots, Geographic Map, Pie Charts and Bar Charts, and Density Chart.
Involved in performing data conversions from flat files into a normalized database structure using SSIS and other tools.
Generated data extracts in Tableau by connecting to the view using Tableau MySQL connector.
Create new EC2 instance in AWS, allocate volumes and giving Provisionals using IAM.
Responsible for writing complex SQL queries for extracting large volume data.
Created Tableau Server landing page to give user quick access to their most recently accessed reports.
Created Multiset, temporary, derived, and volatile tables in Teradata database.
Prepare detailed statistical reports in SQL, SSRS, R & SPSS to track progress of weekly, monthly and quarterly Students Trends and Enrollment.
Perform statistical analysis of data using SAS procedures and R packages (such as mixed models, linear, logistic, and nonlinear regression, multivariate regression, simulation modeling, best model selection, and survival data analysis)
Used JIRA tool and other internal issue trackers for the Project development.
Gained expertise on high performance data integration solutions - Microsoft SQL Server Integration Service (SSIS).
Worked on version control system tools like Git.
Environment: SDLC, Agile, R, Python, Jupyter Notebook, Spyder, AWS, EC2, IAM, NumPy, SciPy, Pandas, Scikit-Learn, Teradata, SAS, SQL Server, Tableau, OLAP, OLTP, JIRA, GIT.

Deloitte, India Jan 2017 – Jan 2019
Roles: Business Data Analyst
Responsibilities:

Involved in requirements gathering, Analysis, Design, Development, testing production of an application using the Agile model.
Used pandas, NumPy, SciPy, matplotlib, Scikit-Learn in Python for developing various machine learning algorithms.
Responsible for extracted patterns in the structured and unstructured data set and displayed them with interactive charts using R and Python.
Worked on data analysis with various analytic tools, such as Jupyter Notebook and Spyder.
Worked on customer segmentation using an Unsupervised learning technique-clustering and supervised, regression techniques to create a building model.
Analyzed old information architectures and contributed to the design and development of the new ones.
Created SQL scripts for testing and validating data on various reports, dashboards, and scorecards and handled performance issues effectively in Tableau.
Designed various analytical reports and dashboards on Sales performance, campaign response data in Tableau.
Used Amazon IAM to grant fine-grained access to AWS resources to users.
Built reports against Single Table, Multiple Tables and built formulas in Tableau for various business calculations.
Worked on to create ETL packages to Validate, Extract, Transform and Load data into Data Warehouse Using SSIS.
Designing and developing data ingestion, aggregation, and advanced analytics from MySQL.
Developed SAS coding and table templates for preparing, processing, and analyzing clinical data.
Responsible for Applied analysis methods such as Hypotheses testing and Analysis of variance (ANOVA) for validating the existing models on the observed data.
Confident communicator to ensure clients’ needs are always met in a manner that is consistent with their specifications and requirements.
Extracted data from Access to MS Excel and Analyzed database for dup/missing data while maintaining data integrity.
Involved in OLAP processing for changing and maintaining the Warehousing Optimizing Dimensions, Hierarchies and adding the Aggregations to the Cube.
Developed the necessary Stored Procedures and created Complex Views using Joins for robust and fast retrieval of data in SQL Server.
Used JIRA for defect tracking and project management and developed the project in Linux environment. Environments: Agile, R, Python, Jupyter Notebook, Spyder, Pandas, AWS, IAM, NumPy, SciPy, Matplotlib, Scikit-Learn, SSIS, SQL Server, MS Excel, MySQL, SQL Server, SAS, JIRA, Git.

IBM, India Apr 2013 – Dec 2016
Roles: Consultant
Responsibilities:

Involved in requirements gathering, Analysis, Design, Development, testing production of an application using the Waterfall model.
Implemented Data Exploration to analyze patterns and to select features using Python.
Embedded Power BI reports to internal Portal to manage access of reports and data for individual user using based on roles.
Developed SQL scripts using OLAP functions like rank Over to improve the query performance while pulling the data from large tables.
Utilizing Power BI to create various analytical dashboard that depicts critical KPIs.
Import data from multiple data sources and create automated reports in Power BI after cleaning and preprocessing the data.
Highly experienced in SAP HANA administration and Solution Manager configurations.
Worked collaboratively in a team environment.
Utilized strong interpersonal and communication skills to serve customers.
Assigned as the point of contact for the new Graduate hires, conducted weekly technical KT sessions including project-
specific tool’s access.
Oracle administrations using BR tools, Tablespace extensions, Table reorganization.
Deep understanding of business processes, and have experience working with people on different backgrounds, priorities, and responsibilities.
Created an Analysis tool using Excel Power Pivot loading millions of rows of data to Excel from SQL.
Developed normalized Logical and Physical database models for designing an OLTP application.
Used pandas, NumPy, SciPy and Matplotlib in Python for developing testing and data analysis.
Developed CSV files and reported offshore progress to management with the use of Excel Templates, Excel macros, Pivot tables and functions.
Used Git for version control with Data Engineer team and Data Scientists colleagues.
Review and analyzed user Stories in Jira to provide Level of effort for testing.
Environments: Waterfall, Python, Power BI, OLAP, KPIs, SQL, MySQL, OLTP, pandas, NumPy, SciPy, Matplotlib, Excel, Git, Jira.

Phone: 401-***-**** Email: adnlgp@r.postjobfree.com Location: MA
Prathyusha Rao
Data Scientist

"
,
Senior Data Scientist,"
Technical Skills:

Analytic Development
Python, R, IDL, SAS, SQL, MatLab

Packages and Visualizations
Numpy, Pandas, SciPy, TensorFlow, PyTorch, Keras, Theano, Caffe, Matplotlib, Seaborn, Ggplot, Tableau, and Plotly.

Machine Learning
Supervised and unsupervised algorithms, Natural Language Processing, Image Recognition and Detection, Forecasting.

Linear Regression, Lasso and Ridge, Logistic Regression, Ensemble classifiers (Bagging, Boosting, and voting), KNN, Naïve Bayes Classifier, Clustering (k-means, DBSCAN), PCA, SVD, ARIMA, Decision Trees.

Artificial Intelligence (AI)
Natural Language Processing (NLP), Text understanding, classification, Pattern Recognition, computer vision, embeddings (BERT, ELMO, Skip-gram),
Encoder-Decoder, Sentiment analysis, and Latent Discriminant Analysis (LDA).

Deep Learning
Data Mining, Machine Learning Algorithms, Convolution Neural Networks (CNN), Recurrent Neural Network (RNN), Multilayer Perceptional Neural Network (MLPNN), Long Short-Term Memory (LSTM), Support Vector Machines (SVM), and Random Forest (RF).

Analysis Methods
Advanced Data Modelling, Forecasting, Statistical, Sentiment, Stochastic,

Bayesian analysis, Regression analysis, Linear models, Multivariate analysis, Sentiment analysis, Big data, and clustering.

Analysis Techniques
Classification and Regression Trees (CART), Gradient Boosting Machine (GBM), TensorFlow, PCA, Regression, Naïve Bayes.

Data Modeling
Bayesian Analysis, Statistical Inference, Predictive Modelling, Linear Modelling, Probabilistic Modelling, Time-Series Analysis.

IDE
Jupyter Notebook, Spyder, RStudio, Google Colab

Version Control
GitHub, Git

Soft Skills
Excellent communication and presentation skills; ability to work wee with stakeholders to discern needs accurately; leadership, mentoring, and coaching

Deployment
Azure, AWS, Docker, Kubernetes, and Jenkins

Deployment
NLTK, Spacy, Genism, BERT, Elmo

Professional Work Experience

Senior Data Scientist Jun 2020 - Present
FORM.COM
Los Angeles, California (Remote)

Headed the Data Science team at Form.Com. The team was in charge of developing a computer vision and evaluation application to identify products being sold in individual stores. I implemented a Convolutional Embedding system to identify brand name products and movements in the cooler after successive image captures.
●Develop a custom dataset for fine-tuning a deep neural network.
●Fine-tune a variety of image models with object-detection heads.
●Use both Single Shot Detection (SSD) and You Only Look Once (YOLO) object detection models.
●Deploy finished model on edge devices using Tensorflow-Lite.
●Build various statistical models Statistical algorithms involving Time Series analysis, Survival Analysis, Multivariate Regression, Linear Regression, Logistic Regression, and PCA in financial projection
●Lead the development of the expected profit projection engine by applying machine learning with financial engineering and actuarial science.
●Build various statistical models: Statistical algorithms involving Time Series analysis, Survival Analysis, Multivariate Regression, Linear Regression, Logistic Regression, and PCA in financial projection.
●Perform in-force management, including survival analysis, churn/retention analysis, and risk identification.
●Use pre-trained models to visualize feature maps in the intermediate layers and perform transfer learning.
●Use pre-trained models (VGG16, ResNets, Inceptions, DenseNet, U-Net, etc.) for transfer learning on small datasets.
●Design and implement the enterprise Financial Value-at-Risk model.
●Lead various cross-department projects and work closely with internal stakeholders such as business teams, product managers, and engineering teams.
●Work on customer segmentation using an unsupervised learning technique clustering.

WATSON NLP Engineer Aug 2019 - Apr 2020
IBM
San Jose, California (Remote)

Spearheaded IBM’s Physical Documentation WATSON initiative. The project entailed processing thousands of scanned documents using a combination of Convolutional Neural Networks and Natural Language Processing, and worked with Word2Vec, BERT, LDA (Latent Dirichlet Analysis), ELMO (bidirectional LSTM), and Spacy (NER). This project entailed converting optically scanned documents into usable data for a later consumption by WATSON. The project involved using CNNs for document classification, AWS Textract, and Google Tesseract for OCR and NLP techniques for Named Entity Recognition (NER) and text document processing.
●Worked with a team to implement a high quality human interactive system NLP and other Deep Learning techniques.
●Performed classification on text data using NLP fundamental concepts, including tokenization, stemming, lemmatization, and padding using millions of documents.
●Constructed an NLP-based filter utilizing embedding and LSTM layers in Tensorflow and Keras.
●Used of a variety of NLP methods for information extraction or named entity recognition, topic modeling, and relationship extraction using NLTK and Spacy.
●Some of my NLP models using Python:
ohttps://github.com/Kishore1818/Neural-Networks-and-Deep-Learning-models

●Collected the image/text data, identified the proper features in the data, image pre-processing, and data split into train and test for build the Deep Neural Network and Artificial Intelligence models.
●Designed and built document summarization models using Python and Tensorflow to scan large documents and extract/summarize key facts and items from the scanned documents.
●Used Python to build Transformer models for translation of document languages.

Senior Data Scientist Sep 2015 - August 2019
The Weather Channel
Atlanta, Georgia
Heavy rainfall prediction is a major problem for the meteorological department, as it is closely associated with the economy and daily life. For this project, I developed several time-series models to predict rainfall across several distinct regions using a large amount of historic data from 1901-2015. These predictions were then additionally used to generate advance warnings in advance for natural disasters like floods and drought across the globe.
●Feature selection/reduction using Principal Component Analysis (PCA) to avoid multi-collinearity issues.
●Worked on data cleaning and ensured data quality, consistency, and integrity using Pandas and Numpy.
●Data stationarity purpose: I removed the long-term trend, seasonality, and periodicity from each region/location of the time series.
●Data stationarity validated with the Dickey-Fuller test.
●Applied Multiple Linear Regression and ARIMA Machine Learning models for rainfall prediction.
●Strong seasonality favored ARIMA performance.
●Final prediction accuracy was 86.67% and the F-measure value was 0.88 for estimating the efficiency of the model.
●Used Machine Learning techniques to predict future long-term trends using historical datasets.

Data Scientist Aug 2013 - Feb 2015
US Bank
Irvine, California

For this project a request evaluation system was developed to ingest customer requests from a diverse set of digital and handwritten sources, which were then filtered based on urgency and forwarded to the appropriate department. Handwritten sources were pre-processed using Optical Character Recognition (OCR) techniques, and then handed to the classification model in a hierarchical approach. Relevant departments were identified using metadata such as form and request type determined by the source. I used Python for developing Machine Learning models and Deep Learning models.

●Machine Learning models of Bayesian and KNN techniques along with Tesseract were compared for OCR applications based on model accuracy and speed.
●Models achieved 91%, 96%, and 97% accuracy, respectively.
●OCR model performance evaluated on textual and MNIST datasets.
●Tesseract provided the most consistent OCR results and was used in the productionized solution.
●Sorting subsequently done through the training and testing of an artificial neural network.
●Data Cleaning, Imputation, Tokenizing: used Python libraries (Pandas, NLTK, Numpy, Keras, Tensorflow, and PyTorch) to clean and prepare the data for analysis.
●Performed data integrity checks, data cleaning, exploratory analysis and feature engineering using R and Python.
●Performed machine learning, natural language processing (NLP), and statistical analysis methods such as clustering and classification.
●Urgency identified using a Natural Language Processing-based classifier.
●Final Deep Learning Bidirectional LSTM model achieved 85% test accuracy for identifying urgent vs. non-urgent.
●Production model deployed to a flask API for use by the business.
●Worked with a core-team of 4-6 individuals of Data Engineers, Data Analysts and Data Scientists.

Researcher Earth System Science Aug 2011 - July 2013
University of California Irvine
Irvine, California

Worked on several deep research projects in conjunction with National Science Foundation and UC Irvine. These projects involved creation of project proposals and submissions as well as experiment design, data gathering and project management.

●Served as Project Manager. Led five engineers and three data scientists in this NASA/UCI joint project.
●Built and maintained dashboard and reporting based on the statistical models to identify and track key metrics and risk indicators.
●Used Python and IDL to retrieve the historical hierarchical format (HDF5) and clean data prior to implementing and model training.
●Wrote functions to perform pre-processing to impute the missing values using a linear interpolation technique.
●Established normalization of features in the data to reduce noise and maximize signal-to-noise ratio.

Project Scientist Prior to 2011
NICT
Kyoto, Japan

As Japan's sole National Research and Development Agency specializing in the field of information and communications technology, NICT is charged with promoting ICT sector as well as research and development in ICT, which drives economic growth and creates an affluent, safe, and secure society. I worked for 6+ years in NICT, Japan, as a project scientist, and I was involved in several projects, namely the TRMM precipitation project and other GPS (Global Positioning System) projects. All these are projects in which I collaborated with a team to develop software for extracting signals from raw data and analyze the collected data using several statistical and mathematical techniques.

Personal Project
Covid-19 CT Scan Analysis

Procured Covid Patient Lung CT Images and developed a CNN model to identify Covid Positive Lung Morphology. The idea was to develop a machine learning method to aid radiologists in detection. At the time, there were very few tests available. The Developed model could identify the illness correctly 78% of the time and had a recall of 0.9.

●CT scan image data analyzed using Deep learning models of Convolution Neural Networks (CNNs).
●Dataset consisted of an even split of 743 COVID and non-COVID images from the medical office.
●Improve patient care standards and reduce workloads by extracting essential information from literature, manuals, and research articles using Natural Language Processing (NLP).
●Generated some of COVID-19 models and visualizations:
https://github.com/Kishore1818/Neural-Networks-and-Deep-Learning-models
https://github.com/Kishore1818/Animations
●Generated Visualizations from SAS data using R: https://preethamvignesh57.wixsite.com/mysite/covid-19
●Generated Tableau visualization of Covid-19 Data: https://preethamvignesh57.wixsite.com/mysite/tableau
Education
Ph.D in Physics
Radar Signal Processing and Data Analysis
Sri Venkateswara University
Tirupati, Andhra Pradesh, India
Masters of Science in Physics
Sri Venkateswara University
Tirupati, Andhra Pradesh, India
Bachelor of Science in Mathematics, Physics, and Statistics
Sri Venkateswara University
Tirupati, Andhra Pradesh, India
Machine Learning, Deep Learning, Artificial Intelligence models, codes, and visualizations:
https://github.com/Kishore1818

Professional Profile
Accomplished Data Scientist with 10 years of experience and overall IT experience of over 16 years that leverages a deep understanding of machine learning, artificial Intelligence, statistical, and mathematical techniques to propel business performance and extract maximum value from across several key domains.

●Experience in the application of Naïve Bayes, Regression Analysis, Neural Networks/Deep Neural Networks, CNN, RNN, Natural Language Processing (NLP), Support Vector Machines, and Random Forest machine learning techniques.
●Advanced statistical and predictive modeling techniques to build, maintain, and improve real-time decision systems.
●Creative thinking/strong ability to devise and propose innovative ways to look at problems by using business acumen, mathematical solutions, data models, and statistical analysis.
●In-depth knowledge of statistical procedures that are applied in both Supervised and Unsupervised Machine Learning, Deep Learning, and Artificial Intelligence problems.
●Machine Learning and Deep Learning techniques for implementing marketing and merchandising ideas.
●Ability to quickly gain an understanding of niche subject matter domains, and design and implement effective novel solutions for use by other subject matter experts.
●Experience implementing industry standard analytics methods within specific domains and applying data science techniques to expand these methods (e.g., using Natural Language Processing methods to aid in normalizing vendor names, implement clustering algorithms, and derive novel metrics).
●Experience in text processing (text detection, text recognition), Image processing, Image classification, labeling, DPI encoding for image quality optimization, blur correction, and skewed text correction.
●Experience working on advanced analytical teams to design, build, validate, and refresh data models.
●Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
●Experience with ensemble algorithm techniques, including Bagging, Boosting, and Stacking; knowledge with Natural Language Processing (NLP) methods, in particular FastText, Word2vec, Sentiment Analysis.
Contact
213-***-****
adnhmu@r.postjobfree.com

GitHub Profile
https://github.com/Kishore1818
Experience
10+ years of experience
Data Science & Machine Learning
8+ years of experience
Research and Scientific Inquiry
Competencies
Machine Learning
-Data Modeling
-Machine Learning
-Deep Neural Network
-Convolutional Neural Network
-NLP
-Artificial Intelligence

Predictive Analytics
Marketing Analytics
Retail Analytics
Geospatial Analysis

"
,
Data Scientist Berkeley,"
PARDEEP KUMAR
EMAIL: adnhdg@r.postjobfree.com PHONE: 510-***-**** LOCATION: San Francisco, CA
LINKEDIN: pardeep-kumar-ucb GITHUB: PraddyUno/
SUMMARY Data Scientist with a Ph.D. in Engineering and 4 + years of hands-on experience in data exploration and processing, and developing computational models in machine learning. Fluent in Python, SQL, RStudio, and Matlab. Recently obtained a data science certificate growing my knowledge in computer vision, natural language processing, and time series analysis using regression models and neural networks. SKILLS PROGRAMMING LANGUAGES & SOFTWARES: Python, MySQL, RStudio, Matlab MACHINE LEARNING & STATISTICS: Computer Vision, Neural Networks, Natural Language Processing, Time Series Analysis
EXPERIENCE
Springboard Data Science Career Track Certification 6 months intensive course in data science, machine learning, Python, SQL, and R University of California, Berkeley
Ph.D. Civil & Environmental Engineering
M.S. Civil & Environmental Engineering
PROJECTS Hate Speech Detection in Tweets using Decision Trees and Neural Networks (Python)
• Extracted features from the text data using three different approaches: TFIDF, N-grams, and word embedding for the multiclass classification of text
• Evaluated the Random Forest, XGBoost, MLP, CNN, LSTM in multiclass classification with balanced and unbalanced datasets using Keras and Scikit-learn
Stock Price Predictions using Time Series and Machine Learning Models (Python)
• Performed time series analysis on the major IT stock time histories with different volatility to develop a short-term stock price prediction model
• Compared to the traditional time series (exponential smoothing, ARIMA) approach with machine learning methods such as XGBoost and LSTM
Decision Tree and Time Series Models for Predicting Housing Prices in the Alameda County (RStudio)
• Evaluated the performance of regression and time series analysis in short- and long-term housing price predictions using inbuilt libraries in RStudio
• Developed a dynamic model combining the feature extraction ability of decision tree with ARIMA models to improve the predictive capabilities
EXPERIENCE BusyBee Paperless Transactions, Consulting Data Scientist, Part-time 2021 – Current
• Developed an efficient OCR system that identifies the handwritten data (signature, initials, date, etc.) in thousands of real estate forms using Python
• Trained SVM classifiers and deep neural networks for binary classification of handwritten vs. printed input using Scikit-learn and Keras in Python
Citco Fund Services, Administrator, San Francisco, CA 2016 – Current
• Upgraded and installed IT equipment in the server room on a regular basis and conducted regular testing in case of service interruptions
• Negotiated contracts with vendors to eliminate the unnecessary cost of manual data storage at an off-site facility saving more than $2,000 per month
University of California at Berkeley, Research/Teaching Assistant, Berkeley, CA 2009 – 2014
• Experimentally and analytically evaluated techniques to retrofit critically damaged section of the bridge columns after a major earthquake
• Suggested modifications to the existing 3D finite element models in OpenSees based on the processing and analysis of experimental data
• Provided hands-on training to the undergraduates in implementing and developing different statistical models and their applications in MATLAB
• Authored and peer-reviewed project reports, research papers, and journal articles for ASCE in seismic, bridge and structural engineering

"
,
Data Scientist,"
Sven Wu
Bay Area, California Mobile: 510-***-**** Email: adngjw@r.postjobfree.com
Linkedin Github Kaggle
EDUCATION
UC Berkeley - B.A. Data Science (Domain Emphasis on Human & Population Health) Berkeley, CA May 2021
● Activities: UC Berkeley Data Scholars - Pathways Member, The Sage Mentorship Program - Mentor PROJECTS
Swordﬁght Arena (2016) Main Game Developer
● Sword Fighting game I started building in 2014 for fun using Lua and ROBLOX Studio. Diet & Disease (2018) Programmer
● Analyzed data from cardiovascular health observational study to determine major causes of death in the world, examined clinical trials and ran hypothesis tests using Python. Classifying Movies (2018) Programmer
● Built a k-nearest neighbors classiﬁer to predict whether a movie was a romantic or an action ﬂick by exploring its screenplays.
Bear Maps (2019) Programmer
● Built a “mini” google maps-like application that mapped out the city of Berkeley with Java.
● Utilized data structures and built a small AI.
Spam vs. Ham (2019) Programmer
● Built a logistic regression model for classifying between spam and “ham”(non-spam emails).
● Threshold was 88%. Got 93.33% and was in the top 25% for the Kaggle competition. GGWP- Identify Toxic Behavior in Gaming (2020) NLP Algorithm Developer and Insight Reporter
● Used AWS to store and retrieve large data bases (21 million points) provided by GGWP startup.
● Examined factors for player toxicity by using Python and employing NLP.
● Generated player proﬁles so moderators could make informed decisions on dealing with toxic players(pban timeouts, IP block etc).
Collaborative Filtering for Recommending Songs to Users (2021) ML Algorithm Developer
● Used data of derived user ratings of songs based on each user’s listening behavior
● Utilized various machine learning methods such as cross-validation and validation set processing to construct a Collaborative Filtering model that recommended songs to speciﬁc users. SKILLS
● Technical Skills: Python(Numpy, Pandas, Sci-Kit Learn), R, SQL, Java, AWS, Google Colab, Github.
● Interests: Video Games, Basketball, Frisbee, Game Dev, Traveling, Movies, Writing.
● Awards: AP Literature Achievement Award
● HS Clubs: HS Frisbee Club Co-Founder/Teacher, Team NBA Tutor, CS Club Member
● Currently Self-Teaching: Javascript, CSS, HTML, Databases.

"
,
Prophet Data Scientist,"
RASHMITH REDDY BOPPIDI
469-***-**** adngiv@r.postjobfree.com https://www.linkedin.com/in/rashmith-reddy-boppidi-4669991a3/ SUMMARY:
Graduate in computer science from Rowan University, New Jersey with a master’s degree. Skilled in python, SQL, Apache airflow and Tableau with academic projects which prove my experience in Data analysis, Data science and Data engineering.
ACADAMIC QUALIFICATIONS
• Master of Science (MS), Computer Science 2020-2021
• Rowan University, New Jersey GPA: 3.2/4
TRAININGS AND CERTIFICATIONS:
• IBM Data Science Professional Certification
• Data scientist with Python Track
WORK EXPERIENCE:
MedTourEasy
Data scientist trainee March 2021 - April 2021
• Source data is from a GPS fitness tracker called Run-keeper.
• Analysis of fitness data using python.
• Finding Statistics such as average distance, average heart rate. PROJECTS:
yelp top five bookstores in New Jersey : (Python, SQL, Apache Airflow, ETL/ELT pipelines)
• Used Yelp Api to get business data (book stores).
• Transformed the data then loaded in SQLite database. (ETL/ELT)
• used sqlachelmy to extract the loaded data.
• Scheduled it with Apache Airflow.
• Using sql showed top 5 book stores.
California location recommendation system for migrants: (Python, Scikit-learn)
• Used foursquare api for location data.
• One-hot encoding
• Used K-means clustering.
• Used K-NN for recommendation.
Age-Gender-ethnicity classification from human images: (Python, Keras, Matplotlib, CV2)
• Used UTK data set.
• Used CV2 for image pre-processing.
• Built three CNN models for age, gender and ethnicity predictions
• Compared different models and their accuracies.
• Matplotlib for viewing results.
COVID-19 analysis and forecasting: (python, time series, prophet)
• Data merging and cleaning using pandas.
• Covid data analysis and visualization using pandas, matplotlib and seaborn.
• Use of prophet library for forecasting future Covid cases Co2 Emissions and Population for Top 10 Countries( Tableau, Dashboards, Visualization)
• Used World indicators data set.
• Created interactive dashboards for representing population on the map And Bar charts for GDP, Avg co2 emission.
• Added Filters for years.
SKILLS
• Programming languages: Python(numpy, pandas, matplotlib, seaborn, scikit-learn, keras, tensorflow), SQL, Java script, CSS, Php, HTML, Apache Airflow
• Software’s and tools: Tableau, Linux/Unix, jupiter notebook, pycharm,
• GITHUB : https://github.com/mlengineer007?tab=repositories

"
,
Research Scientist Data,"
SAYYED M. ZAHIRI
US Citizen
Milpitas, California jTel: 408-***-**** jEmail: adnees@r.postjobfree.com jLinkedIn URL PROFILE
• 10+ years of experience in Machine/Deep Learning and Predictive Modeling; 3+ years of leadership experience
• 4+ years of experience in Information Extraction, Knowledge Graph Construct and Data Mining
• Customer facing experience, architecture and implementing solutions. EXPERIENCE
Principal Data Scientist - Search and Recommendations at Kaiser Permanente, Pleasanton, CA April 21-Present Query Understanding and Search Retrieval Enhancement:
Developing machine/deep learning models to recognize the intents from search terms
Constructing a healthcare knowledge graph which will empower both search and recommendations with Probabilistic Reasoning. Conversational AI: Leading a team of two data scientists to build a healthcare chatbot which leverages AI to infer members’ intents and connecting them to a right department
Leading University Collaboration program: Mentoring three computer science PhD students. This is part of Kaiser university-academia collaboration program.
Senior Data Scientist - Search Science at The Home Depot, Atlanta, GA October 18-April 21 Semantic Product Search:
• Developed deep semantic matching in e-commerce product search. Given a customer query, deep learning model retrieves all semantically related products from the catalog. The model is expected to increase key performance indicators by a large amount. I led deep learning model development in this project
• Utilized a new loss function that has an inbuilt threshold to di erentiate between the signals available in behavior data Led Product Knowledge Graph Enrichment and Search Retrieval Enhancement project:
• Built an active learning framework to discover product types and intrinsic product relations (this project accepted in KDD 2020 workshop of Knowledge Graph and E-Commerce). In total 7 Data Scientists, Analysts, Engineers participated
• Multimodal Deep Learning model to predict types of products. The model leveraged products’ titles, images, ... Query Classi cation: Mixed encoder neural model to predict multiple categories for a given query. User clicks employed as weak supervision to capture user intent. Leveraged continual learning pipeline to incorporate users’ new implicit feedback Automatic Product Categorization model: A Deep Learning based model which takes product’s information from a vendor and recommends relevant product categories. This model is expected to shorten on-boarding process time Led University Collaboration program: Mentored two computer science PhD students at Emory university. This was part of The Home Depot university-academia collaboration program. Two Papers recently accepted at SIGIR2021 Arti cial Intelligence Engineer at Stanley Black and Decker, Atlanta, GA February 18-October 18
• Abnormality Detection from Images: Collaborated with the department of Oil & Gas to develop computer vision algorithms for detecting defective areas in weld radiography images
• Product Review Analysis:Analyzed and extracted information from Stanley0s online product reviews using NLP and Deep/Machine learning algorithms such as POS tagging, dependency parsing, sentiment analysis, TFIDF, Word2Vec, Doc2Vec and Latent Dirichlet Allocation
Machine Learning Research Scientist at Emory, Atlanta, GA May 16-December 17 Led Neural Networks-Based Model for Multi-modal Emotion Recognition project:A multi-modal approach for the emotion recognition that integrates information coming from videos and subtitles of Friends TV show was proposed. Dataset annotated through Amazon Mechanical Turk platform. In this project, I led two graduate students Text-Based Emotion Detection from TV Show Transcript (preprint version) :
• Utterance-level emotion detection from transcripts of Friends TV show by applying Natural Language Processing techniques
• Emotion of each utterance was classi ed into one of the seven primary emotions using the following approaches:
- A novel Sequence-based Convolutional Neural Networks (CNNs) with an attention mechanism
- A combination of a Recurrent Neural Network (RNN) and CNN
• The dataset was gathered through Amazon Mechanical Turk platform and is publicly available (link to the dataset) Collaboration with Evolution of Language and Information Technology (ELIT) team:
• Elit is developed by Emory NLP research group with collaboration of Amazon MXNet team to provide end-to-end NLP pipeline
Abnormalities Detection in 3D MRI Brain Images:
• Abnormalities detection in 3D MRI brain images using 3D CNNs, RNN-CNN models and Image Segmentation algorithms. The dataset was provided by the department of Radiology and Imaging Sciences of Emory hospital Machine Learning Research Scientist at Amir-Kabir University, Tehran, Iran September 11-January 13
• Simulated Passive Dynamic Walker (Robot) that energy lost in the collisions is balanced by kinetic energy gained going down a ramp. This allows walker to achieve dynamically stable limit cycles, resulting in a natural and anthropomorphic motion without any external energy input.
• Utilized spring on the hip joint; value of the spring constant determined by a Feed Forward Neural Network (Supervised Learning). All simulations proceeded by aid of MATLAB.
• The improved walker could walk at ramp angle 50% larger. In this project I mentored one under-graduate student Natural Language Processing Research Internship at CareerBuilder, Atlanta, GA December 17-February 18
• Applied NLP, Machine/Deep Learning algorithms for building a resume to job matching system SKILLS
Programming/scripting languages: Python (Pro cient), MATLAB (Pro cient), C/C++ (Familiar), SQL,PHP, JavaScript Frameworks and tools: Tensor ow, Keras, OpenCV, PyTorch, MXNET, PyCa e, Scikit-Learn, Pandas, Flask, Git, AWS Arti cial Intelligence: Machine Learning and Statistics, Deep Learning, Natural Language Processing, Computer Vision Control Systems/Robotics: Linear Systems and Dynamics, Optimal Control, Multi-Agent Systems and Networked Control EDUCATION
Georgia Institute of Technology, GA, US August 13-December 15 MS in Electrical and Computer Engineering (minor in Mathematics) Overall GPA: 3.90/4.00 Focus: Machine Learning, Deep Learning, Natural Language Processing, Computer Vision, Robotics University of Birmingham, Birmingham, UK July 10-June 11 Bachelor of Electrical Engineering Overall GPA: First Class Honours Amirkabir University of Technology, Tehran, Iran September 07- June 10 Transferred to University of Birmingham
PUBLICATIONS
APRF-Net: Attentive Pseudo-Relevance Feedback Network for Query Categorization (Accepted in SIGIR 2021, preprint version)
De-Biased Modeling of Search Click Behavior with Reinforcement Learning (Accepted in SIGIR 2021, preprint version) CRAB: Class Representation Attentive BERT for Hate Speech Identi cation in Social Media (Submitted to WWW 2021, preprint version)
Active Learning for Product Type Ontology Enhancement in E-commerce (Published in KDD 2020 workshop of Knowledge Graph and E-Commerce, link to the paper) Next-Generation of Weld Quality Assessment Using Deep Learning and Digital Radiography (Published in Arti cial Intelligence in Manufacturing, The 2020 AAAI Spring Symposium Series, link to the paper) Segmentation of brain MR images using a proper combination of DCS based method with MRF (Published in Multimedia Tools and Applications, Springer, link to the paper) Emotion Detection on TV Show Transcripts Using Sequence-based Convolutional Neural Networks (Published in AAAI-18, A ective Content Analysis workshop, preprint version) PROJECTS
CRAB: Class Representation Attentive BERT for Hate Speech Identi cation in Social Media May 20-July 20
Proposed a transformer-based neural model to identify hate speech from Twitter data.
The model leveraged matching scores between trainable class representations and encoded input data to detect hate speech
CRAB model outperformed strong state-of-the-art baselines by a large margin (preprint version) Facial Expression Recognition using CNNs and Faster Region-based CNN January 17-March 17
Classi ed 100000 labeled 256 256 color images containing one of 7 emotional categories (Angary, Happy, . . . )
Faster region-based CNN was utilized to detect the faces from the training examples. Accuracy rate of 75% achieved Segmentation of brain 3D MR images January 16-May 16
A novel algorithm called Dynamic Classi er Selection Markov Random Field (DCSMRF) for supervised segmentation of MR images into three main tissues was introduced. This project led to a publication in the Journal of Springer
DCSMRF combines a novel ensemble method called Dynamic Classi er System-Weighted Local Accuracy with MRF algorithm Tra c Sign Detection and Recognition from video Streams September 15-December 15
Color and Shape based segmentation for the detection and Convolutional Neural Networks for the recognition part were used Networked Control and Multi-Agent Systems November 15
Navigated a team of 6 simulated robots through the rough terrain and located and re-activated the 8 disabled robots 3D Remote Sensing September 10-July11
Laser scanning method utilized to render 3D Topographic Map of a 3-Acre outdoor place with accuracy of 2ft. The allocated budget for this project was $1500
Participated in Microcontroller programming and ArcGIS (Geographic Information System) software as part of a 5-member team ACHIEVEMENTS AND ADDITIONAL INFORMATION
Ranked rst student award among all Electrical Engineering students at Amirkabir University Scholarship from University of Birmingham due to outstanding academic background (2010) Language: English(Fluent), Persian(Native)

"
,
Data Analyst Scientist,"
REETI BHAGAT,MBA
adncyy@r.postjobfree.com 714-***-****
https://www.linkedin.com/in/reeti-bhagat/ reetibhagat
Irvine,CA (Green card holder)

SUMMARY
Data Scientist / Data Analyst with a MBA with +4 years of professional experience in Marketing and Sales and 2 years of experience in data science and machine learning using Python . Skilled in data analysis using SQL, business intelligence using Tableau, and AWS cloud management. Proﬁcient in statistical modeling, predictive analytics, time-series analysis,NLP and deep learning. Certiﬁed in data science after completing a very immersive program from Springboard bootcamp and completed two capstone projects. EXPERIENCE
Springboard, Data Science Fellow, Irvine, California, United States May 2020 - Dec. 2020
• Tools: python, pandas, matplotlib, sklearn, scipy, postgresql, git, pyspark, AWS and GCP cloud. Projects:
• Covid-19 Analysis, Visualization and Forecasting using Time Series Analysis
• Detecting Pneumonia from X-ray Images
Tulips, Data Analyst Feb. 2019 - Mar. 2020
Created data visualization dashboard using Tableau, seaborn and matplotlib. Develop a model to increase the revenue of hospital conducting predictive analytics using tool like python increases revenue from 3 to 6%. Utilized descriptive and predictive statistical techniques to measure impact of various actions/studies, internal and external, develop sampling and hypothesis testing to help the hospitals determine outcomes, and forecast and predict future behaviors that explain key ﬁndings using tools like python,SQL and machine learning. Studied client data to identify emerging trends and recommend solutions for programs based on analytical ﬁndings. Executed various MYSQL database queries from python MYSQL connector (SQLite). Experience working with JIRA with project management tools. Health Care Global Enterprises Ltd, International Marketing Executive June 2017 - Feb. 2019 Planned, strategized, implemented and monitored the budget and ﬁnancial targets Conducted oncology health camps, conferences, CMEs, and round table meetings Conducted market research focused on oncology and created strategies to create brand awareness Experience working with google analytics.
Oxford University Clinical Research Unit, Office Manager 2015 - 2017 Oversaw the selection of equipment used in the facility and ensures that it works properly Involved in the selection and ordering of supplies and handling the ﬁnancial aspects of the business, such as billing, banking and accounting Ensured that the staff utilizes the correct coding when designating the diagnosis or results of procedures. MACHINE LEARNING: classiﬁcation, Regression, clustering, feature engineering, Time series, Deep learning, NLP, Excel, ETL STATISTICAL METHODS: hypothesis testing and conﬁdence intervals, principal component analysis, Probability and Statistics, problem solving PROGRAMMING AND VISUALIZATION: Python((scikit-learn, numpy, scipy, pandas), SQL, Tableau, Hadoop(Mapreduce), Spark, GCP cloud(AUTOML /VISION), AWS(EC2,SAGEMAKER,INCOGNITO,LAMBDA,S3) SKILLS
PROJECTS
Covid-19 Analysis, Visualization and Forecasting using Time Series Analysis May 2020 - July 2020 The main goal of this project is to analyze the spread trend of this virus all over the world and its predictions. I have used time series models to forecast the conﬁrmed and deaths cases. It is implemented in Python on Jupyter notebook. Project outcomes will include a google Collab notebook containing associated code, reports and presentation in GitHub. Detecting Pneumonia from X-ray Images
The main objective of project is to help doctors better predict pneumonia in minimal time with high efﬁciency. The aggregation of this will contribute to the health care system for better patient satisfaction and care. I have developed Convolutional Neural Network from scratch to classify the medical images. The CNNs will be implemented in Python on Google Collab using the Keras interface to TensorFlow. I will be using GCP/AI platform to build and deploy models. I will also use AWS sage maker to train and deploy the models.
Project outcomes will include a google Collab notebook containing associated code, reports and presentation in GitHub. EDUCATION
Springboard Data Science Career Track, Certification (Data Scientist) Student in an online program consisting of 600 hours of hands-on curriculum in partnership with Microsoft, with 1:1 industry expert mentor oversight, and completion of 2 in-depth capstone projects. Mastering skills in Python, SQL, data analysis, data visualization, hypothesis testing, and machine learning.
Capstone Projects:
https://github.com/reetibhagat/Capstone-project2
https://github.com/reetibhagat/capstone-1-covid-19 Simplilearn Master's in Data analytics, Certification mastered the concepts of statistics, data analytics techniques, Excel analytics functions, Tableau and Power BI, Python, and R programming. JSS Academy of Higher Education & Research
MBA
Thesis on :Management of the attrition of Human resource in Health care. Publication on 4th International Human Resource Conference VOLUNTEERING
CrowdDoing, Data Scientist volunteer Dec. 2020 - Feb. 2021 Collaborating at nature counter project to determine the effectiveness/beneﬁts of spending time in nature in short term and long term basis by user.

"
,
Science Intern Data Scientist,"
SRIKAR
PRAYAGA
adncws@r.postjobfree.com
909-***-****
San Diego, California
92336
EDUCATION
University Of California - San
Diego
La Jolla, CA • Expected in
**/****
Bachelor of Science : Data
Science
Major GPA: 3.4
● Coursework in Big Data
Analytics, Advanced Data
Structures and Algorithms,
Object Oriented Design,
Digital and Analog Circuit
Design,
Supervised/Unsupervised
Machine Learning, Data
Mining, Database
Management, Data
Visualization, Data Analysis
and Modelling
PROFESSIONAL SUMMARY
Driven Data Scientist ready to thrive in demanding digital intelligence processing environments. Well-informed on latest machine learning advancements. Ready to combine tireless hunger for new skills with desire to exploit cutting-edge data science technology!
WORK HISTORY
Katch Media - Data Science Intern
Los Angeles, CA • 07/2020 - 12/2020
● Skills Learned: Advanced Excel (pivot tables, advanced formulas, nested formulas, macros) BeautifulSoup, Tensorflow, Keras, Pandas, Numpy, Rest APIs, Git, statistical
modelling tools, Data Visualization techniques, AWS, Spark
● Served as team lead for ""TEAM GEO"" by managing
checkpoints on JIRA
● Performed intensive wrangling for large-scale databases using Python and SQL to create effective datasets for projects.
● Successfully developed 70 models using Random Forests, Logistic Regression and Neural Networks for the Movie Genome Project.
● Implemented Gradient Boosting/Tuning algorithms to increase accuracies from 65% to 91%.
● Visualized and presented the best models to the C-Suite using Tableau dashboards and effective data storytelling.
● Created daily reports of work on Confluence and authored 4 official technical reports for company use.
Codepath - Tech Fellow
San Diego, California • 01/2020 - Current
● Skills Learned: Swift, XCode, Parse/MongoDB, REST APIs, Webkit, Mapkit.
● Teaching complex topics in iOS Development such as APIs, MVC Architecture, Database in Parse/MongoDB, WebKit, MapKit, and AVFoundation.
TheCoderSchool - Code Coach
Cupertino, CA • 03/2021 - Current
● Skills Learned: Swift, Python, SQL, Java, C,C++, Scratch.
● Teaching hands on project based coding classes for kids from ages 8-18.
● Developing a bespoke lesson plan and teaching style to match each student's learning capabilities
SKILLS
● Java, Python, R, SQL,
C/C++, Verilog, Swift,
Matlab, Bash, Assembly,
Advanced Excel
● AutoCad, Revit,
Solidworks, Tableau,
Tensorflow, Git, Vim,
React, Parse, MongoDB,
Pandas
ORGANIZATIONS
IEEE,DS3 (Data Science
Society), UCSD Sitaare
(Competitive South Asian
Acapella Team)
AWARDS
Provosts Honors- Warren
College
Most Improved Singer- UCSD
Sitaare Acapella
CERTIFICATIONS
-Coursera Deep Learning AI
Certificate
-Codepath iOS BootCamp
-JP Morgan Software
Engineering Certificate
PROJECTS
/*Data Science Projects*/
COVID Face Mask Detector
Worked with: Open CV, Keras, Tensorflow, Deep Learning
● Use facial landmarking to build a training dataset of images of people with masks
● Achieved a 99% accuracy rate on the test set in terms of identifying if a person had a mask or not.
● Implemented it to detect masks in real time using a webcam.
Fraud detection in Credit Card Transaction
Worked with: Pandas,Sk-Learn, NLP
techniques,Tensorflow,PyTorch,Keras
● Identified fraud transactions leveraging predictive modelling with optimization attaining precision of 98.03%.
● Tested against classification algorithms such as KNN(K Nearest Neighbor), XG Boost Classifier, Random Forest Classifier.
Loan Underwriting Model
Worked with: AWS EC2, AWS EMR, Pyspark, Dask,
Pandas, Sk-Learn
● Built a classification model using Freddie Mac Loan datasets to predict loan defaults.
/*Software Engineering Projects*/
Worked with: Xcode 11, Swift, Cocoapods, Parse
ParsyGram - Instagram iOS app clone with a custom Parse backend
-An Instagram-style social media application where users can create accounts to upload and share pictures. The application utilizes a custom Parse backend to securely store users and passwords.
Twitter Client - Duplicates functionality of Twitter with a fresh look
-Client app that allows users to log in and view their feed. This is done by using Twitter's RESTful API. Likewise, the users can like, retweet, and refresh their live feed. Apartly- Productivity app that implements the Pomodoro method
- A productivity app that is designed to keep any individual on track of work during quarantine. Users will be able to join study/work rooms with specific tags with a global productivity timer.
- A productivity app that is designed to keep any individual on track of work during quarantine. Users will be able to join study/work rooms with specific tags with a global productivity timer.
.

"
,
Analytics Consultant Business Analyst,"
Ting Kai Chang
Sunnyvale, CA adnb5p@r.postjobfree.com 517-***-****
LinkedIn: www.linkedin.com/in/tingkaichang/
EDUCATION
Michigan State University - Broad College of Business East Lansing, MI 05/2021 Master of Science, Business Analytics
National Taiwan University, Taipei City, Taiwan 06/2016 Bachelor of Science, Atmospheric Science
PUBLICATIONS & PRESENTATIONS
Channel Design and OEM Growth in a Multi-market Setup - H.H. Lee, T.K. Chang, C.W. Kuo, European Journal of Operational Research 04/2021
PROFESSIONAL EXPERIENCE
Ford Motor Company – Dearborn MI 06/2020 – 08/2020 Data Analytics Consultant (Project)
• Explored vehicle trace and road network data to reduce confounding noise and identify avenues of data collection improvement
• Developed a semi-interacting multiple model (sIMM) algorithm, using an off-road Kalman Filter run in parallel with a Hidden Markov Model (HMM) to detect various map errors
• Applied visualization tools to demonstrate a 55% accuracy improvement of road network information when utilizing vehicle GPS traces over open-source map data Masco Corporation – Livonia MI 01/2020 – 05/2020
Data Analytics Consultant (Project)
• Contributed insights for profit optimization and promotion decisions by building a price-influencing model using R, and machine learning techniques such as PCA and Ridge regression National Taiwan University - Taipei City, Taiwan 07/2016 - 12/2019 Research Scientist
• Employed Nash solutions with Matlab, Python, and Winedt to explore supply chain and pricing between an ODM and retailers to obtain optimal wholesale price and ensure profitability
• Generated robust insights for an OEM to support direct selling channels in developing countries
• Demonstrated gains in higher profit premiums, market coverage, and product accessibility
• Investigated product positioning and sales of an online retailer versus brick-and-mortar store to understand the impact of selling varying quality products in different environments Business Analyst Consultant
• Implemented Excel workflows to build an efficiency model to support engineers and the successful pricing of services through cost calculations
• Analyzed company operation process using business modeling tools and consumer data to increase customer satisfaction and improve survey responsiveness
Taishin International Bank - Taipei City, Taiwan 07/2015 - 09/2015 CSR department
• Aided in the reduction of carbon emissions in support of corporate sustainability commitments
• Improved Tableau dashboards to generate data-driven, statistics focused insights for decision makers Taishin Fundation
• Reviewed data from funding applications and angel investors to match applicants with financial organizations SKILLS
Technical: Statistics, Modeling, Machine Learning, Regression, Classification, Forecasting, Research Data: Big Data, Data Analytics, Data Mining, Visualization, Data Modeling, Database Management Software: Python, R, SQL, Tableau, Matlab, Microsoft Excel, Microsoft Word, Microsoft PowerPoint Business: Business Strategy, Supply Chain, Management, Business Intelligence, Price Optimization

"
,
J2ee Developer Hadoop Data,"
Sathwik Kodipalli
Ph: +1-443-***-**** Email: adnb37@r.postjobfree.com

PROFESSIONAL SUMMARY:

Around 7+ years of experience with strong emphasis on Design, Development, Implementation, and Deployment of Software Applications.
Over 5+ years of comprehensive experience in Data Engineering with strong emphasis on Big data and Hadoop ecosystem frameworks.
Hands on experience with Hadoop Ecosystem components like Spark, MapReduce (Processing), HDFS(Storage), Hive, Impala (Analytical Querying), Yarn, Sqoop, HBase, Oozie and Kafka.
Strong knowledge on various programming languages with expertise in Java, Scala and Python.
Extensive experience writing end to end Spark Applications both using Scala and Python and utilizing Spark RDD, Spark DataFrames, Spark SQL and Spark Streaming.
Gained good experience troubleshooting long running jobs in Spark and fine tuning the performance bottlenecks.
Good experience creating real time streaming pipelines using Kafka and Spark Streaming for consuming.
Experience working with both Distributions (CDH, HDP) and cloud services primarily AWS.
Solid experience working with various native services in AWS Cloud like S3, EMR, Athena, Glue, Redshift, AWS SWF etc., for building data pipelines.
Experience working with NoSQL databases like HBase and Cassandra.
Good experience in handling data manipulation using python Scripts and experience in developing Python scripts for automation.
Experience in developing MapReduce jobs in Java for data cleaning and pre-processing.
Expertise in writing Hive Scripts and extended their functionality using User Defined Functions (UDF's).
Expertise in modelling data efficiently in Hive tables using Partitions and Bucketing.
Expertise in preparing Interactive Data Visualization's using Tableau Software from different sources.
Expertise in Object-Oriented Analysis and Design (OOAD) like UML and use of various design patterns.
Experience in Java, JSP, Servlets, EJB, Web Logic, Web Sphere, Hibernate, Spring, JBoss, JDBC, RMI, Java Script, Ajax, jQuery, XML and HTML.
Fluent with the core Java concepts like I/O, Multi-Threading, Exceptions, Reg Ex, Data Structures and Serialization.
Extensive experience in Java and J2EE technologies like Servlets, JSP, JSF, JDBC, JavaScript, ExtJS, spring, hibernate, and Junit testing.
Performed Unit Testing using Junit Testing Framework and Log4J to monitor the error logs.
Experience in process improvement, Normalization/De-normalization, Data extraction, cleansing and Manipulation.
Expertise in working with Transactional Databases like Oracle, SQL server, My SQL, and Db2.

TECHNICAL SKILLS:
Big Data Ecosystem
Spark, Pyspark, Mapreduce, Hive, HDFS, Sqoop, HBase, Flume, Oozie, Impala, Kafka, Nifi
Languages
Java, Scala, Python
No Sql
HBase, Cassandra, MongoDb
Databases
MySQL, Teradata, Oracle
IDEs
Eclipse, Intellij, PyCharm
Other Tools
Maven, Jenkins, Putty, WinSCP, Jira, Confluence
Version Control
GitHub, SVN, CVS
Methodologies
Agile, Waterfall

PROFESSIONAL EXPERIENCE:

Client: Thrivent Financials, Dunwoody, GA Jan 2020 – Present
Role: Sr. Data Engineer

Responsibilities:
Worked on development, testing and deployment of Spark applications.
Worked on troubleshooting and fine-tuning Spark jobs.
Worked on building real time pipelines using Kafka and Spark Streaming.
Written Kafka producers using Kafka Producer Api and integrated with Spark Streaming applications for consuming the stream messages.
Worked on automating the data pipelines and ensuring the reliability of the data pipelines.
Used Spark JDBC Readers for connecting to external databases and pulling the data to S3 data lake.
Used Spark JDBC Writers for connecting to redshift and writing the processed dataframes to Redshift.
Used Hive scripting for producing custom and adhoc data sets requested by downstream business teams.
Utilized Glue metastore service in AWS for storing all the Hive metadata.
Utilized Athena Interactive Query Service in AWS for performing data analysis.
Automated launching of EMR Spark clusters using AWS Java SDK and terminating the clusters once step is finished.
Responsible for creating Spring Boot based Rest applications to allow some of the metadata and preview of the processed data to be consumed by downstream application teams.
Responsible for automating CICD build and deployment using Jenkins.

Environment: AWS EMR, Spark, HDFS, Python, Hive, HBase, HiveQL, Sqoop, Java, Scala, Unix, IntelliJ, Autosys, Maven

Client: Cigna, Bloomfield, CT Oct 2018 – Dec 2019
Role: Sr. Hadoop Developer

Responsibilities:

Involved in developing roadmap for migration of enterprise data from multiple data sources like SQL Server, provider databases into S3 which serves as a centralized datahub across the organization.
Loaded and transformed large sets of structured and semi structured data from various downstream systems.
Developed ETL pipelines using Spark and Hive for performing various business specific transformations.
Building data applications and automating the pipelines in Spark for bulk loads as well as Incremental Loads of various Datasets.
Worked closely with our data scientist team’s and business consumers to shape the datasets as per the requirements.
Automated the data pipeline to ETL all the Datasets along with full loads and incremental loads of data.
Performed bulk load of JSON data from s3 bucket to snowflake.
Used Snowflake functions to perform semi structures data parsing entirely with SQL statements
Utilized AWS services like EMR, S3, Glue Metastore and Athena extensively for building the data applications.
Implemented a 'server less' architecture using API Gateway, Lambda, and Dynamo DB and deployed AWS Lambda code from Amazon S3 buckets. Created a Lambda Deployment function, and configured it to receive events from your S3 bucket
Worked on building input adapters for data dumps from FTP Servers using Apache spark.
Generating Data Models using Erwin9.6 and developed relational database system and involved in Logical modeling using the Dimensional Modeling techniques such as Star Schema and Snowflake Schema
Wrote spark applications to perform operations like data inspection, cleaning, load and transforms the large sets of structured and semi-structured data.
Developed Spark with Scala and Spark-SQL for testing and processing of data.
Reporting the spark job stats, monitoring and running data quality checks are made available for each Datasets.
Used SQL Programming Skills to work around the Relational SQL Databases.

Environment: AWS Cloud Services, Apache Spark, Spark-SQL, Unix, Kafka, Scala, SQL Server.

Client: BBH NYC, NY Jan 2017 – Sept 2018
Role: Big Data /Hadoop Engineer

Responsibilities:
Involved in importing and exporting data between Hadoop Data Lake and Relational Systems like Oracle, MySQL using Sqoop.
Involved in developing spark applications to perform ELT kind of operations on the data.
Modified existing MapReduce jobs to Spark transformations and actions by utilizing Spark RDDs, DataFrames and Spark SQL API’s
Utilized Hive partitioning, Bucketing and performed various kinds of joins on Hive tables
Involved in creating Hive external tables to perform ETL on data that is produced on daily basis
Validated the data being ingested into Hive for further filtering and cleansing.
Developed Sqoop jobs for performing incremental loads from RDBMS into HDFS and further applied Spark transformations
Loaded data into hive tables from spark and used Parquet columnar format.
Created Oozie workflows to automate and productionize the data pipelines
Migrating Map Reduce code into Spark transformations using Spark and Scala.
Collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis.
Did a Poc on GCP cloud services and feasibility of migrating onprem setup to GCP cloud and utilizing various services in GCP like Dataproc, BigQuery, Cloud Storage etc.,
Designed, documented operational problems by following standards and procedures using JIRA

Environment: Hadoop, Hive, Impala, Oracle, Spark, Pig, Sqoop, Oozie, Map Reduce, GIT, Confluence, Jenkins.

Kockpit Analytics, Bangalore, India Sep. 2015 – Dec. 2016
Hadoop Developer

Responsibilities:

Involved in importing data from Microsoft SQLserver, MySQL, Teradata. into HDFS using Sqoop.
Developed workflow in Oozie to automate the tasks of loading the data into HDFS.
Used Hive to analyze the partitioned and bucked data to compute various metrics of reporting.
Involved in creating Hive tables loading data, and writing queries that will run internally in MapReduce
Involved in creating Hive External tables for HDFS data.
Solved performance issues in Hive and Pig Scripts with understanding of Joins, Group and Aggregation and perform the MapReduce jobs.
Used Spark for transformations, event joins and some aggregations before storing the data into HDFS.
Troubleshoot and resolve data quality issues and maintain elevated level of data accuracy in the data being reported.
Analyze the large amount of data sets to determine optimal way to aggregate.
Worked on the Oozie workflow to run multiple Hive and Pig jobs.
Worked on creating Custom Hive UDF's.
Developed automated shell script to execute Hive Queries.
Involved in processing ingested raw data using Apache Pig.
Monitored continuously and managed the Hadoop cluster using cloudera manager.
Worked on different file formats like JSON, AVRO, ORC, Parquet and Compression like Snappy, zlib, ls4 etc.
Involved in converting Hive/SQL queries into Spark transformations using Spark RDDs.
Gained Knowledge in creating Tableau dashboard for reporting analyzed data.
Expertise with NoSQL databases like HBase.
Experienced in managing and reviewing the Hadoop log files.
Used GitHub as repository for committing code and retrieving it and Jenkins for continuous integration.

Environment: HDFS, MapReduce, Sqoop, Hive, Pig, Shark, Spark, Oozie, MySQL, Eclipse, Git, GitHub, Jenkins.

INDUS NET TECHNOLOGIES, India July 2014 – Aug. 2015
Role: Java/J2EE developer
Responsibilities:
Involved in designing Class and Sequence diagrams with UML and Data flow diagrams.
Implemented MVC architecture using Strut’s framework to get the Free Quote.
Designed and developed front end using JSP, Struts (tiles), XML, JavaScript, and HTML.
Used Struts tag libraries to create JSP.
Implemented Spring MVC, dependency Injection (DI) and aspect-oriented programming (AOP) features along with Hibernate.
Experienced with implementing navigation using Spring MVC.
Used Hibernate for object-relational mapping persistence.
Implemented message driven beans to get from queues to send again to support team using MSend commands.
Experienced with hibernate core interfaces like configuration, session factory, transactional and criteria interfaces.
Reviewed the requirements and Involved in database design for new requirements
Wrote Complex SQL queries to perform various database operations using TOAD.
Java Mail API was used to notify the Agents about the free quote and for sending Email to the Customer with Promotion Code for validation.
Involved in testing using Junit.
Performed application development using Eclipse and Web Sphere Application Server for deployment.
Used SVN for version control.

Environment: Java, Spring, Hibernate, JM’s, Web Services, Ejb, Sql, Pl/Sql, Html, CSS, Jsp, java script, Ant, Junit, Web sphere.

EDUCATION:
Bachelors of Technology in Computer Science & Engineering at Gitam University, Hyderabad.
July 2010-Apr 2014

"
,
Data Scientist Or Coding,"
Arshita Srinivasa Reddy
330-***-****
adnaqy@r.postjobfree.com

Data Scientist/Machine Learning
Summary
Around 8+ years of experience on Data Analytics, Machine Learning (ML), Predictive Modeling and Natural Language Processing (NLP) and developing Enterprise applications SDLC
Develop Python programs to engineer the Data pre-processing, validation required for the ML Models
Excellent knowledge using python packages like (pandas, numpy, scikit learn, tensor flow, matplotlib, seaborn) for solving different problems.
Adept in applying Machine Learning like Supervised Learning(Classification and Regression) and Unsupervised learning(Clustering) on different datasets.
Familiarity with Machine Learning solution offerings/operationalize from cloud providers such as AWS, Azure, GCP.
Strong experience in developing applications using Flask, Django framework
Basic knowledge in Devops, Master data management
Knowledge in creating data mapping documents for Extracting, Transforming, and Loading (ETL) tasks.
Excellent knowledge in creating Databases, Tables, DDL/DML queries, Triggers, Views, User defined data types, effective functions, Cursors and Indexes using PSQL.
Extensive experience in-depth data analysis on different data bases and structures. Strong knowledge in writing PSQL and MySQL Queries, sub-queries, CTEs and complex joins.
Experience on Hive queries and tables that helped to identify trends and patterns on historical data
Knowledge in Hadoop Architecture, HDFS Framework and its eco system like Hadoop Map Reduce, HIVE, HBase, Sqoop and Oozie.
Knowledge of Java/Scala/Apache Spark
Performed statistical data analysis such as hypothesis testing, Anova, chi-sq test, regression, association, correlation etc.
Experience in Probability, Statistics & Mathematics foundation for AI/ML
Strong computer science fundamentals such as algorithms, data structures, multithreading, object-oriented development, distributed applications, client-server architecture

Technical Skills
Libraries
Pandas, Numpy, Scikit learn, Matplotlib, nltk, Plotly, Seaborn
Machine Learning Algorithm
Linear & Logistic Regression, SVM, Decision Tree, Random Forest, KNN, Naïve Bayes, K-Means Clustering, XG boost
Framework
Flask, Spring MVC, Django
Tools
Pycharm, Jupyter notebook, Anaconda, Eclipse
Database
SQLite, MongoDB
Cloud
Heroku, AWS[EC2,S3,Sagemaker]
Others
Html, CSS, Statistics [Hypothesis testing], AutoML, Postman, Rest API, Chatbot [google dialog flow], NLP, Jira, Dockers, kubernetes
J2EE (JDBC, EJB), XML, JSP, Servlet, Java Script

Professional Experience
State Auto Insurance, Columbus OH June 2019 – Till Date
Data Scientist/Machine Learning
Responsibilities

Working on developing AutoML application and built a classification methodology to determine whether a customer is placing a fraudulent insurance claim.
Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modeling and data visualization with large data sets of structured and unstructured data.
Design and implement Machine learning models and data ingestion pipelines.
Experience with Continuous Data Science (CI/CD) tools
Practiced in exploratory data analysis (EDA) and manipulating large data sets
Collecting the data from different sources includes MS SQL Server, flat files-excel, csv, txt etc
Good understanding of MLOPS
Wrote python scripts to scrape web data for data usage/collection using Beautiful SOUP, and Scrapy.
Using GitHub repository to clone the code and commit the changes and push it to the develop branch from feature branch.
Performed Model Validation and Model Tuning with Model selection, K-ford cross-validation, Hold-Out Scheme and Hyperparameter tuning by Grid search to find the optimal hyperparameters for the model.
Perform data Preprocess, including resolving data quality issues, data transformation
Adhere to agile project management frameworks
Used PANDAS, NUMPY, SEABORN, MATPLOTLIB, SCIKIT-LEARN, SCIPY, NLTK, TensorFlow, Keras, PyTorch in Python for developing various machine learning algorithms.
Dockerization of model and deployment
Creating machine learning pipelines using big data technologies like pyspark etc.
Experience in data extraction, ingestion and processing of large data sets
Performed data analysis by using SQL to retrieve data from Oracle database
Utilized Spark SQL API in Pyspark to extract and load data and perform SQL queries.
Deployed end to end Machine learning applications in AWS EC2 instance

Environment: Power BI, AWS, JavaScript, HDFS, CSS, Python, Hive, Machine Learning Algorithms, NLP, Spark, A/B testing, Html, PyCharm, Flask, API, SQL, MongoDB, GitHub
AT&T, Atlanta GA Jan 2017- June 2019
Data Scientist
Responsibilities
Built an ML model to predict fault wafer for classification problem
Performed data validation, data insertion into Database using Mongo DB and processed files in batch coming from client as per requirement.
Dividing data into subsets of data for better model training using K-Means clustering algorithm
Worked on developing production ready models using Random forest, K means clustering
Used Pandas, NumPy, seaborn, SciPy, Matplotlib, Scikit-learnin Python for developing various machine learning algorithms and utilized machine learning algorithms such as XgBoostand Random Forest.
Creating machine learning pipelines using big data technologies like pyspark etc
Familiarity with language models (SpaCy, NLTK) and using them to operationalize and enhance chatbot user experience.
Used over sampling technique from python SMOTE library to up sample low occurrence target values to be learned more by the model.
Developed train and test data by train test split technique from scikit learn library and by using stratifying technique to split data into same proportion of high and low occurrence target values.
Used k-fold cross validation(scikit learn library) on train data to minimize variance and predict better results on test data.
Explored grid search cross validation(scikit learn library) to tune hyper parameters to find best parameters. Tested various performance metrics like accuracy,F1-score,Precision and Recall and used Recall to calculate the performance.
Used the Google colab environment for running python codes/jupyter notebooks.

Environment: Python(pandas, Machine Learning,numpy, scikit learn, matplotlib, seaborn, Pysql), R studio, Jupyter Notebooks, Google Colab, Excel,Django
Aprimo, Chicago IL March 2015 – Dec 2016
Data Scientist

Responsibilities
Tracked the sales of CPG in multiple categories to identify key drivers of the revenue generated on a weekly basis
Worked on end-to-end data pipeline solution: acquisition, extraction, transformation, loading and visualization of data.
Built machine learning (ML), artificial intelligence (AI) applications and deployed across huge data sets.
Working on NLP based project on analyzing tweets, review and feedbacks of brand. Working on leveraging the brand reputation by understanding customer’s emotions, requirements and needs in Python. Applying Topic modelling, data extracting, data pre-processing includes stemming, stop-word etc. handling outliers, missing values etc
Using AWS S3 for data storage
Extracted terabytes of structured and unstructureds data by using SQL queries and performed data mining tasks including handling missing data, data wrangling, feature scaling, outlier analysis in python by importing pandas.
Utilized data visualization tools such as Tableau and Python’s vast data visualization libraries to communicate findings to the data science, marketing and engineering teams.
Defining requirements, coding, and delivering new functionality to be rolled out to clients.
Defining requirements for and/or coding new internal analytical capabilities (eg. Balance and Profit and Loss monitoring)
Building relationships with clients, the global Prime team, internal business management, and support partners.
Solving issues for clients and ensuring the client experience is balanced with risk and other financial attribute allocation.
Building new statistical models and Machine Learning algorithms that drive Marketing Customer Experience, Credit Valuations, and Operations programs
Leading data science projects from concept to deployment for delivering substantial business value through the application of new-age AI-driven methodologies.
Utilizing Machine Learning algorithms (k-Means, Random Forest, Gradient Boosting, etc.) and using programming languages such as Python

Environment - Python, scipy, Machine Learning,Pandas, scikit-learn, matplotlib, k-Means, Random Forest, Gradient Boosting, SQL, Tableau

NTT DATA, India Feb 2013- Jan 2015
Software Engineer

Responsibilities
Collaborated with data engineers and operation team to implement ETL process, wrote and optimized SQL queries to perform data extraction to fit the analytical requirements.
Supervised data collection and reporting. Ensured relevant data is collected at designated stages, entered into appropriate database(s) and reported appropriately.
Extensively worked on Data Acquisition and Data Integration of the source data from Data stage to Talend.
Performed Data analysis on list of tables that are in-scope of the project and determined the source and target at table/column level for the Data Mapping using SQL SSIS.
Dealt with descriptive statistical analysis like customer profiling, content classifications and categorical data analysis from databases.
Utilized SQL testing and scripting in Databases to integrate data for data analysis, development of data visualization for presentation.
Solving Production Issues and Bug fixing.
Good understanding of OOPs concept in Core java
Involved in the developing of PL/SQL script to fix production issues
Worked on the coding of Servlets and EJB communication
Worked on Maven for getting the latest jar files including common-collection.jar, common-logging.jar, etc from Apache.
Worked on analyzing code to fix UI issues like browser incompatibility, sound knowledge on debugging JSP pages using IE developer tool
Used IBM RSA as an IDE for application development.
Developed code for obtaining bean references in the Spring IOC framework.
Used defect tracker to track all the QA and Production issues
Handled production support of the application

Environment: Windows XP, Java 5.0, Eclipse 3.3, Tomcat 5.5, JSP, Servlets, Oracle PL SQL, Jira, caliber, WebLogic, unit, HTML, JDBC

Education:
Bachelor of Engineering [Electronics], VTU,2012

Certifications:
Oracle Certified Professional, Java SE 6 Programmer [Mar 2013]
ML Masters Certification, INeuron.ai

"
,
"Data Scientist, Data Analyst. SQL, R, Python, Power BI","
Xinqi Liu
Email: adm6fg@r.postjobfree.com Phone: 513-***-**** LinkedIn: linkedin.com/in/xinqi-liu-b362b4156/ Objective
Experienced data analyst with 2 years of work experience in data analysis and business operation. Skilled in SQL, R, Python and data visualization tools. Seeking for data analyst and data scientist positions. Skills
Software: SQL (3 yrs), R (4 yrs), Python (1 yr), Power BI (2 yrs), Excel (PivotTable) (2 yrs), Power Point (3 yrs) Data Management: A/B Testing, Data Visualization, Machine Learning, Time Series, Experimental Design Work Experience
Data Analyst Oct 2019 – Present
Microsoft Commerce Team via Pactera Technologies, Inc. Redmond, WA
Analyzed millions of payment data for Microsoft products by SQL.
Developed A/B test experiments of dunning payment data for consumer products (Xbox, Office 365), the treatment group increased half year revenue by $1M in comparison. Generated R markdown template for similar and repeated experiments, saved the reporting and visualization time by 70%.
Monitored and Analyzed the payment anomaly incidents for consumer and commercial KPIs, provided the timely and sufficient analysis and operation suggestions, supported monthly business reviews.
Created Azure non-payment Power BI dashboard to keep in track with uncollected payment and to distinguish fraud and non-fraud actions, mainly focused on large market and top issuer groups.
Developed and maintained the R Shiny App of cost of payment model to explain the CoP MoM changes.
Generated CSV ad hoc reports of redemption and spend data for supporting business operations .
Maintained services and functions of SQL Server and Power BI Service for the entire team (present admin). Microsoft Azure Build Team via Pactera Technologies, Inc. Redmond, WA
Imported data from Microsoft Azure and OData to Power BI. Used DAX to create columns and measures. Generated customized and real-time data visualization reports, helped the team to monitor the bug status.
Created reports for daily, sprint and monthly time stages, provided statistical insights for further decisions.
Explored Kusto database to extract valuable information (incident managements, user records). Project
Machine Learning Project: TED talk comments prediction May 2019 Miami University Oxford, OH
Web scraped, loaded and cleaned the raw data of TED talk. Created new variables of sentiment scores.
Performed a Leave-One-Out Cross Validation study on several models (dimension reduction, tree-based models, k-nearest neighbors), predicted TED talk comments for testing dataset using the feature variables.
Summarized the results of prediction and determined the best predictive models (RMSE), selected important variables for boosted tree model by VIMP. Presented proper report by R. Education
Miami University, Oxford, Ohio Aug 2017- Oct 2019
Master in Statistics GPA: 3.72/4.0
China University of Petroleum, Shandong, China Aug 2013 - Jul 2017 B.S. in Chemical Engineering and Technology GPA: 3.2/4.0

"
,
Machine Learning Engineer - Data Scientist,"
PROFILE SUMMARY
• Senior data scientist with over 15 years of IT experience, and 11 years of Data Science experience.
• Developing Python solutions for over 9 years with experience in Matlab, R, Java, JavaScript, SQL and C/C++.
• Experience in the application of machine learning techniques including Naïve Bayes, Linear and Logistic Regression Analysis, Neural Networks, RNN, CNN, Transfer learning, Time-Series analysis, trees, and Random Forests.
• Experience in statistical models on, big data sets using cloud/cluster computing assets with AWS and Azure.
• Strong ability to devise and propose creative and innovative ways to look at problems by using business acumen, data models, statistical analysis, and a practical and direct understanding of the subject matter.
• Highly capable of discovering patterns in data using both algorithms, visual representation, and intuition. Ability to use experimental and iterative approaches to validate findings.
• Advanced statistical and predictive modeling techniques to build, maintain, and improve on real-time decision systems. Recommendations are strengthened with precise analysis, a real-world intuition, and an adogmatic approach to modeling techniques.
• In-depth knowledge of statistical procedures that are applied in both Supervised and Unsupervised machine learning problems.
• Ability to perform exploratory analysis on varying types of data and datasets, allowing for a full knowledge of the subject matter, a nuanced understanding of the variables in question, and a technically sound insight into the required modeling approach.
• Strong background of working with advanced analytical teams to design, build, validate and refresh data models.
• Excellent communication skills (verbal and written) to communicate with clients/stakeholders and team members.
• Highly perceptive with other people, allowing for a strong ability to facilitate a dynamic and constructive team environment.
• Ability to quickly gain a keen understanding of niche subject matter domains via research and communication with all parties involved. Ability to design and implement effective novel solutions to be used by other subject matter experts.

TECHNICAL SKILLS
Analytic Development: Python, R-Programing, SQL, Excel

Python Packages: Numpy, Pandas, scikit-learn, TensorFlow, PyTorch, SciPy, Matplotlib, Seaborn

IDE: Jupyter, Spyder, RStudio, Google Colab, MySQL, Oracle

Version Control: Git, GitHub, Jira

Machine Learning: Natural Language Processing & Understanding, Machine Learning algorithms including text recognition, image classification, and forecasting

Data Query: Azure, Google, SQL, data warehouse, data lake and various SQL databases and data warehouses

Deep Learning: Machine Perception, Data Mining, Machine Learning algorithms, Neural Networks, RNN, CNN, Transfer learning, TensorFlow, Keras. PyTorch

Artificial Intelligence: Text Understanding, Classification, Pattern Recognition, Recommendation Systems, Targeting Systems, Ranking Systems, and Time Series

Analysis Methods: Advanced Data Modeling, Statistical, Exploratory, Bayesian Analysis, Inference, Regression Analysis, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Sentiment Analysis, Predictive Analytics, Decision Analytics, Design and Analysis of Experiments, Factorial Design and Response Surface Methodologies, Optimization, and State-Space Analysis

Analysis Techniques: Classification and Regression Trees (CART), Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, RNN including LSTM, CNN, Transfer learning, Linear and Logistic Regression, Naïve Bayes, Simplex, Markov Models, and Jackson Networks

Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series Analysis

Applied Data Science: Natural Language Processing, Machine Learning, Text Recognition, Image Classification, Social Analytics, Predictive Maintenance

Soft Skills: Excellent communication and presentation skills; ability to work well with stakeholders to discern needs accurately and articulate issues clearly; leadership; mentoring; coaching.

PROFESSIONAL WORK EXPERIENCE

Sept 2019 – Present
Location: Chicago, IL
Position: Senior Data Scientist
Company: McDonald’s
Project Summary:
McDonald’s is an American fast-food company. Implemented machine learning algorithm (ANN) build a time series predictive model to forecast demand through data analytics. Worked with the operational research team to optimize warehouse inventory and layout by predicting the time for certain work to be done. Our data model improved supply chain team working efficiency and reduced overage cost of raw materials.
· Explored time series machine learning model like Vector ARIMA, SRIMA, Facebook Prophet for time series data analysis.
· Used Pandas, NumPy, seaborn, SciPy, Matplotlib, Scikit-learn, in Python for developing various machine learning algorithms and utilized machine learning algorithms such as linear regression, multivariate regression, Naive Bayes, Random Forests, K-means, and KNN for data analysis.
· Implementation of machine learning algorithms and concepts such as: K-means Clustering (varieties), Gaussian distribution, decision tree etc.
· Analyzed data using data visualization tools and feature extraction tools and supervised machine learning techniques to achieve project objectives.
· Analyzed large data sets and apply machine learning techniques and develop predictive models, statistical models.
· Supervised, Unsupervised, Semi-Supervised classification and clustering of warehouse inventory in analysis.
· Machine learning classification of documents - Neural Network and Deep Learning, K-neighbors, K-means, Random Forest, Logistic Regression, SVM.
· Created an analysis model to optimize resource allocation and warehouse layout.
· Documented logical data models, semantic data models and physical data models.
· Performed complex modeling, simulation and analysis of data and processes.
· Analysis through different regression and ensemble models in machine learning to perform forecasting.
· Analysis employed key indicators in Python and machine learning concepts like regression, Bootstrap Aggregation and boosting.

Nov 2018 – Aug 2019
Location: Wichita, KS
Position: Lead Data Scientist
Company: Cargill
Experience Summary:
Cargill is an American privately held international food conglomerate, major businesses are trading, purchasing and distributing grain and other agricultural commodities. Our team used CNN with computer vision build the machine learning model to detect unhealthy hydrophytes. Our model helped regulator work more efficient by detecting unhealthy hydrophytes in hydroponic farming automatically and increased their harvesting rate which increased their revenue.
· Performed statistical analysis and built statistical models in R and Python using various supervised and unsupervised Machine Learning algorithms like Regression, Decision Trees, Random Forests, Support Vector Machines, K- Means Clustering and dimensionality reduction.
· Used MLlib, Spark's Machine learning library to build and evaluate different models.
· Define the list codes and code conversions between the source systems and the data mart enterprise metadata library with any changes or updates.
· Developed Ridge regression model to predict energy consumption of customers. Evaluated model using Mean Absolute Percent Error (MAPE).
· Developing and enhancing statistical models by leveraging best-in-class modeling techniques.
· Developed a predictive model and validated Neural Network Classification model for predicting the feature label.
· Implemented logistic regression to model customer default and identified factors that are good predictors.
· Designed a model to predict if a customer will respond to marketing campaign based on customer information.
· Developed Random forest and logistic regression models to observe this classification. Fine-tuned models to obtain more recall than accuracy. Tradeoff between False Positives and False Negatives.

Feb 2015 - Oct 2018
Location: Charlottesville, Va
Position: Data Scientist
Company: Bio Vista
Experience Summary:
BioVista Inc. is a private drug development services company. The company repurposes drugs and calculates risks of drugs before trials. This is done using multidimensional characterizations of pharmacologically relevant features such as genes, diseases, drugs, pathways and cell types, to identify and quantify potential risks and discover new indications for drugs in development or already in the market. We utilized literature mining methodologies to perform Natural Language Processing. Information extraction and named entity recognition are used to link concepts and arguments from different articles and infer implicit knowledge from seemingly unrelated concepts to discover new potential uses for existing drugs. More simply put, we used machine learning, natural language processing and graph databases to perform Graphical representation of Swanson’s ABC model for drug repurposing.
· Became proficient in Natural Language Processing, SQL queries and web scrapping for collecting literature using BeautifulSoup
· Familiarized myself with various drug and disease knowledge databases such as DrugBank, ChemBank, OMIM, KEGG, and Pubmed in addition to genomic databases, such as MIPS, PDB, GEO, and GenBank.
· Utilized graph clustering algorithms, such as ClusterONE (Clustering with Overlapping Neighborhood Expansion) and MbiRW (measure and Bi-Random walk) which is a k-means-based network cluster algorithm.
· Worked with large data sets by creating integration with Spark with Sparklyr.
· Utilized Natural Language Processing (NLP) techniques, such as term frequency-inverse document frequency (TF-IDF) to measure the importance of terms within the literature when constructing the document network for use with above mentioned graphical clustering algorithms.
· Used machine learning with Theano/Keras to further perform risk assessment of the potential adverse effects for drug-drug interaction.
· Collaborated with molecular simulation experts to combine literature-based discovery with computer simulations for the purpose of predicting the risk of certain drugs.
· Utilized multiple cross validation frameworks (k-folds and train-validate-test, where appropriate) to optimize, and ensure generalizability for drug risk assessment.

Oct 2013 - Feb 2015
Location: Muncie, Indiana
Position: Staff Applied Mathematician
Company: Lincoln National Corporation
Experience Summary:
I worked with an actuary analyzing, classifying and looking for patterns in dataframes. This included working with finance and risk analysis and using statistical methods to predict liability. Worked under lead Actuary John Beekman and implemented various types of techniques.
· Tested survival analysis technique using various method: Accelerated Failure Time model, proportional Hazard model and Cox Proportional Hazard (CPH) to estimate the default probability and default time and chose the best performing model.
· Created lifetime loss, prepayment and recovery forecasts for credit due diligence.
· Development of credit risk underwriting models to support underwriting of various card.
· Applied linear regression in Python and R to understand the relationship between different attributes of dataset and causal relationship between them.
· Set up the model risk management framework covering both operational and Base models.
· Delivered portfolio risk dashboard as a package covering all aspects of the credit life cycle.
· Delivered machine learning based models for predicting signs/features of borrowers likely to default to challenge a conventional statistical model.
· Information used included structured and semi-structured data elements collected from both internal and external sources.
· Creation of a target operating model that balances the modelling rigor and product specific priorities and nuances.

June 2010-Sept 2013
Location: Indianapolis, Indiana
Company: CarpetLand USA
Position: Business Intelligence Specialist
Experience Summary:
During my time at Carpetland USA, I was tasked with predicting carpet sales based on prior sales data and seasonality trends. In this job, I had to construct a multi-variate linear regression algorithm to predict the price of several types of carpet including berber, frieze and outdoor to construct an accurate inventory; this, in turn, helped increase profit by 30%.
· Developed processes and tools to monitor and analyze performance and data accuracy.
· Enhanced data collection procedures to include relevant information to build and continuously optimize the analytics systems.
· Collaborated with I.T. to continuously optimize business performance.
· Processed, cleansed and verified the integrity of data from various sources used for analysis and reporting.
· Leveraged the latest data visualization tools and techniques to present and communicate analysis to the leadership team utilizing data management, analytics modeling, and business analysis.
· Used predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.
· Advised leadership team and stakeholders with data-driven solutions and recommended strategies that address business challenges.

Feb 2008 - May 2010
Location: Indianapolis, Indiana
Company: J.S. Hein Masonry
Position: Data Analyst
Experience Summary:
During my time at J.S. Hein Masonry, I developed a classification algorithm using multivariate logistic regression to determine which Indianapolis neighborhoods exhibited the most growth based on housing price, crime statistics, voter and population demographics as well as internal neighborhood growth; included business development and previous development trends. I used this data as a guide for management to determine which building contractors would offer the best investment for J.S. Hein. in terms of job security and overall profit. As a result, we put in bids for five different contracts which increased our yearly profit by 10%.
· Used unsupervised learning techniques such as K-means clustering and Gaussian Mixture Models to cluster customers into different risk groups based on health parameters provided through wearable technology regarding their activities and health goals
· Multiple statistical modeling approaches were applied to determining the usefulness of the wearable technology data for various insurance products.
· Survival modeling techniques, such as Poisson regression, hidden Markov models, and Cox proportional hazards, were used to model time to different events utilizing wearable data (time to death for life insurance, time to next hospital visit, time to next accident, time to critical illness, etc.).
· Data required extensive cleaning and preparation for machine learning modeling, as some observations were censored without any clear notification.
· We solved a binary classification problem transferring to lower risk group or not with given financial incentive) with a logistic regression.
· An artificial neural network was utilized with Keras/TensorFlow in python to solve binary classification problem for premiums and their intersection with the discriminant.
· We used modifiers, including L1 regularization, dropout, and Nesterov momentum to enhance the neural network and optimize generalization
· Provided reporting dashboard for stakeholders and project owners to rapidly provide information via Plotly, Bokeh, Matplotlib, and Seaborn.
· Retrieved data from devices, which was streamed to our company database using AWS-kinesis (real-time data streaming).
·
Education
BA in Mathematics, Ball State University
PhD in Differential Topology, Wayne State

"
,
Mustafa Alam - DATA SCIENTIST,"
Mustafa Alam

214-***-****

adm45z@r.postjobfree.com

Summary
12+ Years Data Science Experience

Professional in the field of Data Science with 12+ years of experience in statistical analysis, data analytics, data modeling, and creation of custom algorithms. Application to the disciplines of machine learning and neural networks using a variety of systems and methods in training algorithms with different could platform. Industry experience includes predictive analytics in finance, marketing, advertising, geospatial and Internet of Things (IoT). Use of NLP and Computer Vision technologies.
Skills:
• Experience with a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction
• Familiarity with developing, deploying, and maintaining production NLP models with scalability in mind
• Worked on Natural Language Processing with NLTK, SpaCy and other module for application development for automated customer response
• Wrote automation processes using Python and the AWS Lambda service
• Utilized Docker to handle deployment on heterogeneous platforms such as Linux, Windows, OSX, and AWS
• Reviewed the use of MongoDB, node.js, and Hadoop to automate the data ingestion and initial analysis processes
• Reviewed and deployed the infrastructure on AWS to minimize cost while providing the required functionality
• Scale analytics solutions to Big Data with Hadoop, Spark/PySpark, and other Big Data tools
• Experience with Public Cloud (Google Cloud, Amazon AWS and/or Microsoft Azure)
• Experience working with big data infrastructure with tools such as Hive, Spark and h2o, sparkling water
• Implementing solutions with common NLP frameworks and libraries in Python (NLTK, spaCy, gensim) or Java (Stanford CoreNLP, NLP4J)
• Experience with knowledge databases and language ontologies
• Quantitative training in probability, statistics and machine learning
• Experience in the application of Neural Network, Support Vector Machines (SVM), and Random Forest.
• Creative thinking and propose innovative ways to look at problems by using data mining approaches on the set of information available.
• Identifies/creates the appropriate algorithm to discover patterns, validate their findings using an experimental and iterative approach.

Technical Skills
Programming
Python, Spark, SQL, R, Git, MATLAB, bash Libraries
NumPy, Pandas, Scipy, Scikit-Learn, Tensorflow, Keras, PyTorch, statsmodels, Prophet, lifelines, PyFlux, arch, FeatureTools, Lime
Version Control
GitHub, Git, BitBucket, Box, Quip IDE
Pycharm, Sublime, Atom, Jupyter Notebook, Spyder
Data Stores
Large Data Stores, both SQL and noSQL, data warehouse, data lake, Hadoop HDFS, S3 RDBMS
SQL, MySQL, PL/SQL, T-SQL, PostgreSQL
NoSQL
Amazon Redshift, Amazon Web Services (AWS), Cassandra, MongoDB, MariaDB Computer Vision
Convolutional Neural Network (CNN), Faster R-CNN, YOLO
Big Data Ecosystems
Hadoop (HBase, Hive, Pig, RHadoop, Spark, HDFS), Elastic Search, Cloudera Impala. Cloud Data Systems
AWS (RDS, S3, EC2, Lambda), Azure, GCP
Data Visualization
Matplotlib, Seaborn, rasterio, Plotly, Bokeh NLP
NLTK, Spacy, Gensim, Bert, Elmo
Machine Learning
Supervised and unsupervised Learning algorithms
Machine Learning, Natural Language Processing,, Deep Learning, Data Mining, Neural Networks,
Linear Regression, Lasso and Ridge, Logistic Regression, Ensemble Classifiers (Bagging, Boosting and Voting), Ensemble Regressors, KNN,
Naïve Bayes Classifier, Clustering (K-MEANS, GMMs, DBSCAN), PCA, SVD, ARIMA. Analytical Methods
Advanced Data Modeling, Regression Analysis, Predictive Analytics, Statistical Analysis (ANOVA, correlation analysis, t-tests and z-tests, descriptive statistics), Sentiment Analysis, Exploratory Data Analysis. Time Series analysis (ARIMA) and forecasting (TBATS, LSTM, ARCH, GARCH), Principal Component Analysis (PCA) and SVD; Linear and Logistic Regression, Decision Trees and Random Forest.

Professional Experience
AEP Texas in Dallas, Texas
October 2019 – Present
Senior Machine Learning Scientist
AEP Texas is a subsidiary of American Electric Power, based in Columbus Ohio. Lead a small team of data scientists and data engineers where we created numerous demand forecasting models from AEP Texas historical data hosted on Hadoop HDFS and Hive, to estimate short-term demand peaks for optimizing economic load dispatch. The project involved prediction of demand for electricity within the market area at 2 hour to 2 week outlooks. Multiple algorithms were employed explored and implemented.

• Endeavored multiple approaches for predicting day ahead energy demand with Python, including exponential smoothing, ARIMA, Prophet, TBATS, and RNNs (LSTM).
• Successfully built a Generalized Autoregressive Conditional Heteroskedasticity (GARCH) using PyFlux to model the uncertainty of other time series, ensuring a ‘safety’ stock of generating units.
• Incorporated geographical and socio-economic data scraped from outside resources to improve accuracy.
• Incessantly validated models using a train-validate-test split to ensure forecasting was sufficient to elevate optimal output of the number of generation facilities to meet system load.
• Prevented over-fitting with the use of a validation set while training.
• Built a meta-model to ensemble the predictions of several different models.
• Performed feature engineering with the use of NumPy, Pandas, and FeatureTools to engineer time-series features.
• Coordinated with facility engineers to understand the problem and ensure our predictions were beneficial.
• Participated in daily standups working under an Agile KanBan environment.
• Queried Hive by utilizing Spark through the use of Python’s PySpark Library.

JC Penny in Plano, Texas
June 2017 – September 2019
Senior Data Scientist
JC Penny is the largest U.S. based retail department store company. Along with its physical stores, JC Penny also has a successful online store which caters to all of customers fashion and home needs through their online store macys.com and their mobile app. As a Data Science consultant for JC Penny online store division I lead a team to optimize a recommendation engine using Mixed Hybrid Recommender system. The revenue impact from deployment of the new optimized Recommender system on their website and mobile apps was expected to be more than 9% increase in online sales.
The recommendation engine was reinforced with advanced NLP techniques using Tensorflow and Keras based Transformer models.

• Used techniques like collaborative filtering, content-based, demographic recommender system for creating the Hybrid Mixed recommender system.
• Used A/B testing to test the effectiveness of different types of recommender system and optimized the most effective recommender system after careful tests and research.
• Partially solved the “cold start” problem of recommender system by incorporating the Demographic based recommender system in the final Hybrid mixed recommender system. Worked on data preprocessing and cleaning the data utilizing Pandas, NumPy, and performing feature engineering and data imputation techniques for missing values in the dataset using Python.
• Performed stemming and lemmatization of text to remove superfluous components and make the resulting corpus as small as possible while containing all important information.
• Experience with Keras and TensorFlow in developing predictive algorithms.
• Solved analytical problems, and effectively communicated methodologies and results.
• Concept space embedding via ELMo was also tested and found to have similar results to bag of words with significant increase in computational time.
• Constructed an NLP-based filter utilizing embedding and LSTM layers in Tensorflow and Keras.

Micron in Boise, Idaho
November 2014 – January 2017
Data Scientist
Micron is a well-integrated semiconductor-based company with foundries and fabs all over the World. Micron specialize in planar based multi-stack systems. Their Austin facility hosts automotive, memory and institutional research. Their principal problem is the detection and forecasting of Angstrom-scale device failure. In order to solve this problem several solutions were implemented. First a combination of logistic regression and decision trees were used to classify failure based on various parameters during the production process. Finally, a machine vision stage was set up to detect physical visible error, using a convolutional neural network to verify production stages and aid in feature engineering for regression stages.
• Developed a predictive model and validate Neural Network Classification model to facilitate prediction algorithms.
• Improved efficiency of the model by boosting method on prediction model to improve efficiency.
• Used Convolutional Neural Networks and Machine Vision to detect and predict flaws in stereo lithography nano machined stacks.
• Used R and Python for programming for improvement of model and explored regression and ensemble models in machine learning to perform prediction.
• Developed a predictive model and validated Neural Network Classification model to predict the feature label.
• Developed machine learning algorithms utilizing Caffe, TensorFlow, Scala, Spark, MLLib, R SciPy, MatPlotLib, NLTK, Python, SciKit-Learn, etc.
• Performed statistical analysis and built statistical models in R and Python using various supervised and unsupervised Machine Learning algorithms like Regression, Decision Trees, Random Forests, Support Vector Machines, K- Means Clustering and dimensionality reduction.
• Used MLlib, Spark's Machine learning library to build and evaluate different models.
• Transformed logical data model to physical using ERwin ensuring the primary key - foreign key relationships, consistency of definitions of data attributes and indexes.
• Designed the Data Marts in dimensional data modeling using star and snowflake schemas.
• Used Erwin for effective model management of sharing, dividing and reusing model information and design for productivity improvement.
• Worked with project team representatives to ensure that logical and physical ER/Studio data models were developed in line with corporate standards and guidelines.
• Define the list codes and code conversions between the source systems and the data mart enterprise metadata library with any changes or updates.
• Developing and enhancing statistical models by leveraging best-in-class modeling techniques.
• Developed a predictive model and validated Neural Network Classification model for predicting the feature label.

Petso Financial Consultants, LLC in Boise, Idaho
January 2010 – October 2014
Data Analyst and Expert System Specialist
Petso Financial is a private asset management firm with over 600,000,000 in assets. As a financial analyst I performed risk management calculations, survival analysis and customer lifetime value predictions.
• Designed a suite of Interactive dashboards, which provided an opportunity to measure performance and allowed executives to adjust business strategies
• Worked on outlier detection with data visualizations using box-plots, feature engineering using Gaussian Mixture Models and K-NN distances built using Pandas, NumPy.
• Analyzed data using data visualization tools and reported key features using statistic tools and supervised machine learning techniques to achieve project objectives.
• Analyzed large data sets and apply machine learning techniques and develop predictive models, statistical models.
• Used Tensor Flow library in GPU environment for training and testing of Deep Neural Networks.
• Used R and Python for Exploratory Data Analysis, A/B testing, ANOVA testing and Hypothesis test to compare and identify the effectiveness.
• Built and analyzed datasets using R, MATLAB and Python.
• Designing and developing various machine learning frameworks using Python, R, and MATLAB.
• Utilized machine learning algorithms such as linear regression, multivariate regression, naive Bayes, Random Forests, K-means, & K-Nearest Neighbor for data analysis.
• Experience with Keras and TensorFlow in developing deep learning based predictive algorithms.
• Implementation of machine learning algorithms and concepts such as: K-means Clustering (varieties), Gaussian mixture distribution, decision tree etc.
• Analyzed large data sets and applied machine learning, and predictive statistical models.
• Provided and created data presentations to executives to guide business decisions.
• Dealt with millions of rows of data using SQL and performed Exploratory Data Analysis.
• Successfully interpreted, analyzed and performed Predictive Modelling using Python with Numpy, Pandas packages.
• Worked with TensorFlow, Caffe2 and Torch.

Education
Ph.D. in Electrical Engineering
University of Texas at Arlington - Arlington, TX

M.S. in Electrical Engineering
University of Idaho - Moscow, ID

B.S. in Electrical & Electronic Engineering
Bangladesh University of Engineering and Technology - Dhaka, BD

"
,
Associate Scientist,"
Bryan N. Ngo
adm415@r.postjobfree.com
OBJECTIVE
Seeking a full-time position as an engineer with interest in applying technical knowledge to complete daily operations EDUCATION
Bachelor of Science in Chemical Engineering, Virginia Tech, Blacksburg, VA, Graduated May 2019 GPA: 3.22/4.00
Related Courses: Business and Marketing for Process Industries, Protein Separation Engineering, Drug Chemistry SKILLS
Software: Aspen Plus, Inventor, Mathematica, MATLAB, MINITAB, MS Office Suite, MS Project, MS Visio Languages: Java, Python, HTML, CSS, JavaScript
Verbal Languages: English (Native), Vietnamese (Proficient), Spanish (Basic) CERTIFICATIONS
AICHE: Chemical Process Safety, Chemical Reactivity Hazards, Dust Explosion Control, Explosion Hazards, Fire Hazards, Risk Assessment, Runaway Reactions, Toxicological Hazards RELATED EXPERIENCE
Pharmaceutical Product Development LLC – Richmond, Virginia Associate Scientist, Apr. 2021 – Present; Assistant Scientist, Jul. 2020 – Mar. 2021 Automation Scientist Tasks:
● Performed a variety of routine to complex sample preparation and analysis procedures to quantitatively measure pharmaceutical and biopharmaceutical compounds in a variety of formulations and/or biological matrices
● Responsible for review and compilation of results and data comparison against Standard Operating Procedure
(SOP) acceptance criteria, methodology, protocol and product specifications
● Utilized MS Excel macros and databases to document results and manage shelf-life of human serum samples
● Trained, performed, and taught routine operation, maintenance, start-up, shut-down, and theory of analytical instrumentation
Inventory Team Tasks:
● Recorded weekly counts of inventory and submitted requests for needed equipment and low consumables
● Physically unloaded pallets, lifted +25lbs boxes, and distributed cases to laboratory shelves/cabinets
● Planned, implemented, and sustained a 5S, GLP, and GCP work environment
● Performed routine audit inspections and integrated results to improve laboratory organization
● Collaborated with automation specialists to plan and utilize R to develop a new inventory count system with automated reordering notifications and statistical displays
● Developed an application in Excel VBA to generate weekly inventory of controls
● Constructed laboratory space diagrams on MS Visio to supplement the inventory-taking process Mid-Atlantic Technology, Research, and Innovation Center – South Charleston, West Virginia Project Intern, Feb 2019 – May 2019
● Prepared process flow diagrams for potential flow batteries (FB) on MS Visio
● Sized processing equipment utilized in proposed processes and simulated on Aspen Plus
● Assessed deficiencies and health, safety, and environmental issues for each FB
● Determined capital and operating costs for each FB for several applications and performed Cash Flow Analyses Virginia Tech – Blacksburg, Virginia
Process Design Project for CHE 4185, Oct. 2018 – Dec. 2018
● Designed a solvent recovery system using Aspen Plus; including piping and instrumentation diagrams
● Verified inherently safe process using a Hazard and Operability Study and Failure Mode and Effects Analysis
● Generated a process economic analysis for the next 20 years using MS Excel OTHER EXPERIENCE
Northern Virginia Community College – Manassas, Virginia Website Development Project for ITD120, Jan. 2020 – Mar. 2020
● Planned, outlined, and developed a professional bio website using HTML, CSS, and JavaScript
● Ensured the code qualified to W3C Standards
● Analyzed peer websites and provided feedback for improvements; reconstructed website from compiled feedback Virginia Tech – Blacksburg, Virginia
Undergraduate Research with Dr. D.G. Baird, Jul. 2018 – Dec. 2018
● Created wholly thermoplastic in situ composites using dual-extrusion technology
● Produced compression molds at various temperatures, thermoplastic compositions, and fiber lengths
● Performed tensile testing on universal testing and recorded properties of plaques
● Collected data and reported findings on the flexibility and strength of the plastics Virginia Tech – Blacksburg, Virginia
Polymer Laboratory Assistant with Dr. D.G. Baird, Aug. 2017 – Dec. 2017
● Prepared and formed compression molds of reinforced thermoplastics using a hydraulic press
● Utilized GIMP to correct reflective microscopic images
● Processed images and calculated fiber orientation and length using MATLAB OMG Nails – Fairfax, VA
Receptionist, May 2016 – Aug. 2016, May 2019 – Mar. 2020
● Managed client appointments and served as initial point of contact for customers
● Assisted with preparing client products for treatment ORGANIZATIONS
Filipino American Student Association (+100 active members), Fundraising Chair, Apr. 2016 – May 2017
● Contacted local stores to organize fundraisers via email and over the phone; Raised over 1,500 dollars
● Coordinated a VT football concession stand with a staff of 15 workers
● Planned and conducted a service auction; Raised over 1,000 dollars Atmospheric Teaching Experiment, Sept. 2015 – May 2016
● Managed outreach station and taught primary school children basic scientific concepts
● Collaborated and designed the structure and landing mechanism of a weather balloon on Inventor HONORS
Eagle Scout Award, May 2015

"
,
Data Scientist,"
Jason Rapsinski
765-***-**** adm4v3@r.postjobfree.com Lafayette, IN 47905 https://www.linkedin.com/in/jason-rapsinski-533aaa213/ https://purdue-csm.symplicity.com/profiles/jason.rapsinski3 Intelligent and fast-learning data scientist possessing two bachelor of science degrees from Purdue University. Formal training in coding, artificial intelligence, and electrical systems. Possessing practical experience with computer systems and data analysis. Excited to continue gaining new knowledge and looking for career development experiences.
SKILLS
• 5 years of experience with C, C++, and
Java
• 2 years of experience with Python,
Rstudio, SQL
• Machine learning, data mining, and
artificial intelligence experience
• Software design and development • Technical documentation • Statistical Analysis
• Trouble Shooting Experience • Interdepartmental communication EDUCATION
Bachelor of Science, Computer Science
Purdue University College of Science, West Lafayette Gained practical experience with computer systems, programming in a variety of languages (C, C++, Java, R, SQL, Python), data analysis, machine learning, and artificial intelligence.
Dean's list for 3 semesters
Bachelor of Science, Electrical Engineering Technology Purdue University Polytechnic Institute, West Lafayette Gained practical experience with digital and electrical systems, project management, and technical writing skills. Dean's list for 3 semesters
EXPERIENCE
Software Intern
Landis+Gyr, 2019 - 2019
• Designed script in C# to clean and standardize XML files, was able to operate on more than 4000 files in less than 2 minutes. o Investigated XML file standardization requirements and found no previous solutions existed, began self-directed programming analysis and planning.
o Completed functional programming and technical documentation 1.5 months before deadline. o Familiarized myself with Github push and pull requests, including managing over 3000 files. o Demonstrated effective operation of the script to 4 members of the software team.
• Communicated with 4 different departments to keep up to date with project requirements.
• Presented accomplishments at the end of the summer to over 30 employees, including a basic explanation of the code such that attendees in non- technical positions were able to understand.
• Co-ordinated with between 5 and 8 software designers through monthly and daily scrum meetings, respectively. Summer Lab Intern
Landis+Gyr, 2018 - 2018
• Enabled the lab team to succeed through an influx of 30 meter type testing requests by learning and logging testing standards.
• Verified operational functionality on 30 meter types and over 200 total meters in extreme conditions via QA tests. o Performed meter tests in high power environments exceeding 2000 volts at 120 amps. o Managed Excel files on each of the 200 tested meters in addition to meta-analysis files on each meter type. o Verified operational quality of meters under extreme conditions including simulated heavy weather lasting over 30 days.
• Presented results to 5 other interns and managers, including a basic explanation of tests and meter types. Maintenance Intern
Purdue University, 2016 - 2016
• Created a safe and functional living space for 3 summer events at the building through maintenance and repair work.
• Verified safety of living conditions, including operational verification of over 150 fire alarms and repairs on over 300 screens.
• Refined practical work experience by returning in 2017, reverifying fire alarm operation and repairing an additional 100 screens.

"
,
Nuclear Reactor Electrical Power,"
DAVID L. FUSON
adm1p6@r.postjobfree.com
Houston, Texas 77089
Cell 281-***-****

Software / Firmware Engineer
Results-Driven Bilingual Professional with extensive experience in analysis, design, implementation, development, and testing. Managed five to eight employees per project as a strong team leader. Organized and able to communicate with a diverse group of people and individuals in a fast-paced environment.

Awards or Major Successes
• Updated the software on Oceaneering embedded EMAT Underwater ROBOT. • Helped to train Astronauts to fly the Space Shuttle. • Co-authored a cost reduction saving, totaling $800,000 to NASA. • Additionally, recognized for the “Cost Reduction Award,” saving $225,000 in one year. • Experienced user of the embedded RMS and CCTV systems for the space shuttle. • I convinced NASA to spend $200,000,000 to install the embedded Multi-Function Equipment Display Systems (MEDS) into the Space Shuttle Fleet. • Hand-picked by NASA management at the Johnson Space Center (JSC) to be the sole person that verified the functionality of a flight-critical LabVIEW program that was to be used to help prevent any other Space Shuttle from disintegrating during reentry like the Space Shuttle “Columbia” did on Feb 1, 2003. • Received an award for re-sequencing the Iridium Satellite’s GNC system.
• In my spare time, I had mentored Rice Undergrads to two first-place positions in the same year as they developed their Martian UAV flyer.
• Boeing’s resident expert on all systems in the Space Shuttle for the $1.6 billion upgrades. Asked by Boeing Corporate to recommend whether to weld or clamp the joints on the International Space Station.

TECHNICAL SKILLS
• DevOps Engineering, • Real-Time Software Coding • Software Configuration Control,
• Validation & Verification of software and hardware, • Software Testing, • VMware ESXi System Creation and Development, • Hardware Interface testing, • Software Development, • Object-Oriented Design(OOD), • DOORS requirements creation and matching. • Trained by NASA to code according to the Health Insurance Portability and Accountability Act of 1996 (HIPAA) Privacy, Security and Breach Notification Rules, for Astronauts at the International Space Station (ISS). • Cyber Security proficient, trained at the University of Houston in the summer of 2015. • QT UI software development. • UAV software development for the Instructor’s training console for USAF Ground Launched Cruise Missiles (GLCM). • Mentored Rice Undergraduate Students in their senior class project “Martian UAV flyer”. Worked on Lockheed Aquila Drone “UAV” configuration software system.

CYBER SKILLS
• Received an introduction to AES 256 encryption technology. Familiarized with the usage of the following cyber tools: ping, traceroute, ifconfig, nslookup, whois, Netstat, finger, Port Scanning with Nmap. Experienced with IMPERVA WAF and SOM usage, QRadar, FireEye at the Veterans Administration site in Austin TX.
• One of 1420 people in the United States allowed entering the “1st Hack the Pentagon” contest. No, I did not find problems with the DOD servers, but it was nice to know that the United States government trusted me to look for problems in their systems.

CURRENT EDUCATION
Completed the ""2015 Information Security Bootcamp"" at the University of Houston, which was funded by a grant from the State of Texas. This was a class in Cyber Security that ran for 8 weeks for a total of 320 hours of classroom instruction.

SOFTWARE SKILLS
• Embedded Programming Languages - Ada, Pascal, FORTRAN, LabVIEW, C/C++, C#, Dynamic C, • Embedded SQL 2005/2008, • X11, Other languages • COBOL, • CLIPS, • LISP, • BASIC, • FORTH, • HAL/S, numerous • Assembly Languages. • VMware virtualization tools for servers and singular development platforms, like ESXi, Workstation. • Cyber Security trained at the University of Houston by a grant from the state of Texas.

Note:
I enjoy being a LabVIEW programmer, as I consider my LabVIEW skills to be similar to an artist and all of my code to be a work of art.

Software Tools Used – • LabVIEW, • DOORS, • Atlassian • SCRUM, JIRA, • Agile, • Eclipse, • PLM, • Visual Studio, • VxWorks, • VMware ESXi, • VMware Player, • VMware Fusion, • VMware Workstation, • HyperV, • VHD, • Parallels, • CVS, • Subversion (SVN), • eMatrix. • Video Tools used were • transcoders, • FFMPEG, • Elemental server, • Haivision Calypso server and several Video formats: • MPEG2, • MPEG4, • AVI, • Quicktime, • H.264.

OS PLATFORMS
• Embedded Operating Systems/Platforms or RTOS systems – • Red Hat Enterprise Linux(RHEL), • Linux, • CentOS, • UBUNTU, • QNX. Other O/S used • Windows NT/2000/XP/Vista/7, • Apple/Mac OS 10 series, • UNIX, • SunOS, • Solaris, • VMS, OPENVMS, • VAXELN and • IBM AIX.

HARDWARE PLATFORMS
Hardware Used – • EMAT inspection equipment for Surface and Subsurface operation, • SuperMicro Server, • ARM-9 series processor and custom cards, • Power PC (PPC), • Numerous PCI cards including several • LabVIEW cards, • Graphics cards, • 2-Wire 4-20mA current loop devices, • Analog (ADC, DAC) • DAQ Signal Enhancement for Custom Transducers, Digital I/O (DIO), • UART, Serial (RS232, RS422, RS485), • USB, • PC104 bus. • VME Bus, • National Instrument (• PXI-1010, • PXI-1050), • FieldPoint and • Compact FieldPoint.

PROFESSIONAL EXPERIENCE

Leonardo DRS Huntsville, AL 01/2017 to 04/2017
Principal Software Engineer
Working on embedded software for M1A2 Abrams Main Battle Tanks and Bradly Fighting Vehicles (BFV). Hand translating embedded Ada software to embedded C++due to current college graduates not knowing the Ada software programming language. Work assignments included using embedded MIL-STD-1553 devices, updating embedded firmware with the software package changes, and installing. Team Leader.

Veteran’s Administration “Cyber Security” Austin, TX 07/2016 to 01/2017
Cyber Security Specialist
Instructed the ins and outs of scanning servers using IBM (AppScan, QRadar), FireEye, and IMPERVA Web Application Firewall (WAF) and SecureSphere Operations Manager (SOM) “formally” known as the Database Activity Monitoring (DAM) tool set. Making sure that “Actors were not breaking into, spying on, copying, altering, or destroying U.S. Veteran’s private medical records.
Attended the one-week in-house course on the theory of operation of IMPERVA devices presented to the Veteran’s Administration Cyber Staff in Austin TX.
Loaded new SSL Keys into the WAF, Scanned Databases with the SOM.
Scanned new web-pages with IBM’s AppScan App.

Oceaneering Inc. – Houston, TX 02/2014 to 03/2015
Software Engineer - Video Developer Specialist
Used LabVIEW to program changes into an Electro-Magnetic Acoustical Transmitter (EMAT) that is used to find imperfections, flaws, and cracks in Surface or Underwater pipelines. The software worked from the surface to 10,000 ft. below the ocean on Oceaneering’s new underwater ROBOT. Certified Level II Non Destructive Testing (NDT) Technician for EMAT operations. NAS storage was used for file archiving for all EMAT output data files.
Enhanced embedded EMAT built-in software test tools to increase the accuracy of underwater pipe inspections. Increased the accuracy of all debug software, overall execution speed, and analysis display screens. This was done by enhancing DAQ signal conditioners to new hardware signal receivers.
QT User Interface (UI) software development prototyping for Video Consoles On-Shore to supporting video archiving and review. Encoding/Decoding Metadata to/from video files.
Tested numerous embedded video software & hardware components (Elemental 200 series servers, Haivision Calypso severs) for form and functionality, so that management could use my reports for product selection or rejection. Tools used were transcoders, FFMPEG, Elemental server, Haivision Calypso server, and several Video formats: MPEG2, MPEG4, AVI, Quicktime, H.264. NAS storage was used for inputs and outputs created or used by the Haivision and Elemental servers. Test Tools used were BASH scripts.
VMware systems creation, development, and management using VMware ESXi tools. Worked on a product that was used as a disaster support tool to be used to coordinate support personal in the field for regional disasters or emergencies. Version CentOS 6.5 was used as the base OS for development. NAS storage was used for inputs and outputs for all of the virtualized servers inside the VMware server. Also, tested sending video files to the Amazon Web Services (AWS) cloud storage devices for storage and retrieval.
Assembled a SuperMicro 1U blade server with 128 GB memory, 4*1TB drives, 2*XEON quad-core processors each with quad threads. Giving a total of 32 virtual processors, each executing at 2.5ghz. This SuperMicroServer system contained 5 Linux CentOS 6.5 embedded systems (Apache HTTP Server, GIS Server, SQL server, Telephonic server, and “A special customer Server package”), 2 Ubuntu Software development systems for onsite software changes, 1 embedded Windows 7 Professional system for a “Portable Virtual Desktop” for a standard Windows user: (Office, Visual Studio, IE, …, etc.) This VMware ESXi system had hot-swappable drives using a RAID 5 setup. All documentation on how to H/W assemble this system, install all OSes, installing custom/special software packages were tracked using Scrum framework, JIRA. All documentation and processes were stored away in Oceaneering’s File Archive system.
Also, my duties required me to modify a standard Linux *.ISO image to replace the standard ETHERNET driver with a special ETHERNET driver from the Intel Corp. and rebuild the Linux *.iso as a new image.
Agile was used in all testing, installation, data collection, development, and procedures for Video servers test articles.

GE Hitachi I&C Nuclear – Wilmington, NC 11/2012 to 10/2013
Nuclear Professional
Coded to the Nuclear Regulatory Commission (NRC) software standards C.F.R 10 CFR 73.54 for software installed and used in nuclear reactor computers.
Developing test procedures, test cases, and dry runs in multiple embedded communicating environments. Used embedded LabVIEW programming of test tools for safety-critical devices used to control Nuclear Reactors. These efforts were used in a verification/validation effort for a series of reactor control devices. Five software applications were written to verify the correct operation of the Lungmen 1&2 nuclear reactors in Taiwan. One of the LabVIEW apps was a testing tool for the safety-critical High-Pressure Core Flood (HPCF) System. (H/W Features used, Serial RS485, VISA devices, DIO, AIO, 4-20mA sensors.
Testing board-level communication card for software/hardware problem using VxWorks 5.4, using the remote debugging features.
Test and Verified/Validated all software and hardware changes for most of the safety-critical Instruments and Controls (I&C) that are embedded in all of GE Hitachi’s nuclear reactors in the U.S.A. and other countries.
Verified/Validated C/C++ embedded ROM software for GE Nuclear Reactor Support Equipment. GE Nuclear program is in a major update mode for all existing Nuclear Reactor design documents to use IBM DOORS to meet the new NRC regulations.

Deaton Engineering – Georgetown, TX 07/2012-09/2012
Firmware Engineer
C# programming for a second-generation Robotic Barista “Coffee maker” by Briggo. Using CAN technology with multiple ARM-9 processors. Some design work with IBM's Rhapsody.

Unitrends- Columbia, SC 02/2012-07/2012
Embedded Engineer
Embedded C/C++ software development on a Linux computer. Programming effort calls for the use of Embedded SQL2005/2008. Used the Eclipse software development tool package.
Some Windows Powershell development in Pascal.
Designed and coded for an embedded file archiving system using virtual file formats HyperV, VHD, vDisk, and VMware standards.
Used CentOS 5.6 & 6.0 and Linux Operating Systems development platform.

Additech- Houston, TX 11/2011-01/2012
Firmware Engineer
Embedded C/C++ software development with some C#coding, all coding used Eclipse, Dynamic C, or Microsoft Visual Studio.
Firmware programming on the “Rabbit” processor, which is a Z180, using embedded Dynamic C. This was an embedded system in gas station pumps.
Microsoft Windows programming and conversion from Linux to Windows Applications.
Modified existing software to correct errors, to adapt it to new hardware, or to upgrade interfaces and improve performance.
The company sells a gasoline additive and their processor card has to be integrated into their customers’ Gasoline Dispensers sale/payment data stream.
Originally hired to deliver the company’s next-generation embedded processor using embedded ARM-9 technology.
Embedded Ubuntu and Centos Linux Operating System development platform usage.

UTC Aerospace Systems, Houston, TX 05/2008-09/2011
Software Engineer
All software development was in embedded C and met all of NASA’s Embedded Flight-critical standards 2003 according to FARs 14 CFR Parts 1200-1299.
Prime software developer for NASA's next-generation Radiation Detector project known as Tissue Equivalent Proportional Counter (TEPC) that was deployed at the International Space Station (ISS). Interfaced with Low-level Linux drivers and FPGA hardware to collect radiation data to be used to protect the Health and Lives of Astronauts at the ISS.
All software development was in embedded C and to NASA’s embedded safety-critical standards.
100% Linux Host / Target development and debugging, using the Eclipse software package with Subversion (SVN)configuration control, Bugzilla for software tracking and corrections.
Modified existing software to correct errors, to adapt it to new hardware, or to upgrade interfaces and improve performance.
Advised customer about or perform maintenance of software system.
Analyzed information to determine, recommend, and plan the installation of a new system or modification of an existing system.
Consulted with engineering staff to evaluate interface between hardware and software, develop specifications and performance requirements, or resolve customer problems.
Directed software programming and development of documentation.
Stored, retrieved, and manipulated data for analysis of system capabilities and requirements.
Conferred with data processing or project managers to obtain information on limitations or capabilities for data processing projects.
Consulted with customers or other departments on project status, proposals, or technical issues, such as software system design or maintenance.
Coordinated installation of a software system and development utilizing Microsoft. NET.
Prepared reports or correspondence concerning project specifications, activities, or status.

Jacobs Engineering- Houston, TX 08/2007-05/2008
Senior Software Test Engineer
Hand-Picked by NASA management at the Johnson Space Center (JSC) to be the person that solely validate and verify the functionality of an Embedded flight-critical LabVIEW program that was to be used to help prevent any other Space Shuttle from disintegrating during reentry like the Space Shuttle “Columbia” did on Feb 1, 2003.
Documented the execution of current Wing Leading Edge (WLE) tile impact software version 2 of WLE tile impact software.
Formally tested all LabVIEW software used in WLE real-time safety-critical software system.
Rewrote safety-critical software package to support version 3 file structures stored in XML format, using structured programming techniques for LabVIEW.
Provided numerous software updates and coding corrections to support this mission-critical software.
Supported three Space Shuttle flights in the MER by manning two of the four console positions for WLE. Assigned to man the WLE console for the reentry phase of the very last Space Shuttle Flight.

Collins Aerospace - Cedar Rapids, IA 06/2007-08/2007
Software Engineer / Ada Software Engineer
Developed code for the FAA Embedded real-time safety-critical s/w system installed into all Boeing 767 Glass Cockpit avionics system to support Delta Air Lines, known as “Large Screen Format”, the processor that was used was an ARM-9. All software was written in Ada.
Updated test procedures, test cases, and performed dry runs with s “Large Screen Format” displays using ARM-9processors. The scripting language used was Python.
Altered Rockwell Collins Automatic Test Equipment scripts using Python to do a formal s/w test of real-time safety-critical s/w system, which was used to validate all Ada software changes.

The Boeing Company- Houston, TX 09/1996-04/2007
Full-time Employee
Senior Software Scientist
Space Shuttle Payload Requirements:
Reviewed Cockpit Avionics and Upgrade requirements and software.
Verified Shuttle payload software in the Shuttle Avionics Integration Lab (SAIL).
ISS Payloads Utilization Group:
Developed new features and maintained existing Ground Support Equipment software in C/C++, maintained existing embedded applications using the QNX OS and C/C++, maintained all code in the Mission Build Facility for the ARIS POP computer, installed all software that was shipped to International Space Station (ISS).
Utilized MIL-STD-1553B, TCP/IP networking, C/C++, and TReK support software.
International Space Station (ISS):
Assigned to two different Electrical Power System groups for software support.
Verified hardware MDM systems utilizing Matlab/Simulink and used VxWorks on an embedded RTOS system used to verify a rocket engine gimballing action.
Developed C/C++ applications to assist users in analyzing and verifying electrical power loads for ISS.
Shuttle Upgrade Proposal Group:
Team member of a $1.6B proposal to upgrade the Space Shuttle fleet to use PowerPC (PPC) microcomputers, a major replacement of wiring, replace the hydraulic system with electro-driven motors, and other major orbiter updates. My expertise was in the field of Space Shuttle flight operations, a theory of control, computer execution, and crewmember interaction.
Iridium Communication Satellite System:
Worked on the integration of the flight software for the Iridium satellites to change the Guidance Navigation & Control (GNC) System used by every first-generation Iridium satellite.
Developed an embedded PowerPC (PPC) system that used VxWorks and Ada; this system was remotely accessed via Host/Target relationship.

GHG Corporation and Dual Incorporated- Houston, TX 09/1991-09/1996
Fulltime Employee via Lockheed Engineering & Sciences Co.
Software Engineer
Key Team Member that helped to start the $200,000,000 project known as the Multi-Function Equipment Display Systems (MEDS).
Wrote a C/C++ program that sent simulated Global Positioning System (GPS) data into the JAEL’s Shuttle Computers. This effort used C/C++ ‘socket’ features.
Developed interactive software tools and math models using Ada, C/C++, and FORTRAN.
Performed code maintenance on the existing real-time space shuttle simulator in Ada and FORTRAN.
Designed and coded hardware device handlers (Serial, DMA, MIL-STD-1553B) in Ada, C/C++, and Pascal to be used during the real-time simulation.
Maintained all TCP/IP and RS232 communications that affected the VAX mainframe computer. Co-authored a cost reduction saving, totaling $800,000 to NASA.
Functioned as a Task Leader in charge of 1-3 people.

Lockheed Engineering & Sciences Co. 02/1988-09/1991
Full-time Employee
Computer System Engineer
JAEL Simulators. Johnson Space Center (JSC) Avionics Engineering Laboratory (JAEL)
Developed interactive software tools and math models using Ada, C, and FORTRAN. Designed and coded hardware device handlers (Serial, DMA, MIL-STD-1553B, ETHERNET TCP/P) communications were in C and/or Pascal to be used during the real-time simulation.
Duties performed the complete Configuration Control of all software and Operating System (OS) configurations.
Cost reduction saving, totaling $225,000 was submitted to NASA.
Functioned as a Task Leader in charge of 1-6 people.
Security Clearance ""SECRET"".
Advanced Programs Group:
Team member of NASA’s think tank to plan “The Next 20 Years for Mars Expeditions” (However, that has seemed to have stretched out to be the Next 40 Years of Mars missions). In 2022+ there is a Mars Mission to get core samples from Mars by dropping a series of penetrators into the Martian soil, as a rover lands on Mars. Then, rover will be sent to retrieve the core samples for return to the Earth. “This is my baby”.
Security Clearance ""SECRET"".
SES Simulation Development:
Developed an interactive program that functioned as a crude ""GLASS COCKPIT,"" all coding was written in Ada.
Maintained the existing real-time SES simulator that was written in FORTRAN. I was called upon to write hardware device handlers in FORTRAN, which interfaced with the simulated space shuttle crew station.
Functioned as a Task Leader in charge of 1-3 people.
Security Clearance ""SECRET"".

Lockheed Missile and Space Co. 06/1984 – 02/1988
Full-time Employee
Computer System Engineer
Ground Launched Cruise Missiles (GLCM): “USA Air Force”.
Designed interactive displays for the Instructor Console Unit (ICU) and the Weapon's Control Unit (WCS). Coded and tested software to meet the critical standards of the Air Force's Nuclear Strike program of ""ZERO Level of Errors."" All software that was developed and adhered to the DOD-2167 standards.
Functioned as a Task Leader in charge of 3-5 people.
Security Clearance ""SECRET & NATO SECRET"".
Aquila Drone: “USA Army”
US Army “UAV” worked in the configuration software system and system testing.
Precision Location & Strike Support(PLSS): “USA Air Force”
Developed software tools in ""MORTRAN"" to debug hardware problems in the PLSS computer complex. Software tools were created to test magnetic-tape units, printers, disk-controllers, and inter-computer communications. All developed software adhered to the MIL-STD-483 requirements. Also, my duties required me to perform Validation and Verification of all software testing on the ""System Integration Level"" and maintain Configuration Control of all software modules and Operating System (OS) configurations that were developed. All this occurred in a critical 6-month transition from Sunnyvale CA to Austin TX.
Performed as an acting Group Engineer in charge of 5-8 people, after the transition to Austin, TX my duties were a Task Leader in charge of 3-5 people.
Security Clearance "" SECRET & NATO SECRET "".

Singer/LINK Co. 04/1980 – 06/1984
Full-time Employee, promoted to Singer Link as written in the Singer/Sperry Contract
System Test Engineer IV

Shuttle Mission Simulation (SMS)
My duties consisted of directly operating the Space Shuttle Simulator for NASA astronaut crew training and development of engineering user software upgrades. This entailed being abreast in many phases of the space shuttle's operation, such as launch, orbit, and entry. Required to Validate and Verify each software load that was used by the next Space Shuttle Mission. My services were required to explain new software models and recent hardware modifications, both to the astronauts and to engineering users. Many times, when software or hardware changes were being developed, my services were required to function as an acting astronaut crewmember and perform all necessary 'hands-on' operations. Yes, I got to play the part of an Astronaut; it was in my job description. Also, I was required to write interactive software tools, and utilities using FORTRAN.

Security Clearance ""SECRET"".

Sperry UNIVAC 06/1979 – 04/1980

Full-time Employee, promoted to Singer Link as written in the Singer/Sperry Contract
System Analyst II

Shuttle Mission Simulation (SMS)

Installed all software packages that were developed in the Shuttle Mission Simulation (SMS) Complex.
Examined software packages to be sure that they conformed to all configuration control standards.
Compiled, and installed said software for all 'HOST' and target 'IC' computers. My duties required that I write a complete set of ""JCL"" utilities that were used as the standard for all software compiles and installations.
Security Clearance ""SECRET"".

CERTIFICATION
Certified Level II Non-Destructive-Testing (NDT) Technician for EMAT operations.
Trained to use MIL-STD-1553B.
Trained by National Instruments to be an advanced LabVIEW developer.
Completed “2015 Information Security Boot-camp” offered at the University of Houston.

PATENT APPLICATIONS
May 2008, I filed for a patent application for a Solid-State Power Converter for a Nuclear Reactor.

EDUCATION
Certification in Cyber Security, July 2015
Information Technology College University of Houston, Houston, TX

University of Texas- Austin, TX
Bachelors in Computer Science

Associate of Applied Science in Welding (Underwater)
San Jacinto Community College- Pasadena, TX

Attended and Majored in Physics
Odessa College, Odessa, TX

"
,
Intelligence Analyst Data Scientist,"
Dallas, Texas *****
adm04h@r.postjobfree.com
Barath Vishnu Vijay Anand +1-214-***-****
www.linkedin.com/in/barath-vishnu
Education
Master of Science in Business Analytics (Data Science Specialization) Jan 2020 - Present University of Texas at Dallas
Bachelors in Electrical and Electronics Engineering Jun 2014 - Apr 2018 PSG College of Technology, Coimbatore, Tamilnadu, India Work Experience
Business Intelligence Analyst Intern, Veritas Technologies LLC, USA May 2021 – Present
• Worked with internal customers and business operations resources to develop strategic reporting, dashboarding and data solutions across Global Services.
• Created global operational templates, scorecards to compile and standardize disparate information that drive standardized reporting and metrics tracking.
Data Scientist, Mu Sigma Business Solutions Pvt Ltd, Bangalore, India Oct 2018 – Nov 2019
• Built a classification algorithm for a telecom company, identified patterns in customer attrition, predicted churn rate with an accuracy of 95%.
• Developed a Time Series Forecasting model for an European retailer, to predict their sales, achieved an accuracy of 94%.
• Led the design thinking workshop to provide direction on the design of trade promotion management system for a CPG manufacturer.
• Identified and Proposed the opportunity of extending the existing solution to trade promotion optimization creating potential for additional revenue.
• Developed an ETL pipeline for data ingestion, manipulation and designed a self-serving dashboard to track trade expenditure across Asian markets.
• Held regular interaction with the stakeholders, acted as liaison between the technical and business teams to help them determine the key metrics that are needed to make business decisions. Academic Project
Stock-Bot (Automation Anywhere) Jan 2020
• Created a bot using RPA tool to extract data from Securities and Exchange Commission website to be uploaded into excel file and analyse the best stock to invest.
Predicting Current Market Values for Houses (Python) November 2020
• Aggregated data through Web APIs, performed data wrangling and data cleaning for more than a million records. Implemented Feature Engine Pipelines, used GridSearch to find best hyperparameters and best performing algorithms using Ensemble learning.
Electricity Demand Prediction through Analytics Jan 2018
• Acquired Energy consumption data, performed EDA using Python, used SAP Lumira for Data Visualization to understand consumption trends.
• Proposed seasonal exponential smoothing procedure to build a prediction model accommodating seasonal variations. Skills
• Programming Languages: Python, R, Pyspark, C, SQL, SAS BI/Data Viz: Power BI, Tableau Big Data: Hadoop, Sqoop, Kafka, Impala, Hive RPA tools: Automation anywhere, Blue Prism Cloud: Microsoft Azure, Azure Devops, Google Cloud Platform, AWS, Adobe Analytics Machine Learning Libraries: Pandas, Numpy, Scikit learn,matplotlib, seaborn Leadership Skills and Awards
• Member Entrepreneurs Club, PSG college of Technology, Coimbatore Jun 2015 - Mar 2016
• Won the best project award for 2018 at PSG College of Technology Mar 2018
• Treasurer, Vaalmeengal Tamil Sangam (VTS), University of Texas at Dallas Apr 2020 - Present
• Member Dean’s Council, University of Texas at Dallas Sep 2020 - Present
• Mentor, Business Analytics Leadership Council, University of Texas at Dallas Sep 2020 - Present

"
,
Mathematics Teacher Data Scientist,"
Nastaran Rahnamaei
Middletown, DE 512-***-**** adm034@r.postjobfree.com LinkedIn GitHub
SUMMARY
Data scientist with a strong mathematical background, with experience in utilizing machine learning methods such Supervised learning, Unsupervised learning to predict data modeling and analysis of data mining algorithms. Delivering insights and implementing action-oriented solutions to complex and challenging business problems.
Strong ability to interpret and reference various formulas to solve complicated challenges.
Highly motivated, methodical, and well organized with several years of proven experience in mentoring/coaching students as well as providing the consultation for their future.

CORE COMPETENCIES
Statistical Data Analysis, computing methods
Programming Language (Python and SQL)
Model Building
Data structures
Data science research methods
Supervised and unsupervised machine learning algorithms
Regression Analysis
Data mining
Data Visualization
Math (Statistics and probability)
Relational database (MySQL)
Technical Problem-Solving
Critical thinking, Decision-making
Education & Certifications
B.Sc., Pure Mathematics, Azad University, July. 2007 (Accredited by WES – New York)
Data Science Career Track Certification, Springboard Apr. 2020 – Current
Data Science Bootcamp, Udemy E-Learning Platform, August 2019
Python for Data Science and Artificial Intelligence, Coursera E-Learning Platform, September 2019
Introduction to Data Science in Python, Coursera E-Learning Platform, September 2019
Machine Learning, Coursera E-Learning Platform, October 2019
Programming (Python) for Everybody, Coursera E-Learning Platform, October 2019
US Citizen (Relocated to the US in 2012)

Professional Experience

Springboard Data Science Career Track Data Science Fellow Online, United States Apr. 2020 – Current
500+ hours of hands-on course material, with 1:1 industry expert mentor oversight, and completion of 2 in-depth portfolio projects. Mastered skills in Python, SQL, data wrangling, data visualization, hypothesis testing, and machine learning.
In one of my capstone projects which was about credit card fraud detection, I built a machine learning models to identify whether new transactions are fraudulent or not and four type of models of machine learning has been used to examine their performance on a dataset which contains real world transaction data.
Addressed the problem of identifying fake news by detecting and analyzing fake news features, identifying the textual and sociocultural characteristics of fake news features.

Eye Level Learning Center, Math Teacher Detroit, Michigan, United States Apr. 2019 – Apr. 2020
Supervised students, oversaw practices and provided solutions to all Elementary, Middle and High School students.
Taught new techniques to solve problems and taught future subjects based on school schedule.
Provided booklets to students and taught critical thinking math to students.

Mezon Nasi, International Sales Entrepreneur June 2015 – April 2019
Provided leadership, expertise and direction in the business as the owner of the international small business.
Bought clothing, accessories at different online and local shopping centers at wholesale/ discounted price to sell them to the customers with marked up price.
Managed local sales agents and other team members to provide the best customer service to clients.
Handled all customer service issues in detail and provide consultation and coaching to team members satisfying customers.

Self Employed, Daily Stock Trader June 2014 – April 2015
Determined market sentiment via research, new evaluation and data analysis.
Forecast of future financial price movements based on an examination of past price movements by analyzing trading charts technically.
Research and evaluate investment opportunities, assessing short- and long-term risks associated with each company.
Monitor business and financial news and gained expertise in quantifying effect of world events on financial markets.

Austin Community College, Mathematics Tutor March 2014 – Aug 2014
Provided teaching assistant to all college students at the learning lab regarding their mathematical concerns, questions, and requests.
Successfully and professionally taught students to fit their individual needs in area of Pre-Algebra, Algebra (I & II), Geometry, Pre-Calculus, Calculus, Trigonometry.
Effectively balanced tutoring hours given for each assigned student to cover as many people as I could teach during specific shift.

RELOCATED TO UNITED STATES - Worked outside of the educational industry as a shift manager Nov 2012 – July 2013
Mathematics Teacher Sep. 2003 – Sep. 2012
Taught Mathematics and related math courses to Middle School and High School and College students.
Supervised student`s practices and solved their problems
Actively instruct students, create lesson plans, assign and correct homework, manage students in the classroom, communicate with parents and help students prepare for standardized testing.
Planned and organized material for instruction and presented and reinforced lessons as appropriate.

Educational Advisor Sep. 2003 – Sep. 2012
Counseled High School & Pre-University Students.
Provided goal setting courses to students and taught them how to set their plan properly to achieve their goals.
Prepared students for the Bi-Weekly exams and evaluated their results to provide them consultations for future examinations.
Met with their parents monthly to provide them updates regarding the progress of their children and being available for their call as needed.

"
,
Research Assistant Data Scientist,"
HARSH SHUKLA
MS, DATA SCIENTIST
SUMMARY
Proven team leader, data scientist, analyst, and engineer with broad, collaborative capabilities. Experience in working with large, unorganized and complex datasets in domains such as healthcare, pro sports, finance, public records, surveys and customer behavior. Experience in building complex algorithms with minimal processing time. Able to explain analytical solutions using effective visualization to a wide range of audiences in a clear and precise manner. Agile, results-oriented, entrepreneurial, collaborative mindset. EDUCATION
Master of Science in Data Science
DePaul University, Chicago, IL
2017 September – 2019 November
Bachelor of Engineering in Information Technology
Gujarat Technological University (GTU), Gujarat, India 2012 August – 2016 June
TECHNICAL SKILLS
• SQL (MySQL, PostgreSQL)
• Tableau, Power BI, Google Data Studio
• Python (Jupyter Notebook, Anaconda), R, SPSS
• AWS (Hadoop, Sage Maker, Glue), Snowflake
• Google Cloud(Office tools, Virtual Machine)
• MS Office Suite (WORD, POWERPOINT, EXCEL)
EXPERIENCE
Data Scientist, Barwis Methods December 2019 - Present Performance Data Scientist for the Grand Rapids Griffins (American Hockey League) and the Detroit Redwings
(National Hockey League), in cooperation with Barwis Methods Training Centers. Accountabilities/Results
• Developed and implemented custom API data pipelines to bring performance device data to current third- party Athlete Management System (AMS).
• Developed custom design of individual performance dashboard for athletes, coaching staff, and performance staff for day-to-day and period performance reporting.
• Implemented Google cloud-based custom server to maintain data pipelines.
• Implemented custom recommender engine within data pipeline to optimize fast processing of performance improvement through automation.
• Lead contact and daily consulting with third party AMS provider/vendor. Design of first generation AMS
• Designing and building a custom architecture, first generation Athlete Management System (AMS) to track, store, and analyze athlete performance data.
• Lead contact and progress monitoring for 3rd party developers.
• Designed a dynamic reporting platform with custom reference lines and other important features to improve reporting.
• Developed 3rd party devices data integration with live data feed and API development.
Grand Rapids, Michigan, USA admzmy@r.postjobfree.com +1-773-***-**** https://www.linkedin.com/in/hshukla2/ Research Assistant at DSL lab, DePaul University May 2019 - December 2019
• Researched and gathered data from various resources.
• Cleaned and organized data, calculated features, performed various analysis to leverage insights from data.
• Performed multiple analytical methods to find similarities in trademark registration.
• Worked with Python [OpenCV, Pandas, Scrapy, Pillow, Matplotlib], Tableau. Classroom Tech Assistant at IS Department, DePaul University September 2017 – December 2019 Responsible for on-the-spot equipment troubleshooting, reconfiguring and installing projectors, equipment and software in classrooms. Provided Tech-support for classrooms. Communicated with clients and resolved queries. Worked under a time constraint. Mentored new recruits. Performed as active-in-charge in absence of supervisor. Data Analyst at Web big2day August 2016 – July 2017
• Researched domain and gathered data from repositories. Cleaned, pre-processed and organized data.
• Converted data into actionable insights by predicting and modeling future outcomes. [Linear -polynomial - logistic regression, decision trees].
• Utilized SQL, Excel [Pivot tables, add on data analytics package], Python [Sk-learn, NumPy, Pandas] for data intelligence and analysis.
Team Leader, Capstone Undergrad and Grad Project August 2015 – May 2016 Active Volunteer at DISA (DePaul Indian Student Association) January 2018 – June 2019 Volunteer data science workshop host for Data Science students’ group April 2019 – September 2019

"
,
"Electrical Engineer,Computer Engineer,Innovation,Six Sigma","
GUSTAVO LARA
Marketing Product Manager
PROFILE
Electrical and Computer Engineer with 15+ years of experience and a successful track record in marketing, product management, IT, software programming, computing hardware, materials innovation, engineering and manufacturing. Excellent interpersonal and communication abilities and possess a wide range of technical skills.

CONTACT
PHONE:
915-***-****

WEBSITE:
https://www.linkedin.com/in/gustavo-lara-034a2a9a/

EMAIL:
admzl3@r.postjobfree.com

EDUCATION
El Paso Community College
August 2019 - Present
Cyber Security Continuing Education Program – 3.5 GPA

The University of Texas at El Paso
June 1996 to December 2001
B.S. in Electrical and Computer Engineering – 3.0 GPA
WORK EXPERIENCE
Vishay, Inc – Marketing Product Manager
2018 – Present
Managed a six million dollar portfolio my first year
Maintain a variable margin of greater than 35%, to increment the portfolio revenue
Meet and work with customers to develop/maintain relationships to meet the revenue, design and share goals

OSRAM Sylvania – Process Control Engineering Scientist
2003 – 2017
Managed Gate-Phase Projects via MS Project and MS Sharepoint and wrote several programs in LabView, MATLAB, C++, Python, MS Access, VB and VBA for data acquisition
Maintained and troubleshoot both software and hardware for R&D lab equipment
Slashed production costs by 10% to 15% by leading Product Quality Improvement (Six Sigma) project teams
Monitored the production process by programming and implementing a remote production monitoring system that connected via LAN from a remote HMI
Aided IT with network connections, desktop troubleshooting on the production floor and the programming of Windows Power Shell batch files and shell scripts

The University of Texas at El Paso – System Network Administrator
1998 – 2003
Served as Chief System Administrator of Windows and Linux Servers
Slashed “downtime” on resolving tech issues by building a “Knowledge Database” and IT Ticketing System with Visual Basic, ASP.net and HTTP
Responsible for the day-to-day operation of the Windows, Apple and Linux networks and preparing disaster recovery
Responsible for the management of users, computers and other devices on the network via Windows Server
Organized, installed, supported and secured the University’s computer systems, including the local area network (LAN), intranets, wi-fi and all audio/visual equipment
SKILLS

"
,
Data Scientist Translator,"
Mohammed Zikrulla
630-***-**** admy8g@r.postjobfree.com LinkedIn
EDUCATION
Master of Science in Data Science July 2019 – May 2021
Lewis University (CGPA: 3.74/4.0) Romeoville, IL

(Coursework: Statistical Programming, Data Mining and Analytics, Concept of Statistics 1, Large Scale Data Storage Systems, Artificial Intelligence 1, Artificial Intelligence 2, Concepts of Statistics 2, Natural Language Processing, Machine Learning, Data Visualization)

Bachelor of Engineering in Information Science Sep 2012 - June 2016
Tjohn institution of Technology (CGPA: 3.4/4.0) Bengaluru, IN

(Coursework: Discrete Mathematical Structures, Data Structures with C, Design and Analysis of Algorithm, Database Management Systems, Data Compression, File Structures, Data Warehousing and Data Mining, Storage Area Networks)

WORK-EXPERIENCE
Data Scientist & Operation Analysist January 2017 – June 2019
Unisys Bengaluru - IN
Developed machine learning models to determine and predict data-sources with the highest confidence interval
Deployed analysis based marketing strategies. generating +$15 million
Gathered data through 10,000+ villagers' interviews and questionnaires for data analysis
Created and leveraged pain point analysis into proposal making, for strategic planning for more than 60 villages
Executed meta analytics using R for internal data operations increasing productivity by 10%
Utilized Tableau & ggplot2 to understand data dependencies and relationships from various regions within India
Assisted technical and non-technical teams by translating requirements according to business needs

PROJECTS
Predicting the ratings of unwatched Movie by User May 2021
Used the real world data and performed Data analysis and pre-processing of the data
Created sparse matrix of the data and split it into train and test data
Implemented different ensemble models and machine learning models on train and test data
RMSE error rate and selected the best among all the models
Created the recommendation using collaborative filtering algorithm and selected model with low RMSE error rate
Got the predicted rating of the movie which is recommended to the user and not been watched yet

Object Detection and Tracking. November 2020
Did research survey IEEE paper with the professor on Object Detection and Tracking based on Machine Learning the same will be published soon

Character communication Application for Windows. May 2016
Developed an application which translate text from one language to another language
Improved the results by 15% compared to Google translator
Purpose to build this application as google translator is blocked in China and this can be used as alternative
Used a technique called Statistical Machine Translation as over-simplification model
Database is flooded with millions of translations of different languages by human which are inserted in machine, and an algorithm used to find patterns to get the results

Robo-zest competition(1st Prize) January 2013
Built a Robot ( Coding the robot to run on the black track using batteries and other electronic devices connected together from point A to B) with the team

SKILLS
Programming : R, SQL, Python (Libraries/Packages: Pandas, NumPy, SciPy, scikit-learn, Matplotlib, TensorFlow, Keras), C++, Linux Shell, Windows Command Line, Basic HTML and CSS, Data Structures, MongoDB
Data science toolkits: MySQL Workbench, Microsoft SQL Server Management Studio, SAS, MATLAB, Tableau, GitHub, Linux, Google Analytics, Canopy, Gephi, Power BI, SAP BO, Jupyter Notebook, PyCharm, Eclipse, Microsoft Office Suite, MongoDB, RStudio, Orange, Android Studio, Anaconda
Data science: Linear Regression, Neural Networks, Time Series Forecasting, Deep Learning, Advanced Machine Learning, Bayesian Learning
Cloud platforms: Amazon Web Services (AWS), Google Cloud Services (GCP), VMware

HONORS AND AWARDS
Best Employee of Month, Unisys September 2017 - November 2017
Best Performer, Unisys April 2018 - September 2018
In the month of August was awarded as best team performer and was promoted to do the transitions of team for a new client.

"
,
Data scientist intern,"
JAHNAVI ALLAM
346-***-****, admy03@r.postjobfree.com, LinkedIn
San Jose, California 95117
OBJECTIVE
Graduate student looking for opportunities in data analytics with strong background in python, R,Tableau,SAS, and advanced data analytics skill.
EDUCATION
University of North Texas, Denton, Texas. April ‘21 Master’s in Data Science GPA - 3.9/4.0
TECHNICAL SKILLS
Hands-on experience with Data analytic algorithms includes Linear regression, logistic regression, KNN, K-Mean, Decision trees, Cluster analysis, Neural Networks. A. Packages and Modules : NumPy, Pandas, Matplotlib, Seaborn, Scikit-Learn, Regex, MySQL, SQL Server 2000, ggplot2,qplot, Dplyr
B. Operating systems: Windows, Linux, macOS.
C. Interest: Data Management and Reporting, Tableau, Data Analytics D. Programming Skills: C, Python, and R
E. DS Tools: SPSS, SAS/Base, SAS/SQL, PowerBI, Jupyter Notebook, RapidMiner, Advance Excel. F. Certification: SAS Certified Base Programmer for SAS 9 (CBP) - N8QPGHCKCEQE1GGP EXPERIENCE
Graduate Assistant, UNT Oct ’19-Nov ‘20
• Designed database tables and structures, created views, functions, and wrote optimized SQL queries for integration.
• Created visualizations for the data by using Tableau, Designed Dashboards, Twisting SQL queries for improving performances.
• Performed Data Visualization using packages like Seaborn, NumPy, matplotlib, ggplot in Python and SAS, (dplyr, ggplot2,qplot) in R to generate Scatter Plot, Box Plot, Histogram etc., also created predictive and descriptive models based on the analytics of data generated using twitter data.
• Produced ad-hoc reports with V-lookups, Pivot tables, and Macros in Excel and recommended solutions to drive decision making, for business problems and provided business modeling solutions. SAS Programmer, Signet soft Private Limited, Bangalore, India. Jan ‘18 - Jul ‘19
• Developed code using various procedures like PROC SQL, PROC SORT, PROC PRINT, PROC MEANS, PROC CONTENTS, PROC FREQ, PROC FORMAT, PROC TRANSPOSE, PROC REPORT.
• Provided statistical support to principal investigators and research staff in the design of research studies, developing statistical analysis plan, methodology, and procedures, IRB protocol.
• Prepared new datasets from raw data files using Import Techniques and modified existing datasets using Data steps, Set, Merge, Sort and Update, Formats, Functions, and conditional statements.
• Developed, maintained, and performed SAS program with the use of SAS/BASE, SAS/MACRO, SAS/SQL for data transfers, manipulation, cleaning, analysis, and reporting.
COURSE PROJECTS
Netflix Video Recommender – [Python, ML-SVD, Tableau, EDA] Spring ‘21
• Preprocessed the Netflix movie data using EDA, done data analysis, data visualization
• Used pandas, pyplot, and seaborn in SVD Algorithm to recommend videos for the users based on gender, age, and genre. Hospital Management system – [Data Modelling-ERD, My SQL Workbench, Draw.io] Fall ‘20
• To computerize all the details about the Physician, patient, and hospital.
• Used Database is the SQL server for writing the Queries of each table like insert statement, create a statement, update, insertion of a new record, alter and delete statements.
• Created the ERD diagram in the draw.co.in and a data dictionary was created based on the ERD and business rules. Impact Of G.D.P_per_capita, HDI On Suicide Rate –[Advanced-Data Analytics, MS Excel] Summer ‘20
• The main aim of the project is to analyze the suicide rates dataset and compare whether the suicide rate is dependent on the GDP per capita and Human Development Index or not.
• The analysis is done by doing the correlation between the G.D.P_per_capita and HDI with the suicidal rates and tested the hypothesis by plotting the scatterplot and Normal probability and compared with the final regression output. FIFA Data Analysis – [Data Visualization, Tableau] Spring ‘20
• Preprocessed the data using different techniques. By using Tableau and Microsoft Excel the processed data is analyzed.
• Performed different visualizations on the data by different visualizing maps or graphs and some of the assumed hypothesis statements are explained in this project.
Data Analysis of the Foodservice Zomato – [Data Mining, SAS enterprise miner] Fall ‘19
• Preprocessed the data using preprocessing techniques (Data Selection, Data Cleaning, transformation, and integration).
• Determined the best classification model (Decision Tree) by comparing the accuracy and other outcome indicators.
• Worked on SAS Enterprise Miner to perform Logistic Regression, Cluster analysis, Decision trees.

"
,
Data Scientist Prophet,"
MANOHAR BOORLU
Kansas City, MO ● Open to relocation ● 816-***-****
github.com/manoharboorlu ● admx89@r.postjobfree.com A highly passionate data scientist with over two and half years of work experience who believes that data is the most valuable resource for the growth of a company. I am skilled in machine learning, problem solving, predictive analytics and programming to perform experiments on data and bring out the insights to tell a strong story with it! SKILL STACK
Languages : Python, R
ML Library : TensorFlow, PyTorch, Pandas, NumPy, Keras, scikit-learn Tools : AWS, GCP, Tableau, Spotfire, Kafka, Talend (ETL), Power BI Database : SQL, Oracle, PL/SQL
WORK EXPERIENCE
Data Scientist - Merck & Co. Sep 2019 – Present
• Multivariate Time Series Forecasting and Predictive Research of $4 Billion sales, drilled down to deep levels of product hierarchy, considering external factors that influence the sales, and creating what- if forecast to tweak external factors and observe their effect on sales.
• Developed a Customer Churn analysis and prediction model and performing targeted marketing and promotions to those customers.
• Clustering and Classification of Sales reps based on sales with effective location deployment and targeted marketing.
• Market Basket Analysis and distributor buying analysis to find interesting patterns in product purchases and suggest marketing and product grouping programs based on location/seasonality. Jr. Data Scientist - Unified Government of Wyandotte County Feb 2019 – Aug 2019
• Analysis and classification of crime data to locate the areas, time, and type of crime to better predict the likelihood of happening a crime and take necessary precautions.
• Performed analysis on historical data and suggested location-based predictive policing and medics. Data Scientist Intern - Real Quantum Oct 2018 – Jan 2019
• Initiated and designed a generalized relational database schema to fit the largely incoming unstructured real estate data from hundreds of counties and used data wrangling methods.
• Migrated data with 40 million data points to PostgreSQL using ETL tool (Talend) and AWS Redshift.
• Implemented machine learning concepts of regression, clustering and time series forecasting using Python that brought out valuable insights out of the real estate data to focus on growing locations. Software Engineer Intern - Chain of Trust Technologies Aug 2018 – Oct 2018
• Debugged around 60,000 LOC and logged around 20 issues during the pre-release testing of the software.
• Fixed the issues using functional programming approach with python. PROJECTS AND PUBLICATIONS
Parking Availability Forecasting Model (Time Series Forecasting) – Master’s Research & Thesis (IEEE Publication)
• Performed data cleaning and pre-processing on 1M rows of historical raw data from parking sensors.
• Trained and tested ML models like LSTM, Prophet, ARIMA and Recurrent Neural Networks with Time Series data to best predict the availability of parking spots at a desired location, date, and time.
• Published the same in the IEEE International Smart Cities Conference in Morocco. Task Recommender Application (Python, Machine Learning, Tableau)
• Suggests the user what tasks they can do using correlations, data analysis and NLP based on previous input considering many variables like task location, task duration, part of the day, weather and success rate. Face Recognition (MATLAB, R, Neural Networks)
• Developed a facial recognition application using MATLAB by training with datasets of human faces and testing the model with other pictures of them in different facial angles, and lighting conditions. EDUCATION
University of Missouri-Kansas City
Master of Science - Computer Science (Data Science) GPA: 3.7 Bachelor of Science - Computer Science GPA: 3.8

"
,
"data science, machine learning","
ANDREW ROSE DATA SCIENTIST
admvdi@r.postjobfree.com 408-***-**** Bay Area, Ca www.linkedin.com/in/andrew-g-rose/ SUMMARY
Burgeoning data scientist with a background in finance and sales. Knowledgeable in a wide variety of financial instruments and comfortable making presentations to prospective clients. Versed in the latest data science and deep learning packages. Capable of wrangling and modeling data for a variety of applications. DATA SCIENCE: Python, Tensorflow, SQL, Statistics, Machine Learning, Computer Vision, PyTorch, Spark, XGBoost SALES: Relationship Management, Public Speaking / Presenting SKILLS
PROJECTS
- Fraud Detection using Credit Card Transaction Data May 2021
- Created a fraud detection model using real transaction data. Utilized feature engineering and tree-based modeling to achieve impressive results. (over 80% of fraud accurately identified.)
- Transfer Learning for Edge Devices (object detection) May 2021
- Created a custom object detection model using transfer learning and deployed to edge device for real-time use. EDUCATION
Springboard Data Science Career Track · May 2021
500+ hours of hands-on course material, with 1:1 industry expert mentor oversight, and completion of 2 in-depth portfolio projects.
Mastered skills in Python, SQL, data wrangling, data visualization, hypothesis testing, and machine learning. Specialization in deep learning applications.
San Jose State University · Fall 2010
Bachelor of Science in Business Administration
- Focus in: Insurance, Options, Portfolio Optimization. EMPLOYMENT
NRG Capital Management
Project Manager · Jan. 2018 to Current · San Jose, Ca
- Help generate income and reduce energy bills by repurposing unused land and rooftop space into renewable energy assets.
- Plan and develop commercial solar energy installations 500Kw - 2Mw.
- Collaborate on cashflow modeling using excel to provide clients accurate expectations of projects.
- Responsible for client management and project oversight. Stifel Financial Corp
Client Service Associate · Fall 2015 to Fall 2017 · San Francisco, Ca
- Provided full-service brokerage functions to institutional customers.
- Performed portfolio analysis and presented investment ideas to hedge funds and investment managers.
- Oversaw several marketing campaigns to stay connected with current clients and to attract new investors. (most successful campaigns were mailers!)
MassMutual Financial Group
Financial Advisor · Fall 2012 to Fall 2015 · San Francisco, Ca
- Provided comprehensive financial planning services to individuals and companies.
- Trained on client relationship management, complex financial instruments, analysis, and productivity tools plus state and federal regulatory requirements.

"
,
Restaurant Owner Data Scientist,"
Technical Skills: Machine Learning, Data Visualization, Big Data, Data
Mining, Microsoft Excel
Programming Languages: Python, R, SQL,
Soft Skills: Problem-Solving, Active Learning, Risk Analysis. SKILLS
SUMMARY
An entry-level data scientist who takes pride in building models that translate data points into business insights. Now eager to apply my knowledge to real-world business problems.
209-***-**** Llinkedin@Adi-Larrazolo
admu7r@r.postjobfree.com
Adi Larrazolo
Data Scientist
CAREER
Gourmet
Restaurant
Group LLC.
Wells Fargo
Home
Mortgage
2008 - 2017
2004 - 2008
BOOKKEEPER / RESTAURANT OWNER
Analyzed cost control to provide timely financial
information to support company goals.
Processed payroll and electronic deposits.
Processed bank reconciliation and financial
reports.
LOAN OFFICER
Evaluated the loan and credit needs of clients,
taking into consideration all aspects of their
financial background in relation to policies and
regulations.
Formed positive and trustworthy relationships
with clients as we explored financial options that best suited their needs.
2020 - 2021 SOUTHERN CAREERS INSTITUTE
GPA 3.91
Took additional courses in Big Data Ecosystems
and Data Visualization.
EDUCATION
GitHub@adilarrazlo83
Data Science

"
,
Research Scientist Springer,"
KUN YANG
W***th Street, New York, NY ***** j +1-631-***-**** j admtn2@r.postjobfree.com
https://github.com/Learn-Live j https://www.linkedin.com/in/kun-yang-a2b92a162/ PROFILE
Highly self-motivated and hard-working Ph.D. with expertise on machine learning and network secu- rity (such as anomaly detection and tra c classi cation)
Pro cient in Python and LaTex
Strong teamwork and self-learning skills
Non-U.S. citizen (J1 visa holder) needing visa sponsorship EMPLOYMENT
Columbia University, Department of Statistics Sept. 2019 - Present Postdoctoral Research Scientist Manhattan, NY
Research on the intersection of machine learning and the Internet of Things (IoT) New York University, Tandon School of Engineering Apr. 2018 - Aug. 2019 Postdoctoral Associate Brooklyn, NY
Research on cybersecurity (e.g., anomaly detection and tra c analysis) by leveraging machine learning EDUCATION
Chinese Academy of Sciences, Institute of Information Engineering Sept. 2012 - Jan. 2018 Ph.D. in Computer Software and Theory Beijing, China Hefei University of Technology, School of Mathematics Sept. 2007 - Jul. 2011 B.S. in Information and Computing Science Anhui, China EXPERIENCE
Encrypted Tra c Analysis, New York University Oct. 2018 - Aug. 2019 Core Member Brooklyn, NY
Abstract: targets to identify and classify encrypted tra c without needing to handcraft features and parse encrypted content, and then further improve network QoS according to the classi ed results
Proposed an end-to-end classi cation framework based on convolutional neural network (CNN), which can classify encrypted tra c with an accuracy of more than 90% without needing to parse the encrypted packet’s content and using any header information. Moreover, the proposed approach has more than 3% improvement in accuracy compared to others (such as SVM). Anomaly Tra c Analysis, New York University Apr. 2018 - Apr. 2019 Core Member Brooklyn, NY
Abstract: aims to detect anomaly tra c (e.g., DDoS ) by leveraging deep learning algorithms and keep the network safe
Proposed a DDoS detection framework based on deep neural network (Autoencoder), which only uses normal tra c to build the detection model and can obtain 80% detection rate with 0 FPR. Moreover, it can detect new and unknown attacks; however, traditional supervised approaches may fails (such as Decision Tree)
Intelligent Analysis, Chinese Academy of Science Sept. 2014 - Jan. 2018 Core Member Beijing, China
Abstract: aims to discover anomaly tra c and unknown, potential security threats from network tra c collected from several government departments networks
Analyzed network tra c with statistical algorithms, such as Random Forest, and obtained more than 95% detection rate. Also, established a distributed system testbed based on Spark, and obtain more than 2 times in testing speed compared to a Mahout based testbed. Furthermore, distinguished two kinds of similar tra c (DDoS and Flash Crowd), and obtained 98% accuracy based on the features extracted by Autoencoder.
Invisible Watermark, Chinese Academy of Science Jul. 2013 - Sept. 2014 Core Member Beijing, China
Abstract: aims to protect images copyrights of a Chinese company by embedding the invisible water- mark into these images, and the watermark can only be extracted by the company
Investigated, designed and implemented the invisible watermark embedding and extracting algorithm, which ensures that it can not only minimize the quality loss of original image, but also maximize the quality of extracted copyright
SKILLS
Machine learning GMM, SVM/OC-SVM, KDE, DT, KMeans, KNN Deep learning CNN, Autoencoder, GAN, RNN(LSTM), CRNN Programming language and tools Python, LaTex;
Pytorch, Scikit-learn, Scapy, Wireshark;
Matplotlib, Seaborn, Pandas, Numpy;
HPC, Google Cloud, Spark;
Git/Github, Pycharm, Markdown
Others Keras/Tensor ow, Linux, MySQL, MS O ce
PUBLICATIONS
1) Kun Yang et al. ""A Comparative Study of Network Tra c Representations for Novelty Detection""
(under review)
2) Kun Yang, Lu Xu, Yang Xu, and Jonathan Chao. ""Encrypted Application Classi cation with Con- volutional Neural Network."" In 2020 IFIP Networking Conference (Networking), pp. 499-503. IEEE, 2020.
3) Kun Yang, Junjie Zhang, Yang Xu, and Jonathan Chao. ""DDoS Attacks Detection with AutoEn- coder."" In NOMS 2020-2020 IEEE/IFIP Network Operations and Management Symposium, pp. 1-9. IEEE, 2020.
4) Degang Sun, Kun Yang, Zhixin Shi, and Chao Chen. ""A New Mimicking Attack by LSGAN."" In 2017 IEEE 29th International Conference on Tools with Arti cial Intelligence (ICTAI), pp. 441-447. IEEE, 2017.
5) Degang Sun, Kun Yang, Bin Lv, and Zhixin Shi. ""Could We Beat A New Mimicking Attack?."" In Network Operations and Management Symposium (APNOMS), 2017 19th Asia-Paci c, pp. 247-250. IEEE, 2017.
6) Degang Sun, Kun Yang, Zhixin Shi, and Yan Wang. ""A Distinction Method of Flooding DDoS and Flash Crowds Based on User Tra c Behavior."" In Trustcom/BigDataSE/ICESS, 2017 IEEE, pp. 65-72. IEEE, 2017.
7) Degang Sun, Kun Yang, Zhixin Shi and Bin Lv. ""A Behavior-Based Method for Distinction of Flooding DDoS and Flash Crowds."" In International Conference on Knowledge Science, Engineering and Management, pp. 129-136. Springer, Cham, 2017. 8) Degang Sun, Kun Yang, Zhixin Shi, and Yan Wang. ""Detecting Flooding DDoS Under Flash Crowds Based on Mondrian Forest."" In International Conference on Wireless Algorithms, Systems, and Appli- cations, pp. 729-740. Springer, Cham, 2017.
9) Bin Kong, Kun Yang, Degang Sun, Meimei Li, and Zhixin Shi. ""Distinguishing Flooding Distributed Denial of Service from Flash Crowds Using Four Data Mining Approaches."" Computer Science And Information Systems 14, no. 3 (2017): 839-856.
(Note: during my Ph.D. period (from 2012 to 2018), the rst author of all my papers is my supervisor
(Degang Sun). I am the second author and have written all the papers.)

"
,
Data Scientist/Analyst,"
Skanda Sridharan Bharadwaj
Master of Science (Data Science)
West Lafayette, Indiana, US +1-765-***-****
adms30@r.postjobfree.com
https://www.linkedin.com/in/skandasbharadwaj/
EDUCATION
Master of Science (Major: Data Science)
Purdue University, West Lafayette GPA – 3.79/4 Graduation: May 2021 DATA SCIENTIST PROFILE
Offer unique combination of experience and knowledge in machine learning, deep learning, statistics and optimization to perform my duties as a Data Scientist at maximum efficiency. I am flexible and available to work just about any time starting immediately. SKILL EMPHASIS
• Deep Learning Algorithms (Convolutional
LSTM Neural Networks, Autoencoders,
Generative Adversarial Networks etc. using
Keras Python)
• Machine Learning (Linear and Logistic
Regression, Neural Networks, etc.)
• Statistics and Probability, Linear Algebra
• Machine learning Certification (Stanford)
• SQL for data science Certification
• Optimization techniques and Algorithms
• Data Imputation (MICE, GAN, KNN)
• Python, Python NumPy, Pandas Library
• AGILE Methodology
• MATLAB, Octave
Professional Experience and Projects
Data Mine Project at Caterpillar Inc. (CAT Digital) Aug 2020 – Apr 2021
• Worked on a project involving data imputation in a multi variate time series data frame and critical event prediction modelling using python.
• Was actively involved in data profiling and Exploratory Data Analysis (EDA), built correlation matrices for the entire data set.
• Implemented MICE, KNN and GAN to impute data using python in AWS.
• Worked on imputed model validation and finalized the model. Certification in Machine Learning (offered by STANFORD) May 2020 – July 2020
• Programmed one-vs-all logistic regression and neural networks to recognize hand-written digits. Similarly, implemented the back-propagation algorithm for neural networks and applied it to the task of hand-written digit recognition.
• Used support vector machines (SVMs) to build a spam classifier.
• Implemented the K-means clustering algorithm and applied it to compress an image.
• Implemented the anomaly detection algorithm and applied it to detect failing servers on a network.
• Scored an aggregate of 98% in the certification course. Deep Learning Projects Aug 2020 – Dec 2020
• Implemented a CNN to classify fashion articles from the Fashion-MNIST dataset.
• Implemented a CLDNN to classify the modulation of time-series data.
• Implemented various types of autoencoder neural networks using the MNIST Database of Handwritten Digits.
• Implemented a GAN capable of generating MNIST-like hand-written digits.
• Implemented attacks (Fast Gradient Sign Method, Projected Gradient Descent, Carlini & Wagner Attack and DeepFool) and defenses (Denoising Autoencoders, Adversarial Training, Dimensionality Reduction and Adversarial Detection) on a classifier trained to classify hand-written digits from the MNIST database and tested their accuracy.
Wing Optimization for DBF 2018 Competition Aircraft Aug 2019 – Dec 2019
• Optimized the wing of the DBF competition aircraft in MATLAB using multi-disciplinary optimization algorithms.
• Multi objective problem was solved using Goal attainment and sequential quadratic programming
(SQP).
• Pareto front was developed and the results obtained were verified with practical flight test results.

"
,
"Data Analyst, Data Scientist, Machine Learning","
SONALI SHINTRE
adms0b@r.postjobfree.com https://github.com/sonalishintre www.linkedin.com/in/sonalishintre +1-518-***-**** New York, NY

Looking for Data Science/Software position with solid background in Machine Learning and Python, R programming skills.

EDUCATION:
The City College of New York, New York Aug/2018 - June/2020 Master’ s in Data Science and Engineering
Oxford College of Engineering, Bangalore, India June/2013 - Sept/2017 Bachelor of Engineering in Information Science and Technology

TECHNICAL SKILLS:
• Programming Language: Python, SQL, R
• Data Visualization: Tableau, Power BI
• Packages: Scikit-Learn, Pandas, NumPy, Matplotlib, Seaborn, NLTK, Keras, TensorFlow, MNE-python
• Machine Learning: Statistical analysis, supervised/unsupervised algorithms, hypothesis testing, classification, regression, clustering, decision tree, SVM, random forest, NLP, deep learning, CNN

EXPERIENCE:
Data Analyst, Institute for Ultrafast Spectroscopy and Lasers, CCNY Oct/2020 – Present
• Utilized Microsoft Excel to categorize budget reports into detailed pivot table to develop financial strategy
• Participated in collecting, organizing and interpreting data along with researcher. Assisted students in laser labs
• Established efficient workflow processes, oversaw office inventory activities, including ordering and requisitions
• Developed complex reports, assist in writing the technical reports, papers, proposals.

Data Analyst Intern, Supplycopia July/2020 – Oct/2020
• Implemented data pre-processing using R for data cleansing containing over million entries and generating valuable insights from the clean dataset. Translating numbers and facts to inform strategic business decisions
• Developed test cases and SQL test scripts based on detail data analysis, functional design and ETL specifications
• Optimized data collection procedures and generated reports. Web Scraping from a given healthcare websites
• Used Python, R, SQL, Excel, Scrapy and other python libraries

CodeLabs Intern, CodeDay July/2020 - Aug/2020
• Developed stack ML model to classify microscopic wood images, deep learning techniques like Convolutional Neural Networks (CNN) for texture classification. Model performed well with the accuracy of 94.6% on the test data
• Selected as one of the top 5 data science projects. Worked in team of 3 with Mr. Keith Callenberg as mentor

Administrative Assistant, Institute for Ultrafast Spectroscopy and Lasers, CCNY Sept/2018 - May/2020
• Content editor for IUSL website, published Newsletters, Annual reports, prepared and organized records
• Prepared financial and administrative reports, placed orders, organized departmental mail, answered phone calls

PROJECTS:
Steel Defect Detection: (Used Python, pandas, Scikit-Learn, Classification/regression algorithm, TensorFlow)
• ML model to predict location and type of defects in steel sheets produced during manufacture. Performed Binary classification, multiclass classification to finds cracks, type of defects. UNET model to find location of defect on sheet EEG Data Analysis, Visualization

and Image Classification: (Used Python, MNE-Python, TensorFlow)
• Understanding the behavior and cognitive ability of the human brain, using BrainVision Analyzer. visualize spatial distribution of brain activity. Published the paper (under review).

Zebra Crossing Detection and Localization: (Used MATLAB)
• Identify crosswalk for visually impaired people. Preprocessing, equalization, thresholding, find contours, Hough Transform. Constructed angle profile to infer orientation with respect to crosswalk

CERTIFICATIONS:
• Udacity Nano Degrees: Machine Learning, SQL (In process)
• Coursera: IBM Data Science Professional, Google Data Analytics, AWS with Machine Learning

"
,
Data Scientist Hadoop,"
AMOL B
Phone: +1-980-***-****
Email:admp98@r.postjobfree.com

Professional Summary:

7+ Years of IT experience with multinational clients performing Data Modeling, Statistical Modelling, Data Mining, Data Exploration and Data Visualization of structured and unstructured datasets.
Experience in data-wrangling, loading in Big Data platforms such as Apache Spark, Kafka working efficiently through SQL server after doing enough Data Extracting process from several sources, transforming in transit, and loading into relevant platform to perform actions.
Experience in Developing Spark applications using Spark - SQL in Databricks for data extraction, transformation and aggregation from multiple file formats for analyzing & transforming the data to uncover insights into the customer usage patterns.
Expertise on using Machine Learning and Deep Learning Techniques in Python & R (R Studio) such as Ranking, Linear Models, Polynomial, Support Vector, LSTM, Random Forest, Clustering, classification models such as Logistic Regression, Decision Trees, Support Vector Machine, and KNN.
Experienced in Building Recommendation engines using Association rule, collaborative filtering, and segmentations.
Expertise in Advanced Ensemble Techniques Stacking, Blending, Bagging, Boosting and models Random Forest, Gradient Boosting and Extreme Gradient Boosting etc.
Expertise of interactive ML tools such as TensorFlow and Caffe, PyTorch and Theano, and expertise in using strong Coding Platforms such as Spyder, Jupyter Notebook, R Studio offered by Anaconda Navigator as well as Google Colab.
Experience in text mining and topic modelling using NLP & Neural Network, tokenizing, stemming, and lemmatizing, tagging part of speech using Text Blob, Natural Language Toolkit (NLTK) and Spacy while building Sentiment Analysis.
Experience in AI & Deep Learning techniques such as Convolutional Neural Network (CNN) for Computer Vision, Recurrent Neural Network (RNN), and Deep Neural Network with applications of Backpropagation, Stochastic Gradient Descent (SGD), Long Short-Term Memory (LSTM) and Continuous Bag of words, Text Analytics etc.
Proficient in using PostgreSQL, Snowflake, Microsoft SQL server and MySQL to extract data using multiple types of SQL Queries including Create Table, Join, Conditionals, Drop, Case etc.
Extensive knowledge in MLOps which aims to deploy and maintain ML systems in production reliably and efficiently.
Skilled creating executive Tableau Dashboards for Data visualization and deploying it to the servers.
Proficient in Data Visualization tools such as Tableau and Power BI, Big Data tools such as Hadoop HDFS, Spark (PySpark), MapReduce.
Experience using Matplotlib and Seaborn in Python for visualization and Pandas in Python for performing exploratory data analysis.
Experience in Web Data Mining using Python’s NLTK, ScraPy, Beautiful Soup packages and REST APIs along with working knowledge of Natural Language Processing (NLP) to analyze text patterns.

Areas of Expertise:
Big Data Ecosystems: Hadoop, MapReduce, HDFS, HBase, Hive, Sqoop, Cassandra
Programming Languages: Python, Java8, Scala, UNIX, LINUX, Shell scripting
Framework: Apache Hadoop, Apache Spark
Databases: Oracle, SQL, PostgreSQL, MySQL, DynamoDB
Tools: Eclipse, JDeveloper, MS Visual Studio, Docker, AirFlow, JIRA
Cloud: AWS, GCP
Version Control: GIT, SVN

Professional Experience:

Client: Bank of America- Charlotte, NC May 2019 – Present
Role: Data Scientist

Responsibilities
Extracted the data from hive tables by writing efficient Hive queries.
Performed preliminary data analysis using descriptive statistics and handled anomalies such as removing duplicates and imputing missing values.
Application of various machine learning algorithms and statistical modeling like decision trees, text analytics, natural language processing (NLP), supervised and unsupervised, regression models, neural networks, deep learning, SVM, clustering to identify Volume using scikit-learn package in python, Matlab.
Performed data cleaning and feature selection using MLlib package in PySpark and working with deep learning frameworks such as Caffe, Neon etc.
Developed Spark/Scala, Python, R for regular expression in the Hadoop/Hive environment with Linux/Windows for big data resources. Used clustering technique K-Means to identify outliers and to classify unlabeled data.
Use Principal Component Analysis in feature engineering to analyze high dimensional data.
Implemented end-to-end systems for Data Analytics, Data Automation and integrated with custom visualization tools using Python, R, Mahout, Hadoop and MongoDB.
Worked with Machine Learning algorithms such as decision trees and random forest.
Used Spark Data frames, Spark-SQL, Spark MLLib extensively and developing and designing POC's using Scala, Spark SQL and MLlib libraries.
Developed various Tableau Data Models by extracting and using the data from various sources files, DB2, MongoDB, Excel, Flat Files and Big data.
Designed and implemented end-to-end systems for Data Analytics and Automation, integrating custom visualization tools using R, Tableau, Power BI
Participated in all phases of Data mining, Data-collection, Data-Cleaning, Developing-Models, Validation, Visualization and Performed Gap Analysis.
Implemented Classification using supervised algorithms like Logistic Regression, Decision trees, KNN, Naive Bayes.
Designed both 3NF data models for ODS, OLTP systems and Dimensional Data Models using Star and Snowflake Schemas.
Used AWS data pipeline for Data Extraction, Transformation and Loading from homogeneous or heterogeneous data sources and built various graphs for business decision-making using Python matplot library
Implemented ETL process wrote and optimized SQL queries to perform data extraction and merging from SQL server database.
Built data pipelines from multiple data sources by performing necessary ETL tasks. Performed Exploratory Data Analysis using R and Apache Spark.
Designed the data flow for the collapse of 4 legacy data warehouses into an AWS Data Lake
Built a data lake as a cloud-based solution in AWS using Apache Spark and provide visualization of the ETL orchestration using CDAP tool.
Created SQL tables with referential integrity and developed queries using SQL, SQL PLUS and PL/SQL.

Client: ADP, Pacedona, CA Sept18- May 2019
Role: Data Scientist

Responsibilities:
●Worked on data that was a combination of unstructured and structured data from multiple sources and automate the data cleaning using Python scripts. Performed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database.
●Created Data Quality Scripts using SQL and Hive to validate successful data load and quality of the data and extracted data from HDFS and prepared data for Exploratory Data Analysis using data munging.
●Developed data analysis by using Hive to retrieve the data from Hadoop cluster, SQL to retrieve data from Oracle database and used ETL for data transformation.
●Created multiple custom SQL queries in Teradata SQL Workbench in AWS, cloudera platform to prepare the right data sets for Tableau dashboards. Queries involved retrieving data from multiple tables using various join conditions that enabled to utilize efficiently optimized data extracts for Tableau workbooks.
●Used Spark and H2O together with Flow UI to perform various deep learning tasks implementing classification and regression algorithms and used Amazon Sage maker to installation fashions and to track modifications in deploying the fashions git is used.
●Used elastic search engine to store, search, and analyze big volumes of data and powered applications that had complex search features and requirements and used in AWS platform to deploy, operate, and scale Elasticsearch clusters.
●Performed pre-processing of data such as handling missing values, dealing with outliers and extreme values also making necessary transformations of the variables for the data to follow normal distribution.
●Implemented a Singular Value Decomposition (SVD) collaborative filtering algorithm to recommend products and services to users.
●Leveraged the Python package SciPy to perform Singular Value Decomposition (SVD) on User-Item matrices to make recommendations.
●In addition to SVD, spearman correlation and Cosine similarity were utilized to compare the performance.
●Explored a K-Nearest Neighbors approach for building the recommender engine.
●Enhanced Key Performance Indices (KPI’s) such as Click through Rate (CTR) to build ratings.
●Built custom algorithms and ensembles to make recommendations.
●Experience processing clickstream data to build ratings.
●Used Power BI for evaluating and improving existing BI system’s, developed and executed database queries and conduct analyses.

Lavie (Bagzone Lifestyles), India Jan ’14 –Dec’15
Data Analyst

Description: Lavie established itself as one of the best handbag brands in India with its first bag collection that was showcased in 2010. A stylish footwear collection under the brand name Fé Lavie was shortly launched thereafter. I was working as a junior data analyst to develop a machine learning model to predict the Sales of fashion products to be sold in the next season. The model used information about the future trends present in web pages and with the data present in the previous year’s sales history. For the creation of this model, we used techniques of text mining and data mining.
Responsibilities:
●Created and updated SQL tables, database, stored procedures, and queries to modify and/or create reports for respective business units and used Mongo DB to create queries.
●Performed Data visualization and Designed dashboards with KIBANA, and generated complex reports, including Charts, Summaries, and Graphs to communicate the findings to the team and stakeholders.
●A user-generated data extraction program was developed to extract potentially useful information from web pages.
●Performed Data Cleaning, Data Visualization, Information retrieval, Feature Engineering using Python libraries such as Pandas, NumPy, Scikit-learn, Matplotlib and Seaborn.
●Feature Engineered raw data by doing imputation, normalization and scaling as required on the data frame. Converted categorical variables to numerical values using Label Encoder for EDA and readability by the machine learning models.
●Performed univariate, bivariate, and multivariate analysis to check how the features were related in conjunction to each other and the risk factor.
●Applied PCA to reduce the correlation between features and high dimensionality of the standardized data so that maximum variance is preserved along with relevant features.
●Built machine learning models for Regressions based on Decision Trees, Support Vector Machine and Random Forest to predict the different risk levels of applicants and used Grid Search to improve the accuracy over the cleaned data.
●Proactively identified opportunities to automate time and resource intensive procedures associated with data validation and transformation using Python, Azure Data Factory.
●Evaluated the model’s performance using various metrics like coefficient of determination (R2), RMSE and Cross Validation to test the models with different batches of data to optimize .

Education:

B.Sc in IT
Mulund College Of commerce,Mulund(w), Mumbai,2012

"
,
Data Analyst Engineer,"
Karthik Anil Kini
Data Analyst Data Scientist Data Engineer
ML Engineer Software Engineer
**** * ******* ***, #***
Chicago, IL

Phone: 312-***-****
Email: admphf@r.postjobfree.com admphf@r.postjobfree.com
LinkedIn: https://www.linkedin.com/in/karthik-kini/
GitHub: https://github.com/karthik-kini-3
Education:
MASTER’S IN COMPUTER SCIENCE AND MATHEMATICS, ILLINOIS INSTITUTE OF TECHNOLOGY, CHICAGO
AUGUST 2019 – MAY 2021
Cumulative GPA: 3.91/4.0
BACHELOR’S IN TECHNOLOGY, COMPUTER ENGINEERING, SARDAR VALLABHBHAI NATIONAL INSTITUTE OF TECHNOLOGY, SURAT
JULY 2015 – MAY 2019
Cumulative GPA: 8.3/10.0

Summary:
Data Science Enthusiast with a hands-on and professional experience of 2 years in Computer Science and Statistics. Actively seeking full-time Data Science opportunities.
Skills:
oProficient in Machine Learning, Computer Vision, Deep Learning, Statistics, Competitive Coding.
oPython (Proficient), R (Proficient), Microsoft Azure (Proficient), Amazon Web Services (Intermediate), Google Cloud Platform (Proficient), Hadoop (Intermediate), Apache Spark (Intermediate), Snowflake (Intermediate), SAP (Intermediate), Microsoft SQL Server (Proficient), C (Proficient), C++ (Proficient), Core Java (Proficient), Octave (Proficient), MySQL (Proficient), JavaScript (Intermediate), AngularJS (Intermediate), Firebase (Intermediate), Microsoft Office (Proficient), Git (Proficient), Tableau (Intermediate), PowerBI (Intermediate).
Projects/Publications:
Masters
oDatabase System [project]: Implemented the following components in C:
Storage Management: It is capable of reading blocks from a file on disk into memory and writing blocks from memory to a file on disk.
Buffer Management: It manages a buffer of blocks in memory including reading/flushing to disk and block replacement (flushing blocks to disk to make space for reading new blocks from disk)
Record Management: It allows for navigation through records and inserting and deleting records.
B+ Tree Index: The index should be backed up by a page file and pages of the index should be accessed through the buffer manager.
oDivvy Bike Analysis [project]: Worked on four hypotheses for the group project:
Predicted the need for bikes according to the season in Chicago to ensure that the customers do not have to wait for more bikes, by ensuring the increase or decrease in the number of bikes at the docking station to maintain the profit. Implemented various regression models in R.
Predicted which station needs more docks based on the traffic (to and fro) at the station keeping in mind the revenue generated v/s the cost required to build more docks. Implemented the KNN Classification, Logistic Regression, and Naive Bayes Classification algorithms.
Identified the need and feasibility (revenue) for new stations between 2 stations for providing parking spaces at heavy traffic stations ensuring that divvy does not lose customers, based on the parking docks unavailability. Implemented the KNN Classification, Logistic Regression, and Correlation applying multi regression models.
Identified the time/season so that the bikes can be put for maintenance so that the Divvy company does not lose any revenue by putting the bikes into maintenance. Implemented the Decision Tree Algorithm, the Random Forest Algorithm, and the Support Vector Machine algorithms.
oWater Height Predictor [project]: Predicted the height of oscillating water from a sprinkler in Python using TensorFlow and Keras. Implemented a linear model, Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), and deep RNN on a synthetic time series data.
oCustom DLIB shape predictor [project]: Prediction of facial landmarks (face shape and eyes) on video stream in Python using an ensemble of regression trees on iBug 300-W dataset. I have used the AWS DeepLens system to record the live stream and created Docker containers to host it in AWS SageMaker. Performed hyperparameter tuning to identify the best hyperparameters for the predictor model.
oGitHub Issue Tracking: Extracted Issues from a Github repository and predicted for all types of issues using TensorFlow, FBProphet, and Statsmodel (ARIMA) APIs.
Bachelors:
oDisaster Management and IoT [project and publication]:
Real-Time Collaborative Processing for Event Detection and Monitoring for Disaster Management in IoT Environment.
Implemented the system in Python, AngularJS, Firebase using Arduino, Raspberry PI (Model 3B), Gas Sensors (CO2 and NO2).
Published our approach and its architecture at 10th ICCCNT and IEEE.
oText Recognizer and Translator [project]: Implemented a web-based tool using Google Cloud Vision and Google Translate API in python flask (backend) and JavaScript (front end) to recognize texts and translate them into any language based on requirements.
oImage Compression tool [project]: Implemented a tool using discrete cosine transform implemented in MATLAB.
oGrade Prediction tool [project]: Implemented a tool to predict final grades in Python and JavaScript using Support Vector Machines.

Experiences:
APRIL 2021 – MAY 2021, Data Analyst (Visualization) at University of Chicago
oWorked with Biomedical Big Data in the form of clinical data, sequence data, and the results of analysis at the Center for Research Informatics.
oCreating a web app to visualize different statistical correlation results.
oHelped the researchers to identify the trends between clinical and experimental data from different databases on a single platform.
oTech Stack: ReactJS (frontend) and Python Flask API (backend).
oCharts: implemented using ApexCharts.JS, React-Table, and Cytoscape.JS
FEBRUARY 2021 – MARCH 2021, Data Analyst at DATEN Solutions
oBuilding end-to-end scalable data science solutions.
oImplementing solutions in Microsoft Azure, Amazon Web Services, Databricks, Datarobot, and Dataiku.
oWorking on the following projects: Sentiment Analysis (LDA), Customer Segmentation (RFM), and Sales Forecasting (ARIMA).
oCreating CI/CD Pipelines using Azure DevOps, Jenkins, and TravisCI.
oCreating Data Migration and Data Validation Scripts in Snowflake.
AUGUST 2020 – DECEMBER 2020, Data Analyst at Accenture
oWorked in pairs throughout the term.
oIdentified biomarkers responsible for Alzheimer’s Disease from PubMed articles using ad-hoc spaCy matcher & regular expressions approach, and topic modeling using LDA (Latent Dirichlet Allocation).
oGenerated dummy COVID-19 patient dataset simulating 2 states.
oWorked on an AI tool to extract key attributes from PDFs and Images using OCR and NLP in Python.
oAnalyzed R&D expenditures, collaborations investment, net income/earnings for top 10 pharma companies.
MAY 2020 – JULY 2020, Data Engineer Intern at DATEN Solutions
oImplemented data warehouses for clients using Snowflake, SAP BODS, and Microsoft BI Tools.
oPerformed ETL (Extract, Transform, and Load) operations to create data cubes and data models.
oHelped to create dashboards for data analysis to get in-depth insights.
MAY 2018 – JULY 2018, Software Developer Intern at UGAM Solutions PVT LTD.
oA machine learning and image processing group project which identifies and analyses different brands on eCommerce advertisements (images & videos) implemented in Python.
oImplemented Multiscale Template Matching and T-distributed Stochastic Neighbor Embedding (t-SNE) approaches in Python.
oEliminated the manual identification and analysis of data and saved working hours significantly.
Achievements and Extra-Curricular Activities:
oData Science Capstone by Harvard University from edX.
oSequence Models for Time Series and Natural Language Processing by Google Cloud on Coursera.
oMachine Learning by Stanford University from Coursera.
oGoogle Cloud Platform Big Data and Machine Learning Fundamentals by Google Cloud from Coursera.
oDeep Learning with Keras and TensorFlow in Python and R by Start-Tech Academy from Udemy.
oSoftware Engineering Virtual Experience by JPMorgan Chase & Co from The Forage.
oMachine Learning A-Z™: Hands-On Python & R In Data Science from Udemy

oExecutive Member, ACM NIT Surat for 2015-16.
oParticipated in coding events on HackerRank, HackerEarth, LeetCode, CodeChef, Codeforces.
oAttended DotSlash, BuildChicago Hackathon, and HackerRank Virtual Career Fair.
o3D puzzle (e.g., Rubik’s Cube) solving enthusiast.
oTechnical writer, TechNews Magazine @ IIT [2020].
oCPS Test Proctor, Spring 2021.

"
,
"ThingWorx, IoT, Data Science, Machine Learning, Java, Python, PLM","
Contact
763-***-**** (Mobile)
dei6j3@r.postjobfree.com
www.linkedin.com/in/sajidpatel
(LinkedIn)
github.com/sajidpatel6 (Other)
Top Skills
Thingworx
Internet of Things
Enterprise Software
Languages
English (Professional Working)
Hindi (Native or Bilingual)
Marathi (Native or Bilingual)
Certifications
Thingworx
Core Java, C, C++
Unix Programming
Machine Learning Implementation
Software Security for Developers
and Architects
Sajid Patel
Leader IoT Data Science / ML / AI VR / AR PLM Digital solutions
Greater Cleveland
Summary
# Over 26 years of progressive experience in data science, software development, and performance engineering.
# Leader with hand-on-expertise. Autodidact.
# Full stack development and analysis of Enterprise applications and ThingWorx/IoT applications.
# Diverse and enriching experience
- ThingWorx Analytics and Machine Learning(scikit-learn, Keras, TensorFlow, ...).
- Thingworx: Development of Mashups, custom Edge agents, and TWX extensions. Certified Thingworx Developer. Development of several IoT and integration apps.
- Kepware/Industrial connectivity
- Development and maintenance of multiple enterprise applications like Windchill, Service Parts Management, Teamcenter Enterprise, etc.
- Multiple programming languages: JavaScript, Python, R, Java, C, C
++.
- Worked in Product Development/R&D as well as Software Services/consulting industry.
# Involved in several performance engineering projects for Windchill
& InService.
- Worked closely with feature development teams to enhance/track performance.
- Early involvement in product development to ensure high-quality performant features development.
- Development of custom automated load generators, generate and load huge datasets, identify performance and scalability issues, analyze and recommend solutions and tune Enterprise applications.
# Executed product development projects for SPM/Windchill.
- Performance analysis and tools/feature development for Windchill.
- Development and maintenance of several features and products using Java/J2EE technologies.
Page 1 of 7
# Continuous education on new technologies, platforms, best practices, etc.
# H1B visa with approved I-140.
Keywords:
Java, J2EE, Javascript, Python, C++, Spring, Hibernate, REST, SOAP, XML, Agile Scrum, Oracle, SQL Server, PostgreSQL, Docker, Thingworx, Thingworx Manufacturing Apps, Kepware, Windchill
Pandas, NumPy, scikit-learn, Matplotlib, seaborn, Tensorflow Data Science, Machine Learning (Shallow/Deep Learning) and Artificial Intelligence
Weka, Knime, RapidMiner
AzureIOT, ESP8266, STM32, Arduino development, etc Experience
Rockwell Automation
Manager Architecture IoT Machine Learning/Analytics AR/VR/MR August 2019 - Present (1 year 10 months)
Cleveland/Akron, Ohio Area
- Closed Loop Operating System [CLOS] (Scheduling, Engineering Feedback
& Staffing) suite of applications development to generate optimal schedules, operator assignment, and analytics of captured data to provide feedback to the Engineering team.
- developing in-house solutions that leverage optimization algorithms like Hungarian Algorithm, Simulated Annealing (single solution metaheuristics), Genetic Algorithm (population-based metaheuristics), and ML models.
- Exploratory data analytics and use case grooming for advanced data analytics (ML)
- CLOS is developed on the Thingworx platform with an optimization module integrated via custom Edge microservice and allows users to visualize the output of the optimization algorithm, set/modify the configuration, and also override the calculated solution as needed. The changes are communicated back to SAP and MES system using REST APIs.
- Machine learning project to predict defects in the solder paste application. Pipeline developed in FactoryTalk Analytics Edge and prediction scoring performed using ThingWorx Analytics microservice. Involved components Page 2 of 7
- Apache Kafka, Mosquitto, FTA Edge, ThingWorx Analytics, Python Scikit, classification models, etc.
- Lead a team of 8 developer/architects
- Established new team and developed processes including use of Jira and Confluence.
- Implementation of ThingWorx applications/dashboards for reporting/investing data, optimizing, scheduling, and applying statistics, ML, and algorithms to convert data into actionable information.
- Development of ThingWorx extension for Automatic login for TV screens
- Optimization and stabilization of Thingworx production servers
- Setup of Git environment for ThingWorx, Vuforia, and FactoryTalk Analytics projects
- Setup Active Directory Domain Services for ThingWorx Kalypso
Lead Applications Architect Data Scientist Manager November 2018 - August 2019 (10 months)
Rochester, New York Area
Designation: Solutions Architect
- Predictive and prescriptive analytics project for a tire manufacturing factory. Collect data from sensors on tire manufacturing machine; predict potential defects & prescribe new settings for the machine using machine learning models trained from historical data. The objective was to minimize defects as well as a reduction in non-operational time. The machine/sensor data retrieved using Thingworx Industrial Connectivity / Kepware KEPServerEX.
- Architect, design and develop role-based apps using ThingWorx platform that provides dashboards, charts, document search/view/download features. The data obtained from Windchill and other enterprise applications using REST web services.
- Development of custom REST endpoints (JAX-RS / RESTEasy) for Windchill. Also OData REST endpoints.
- ThingWorx Analytics solution to get deep insights into gathered data. Feature engineering / pre-model data preparation, ThingWorx Signals, Profiles and Analytics Models (Machine Learning).
- Preprocessing, feature selection, feature extraction, dimensionality reduction, model selection, etc. using Python scikit learn library.
- Design and customize Thingworx Manufacturing App with KepServerEX / Thingworx Industrial connectivity for a Natural Resources company. Page 3 of 7
- Developed Thing model with KepServerEX tag binding, Services, and Mashups.
PTC
10 years 1 month
Software Architect Principal Software Engineer
November 2016 - November 2018 (2 years 1 month)
Rochester, New York Area
Designation: Senior Technical Consultant
- Development and maintenance of IoT (internet of things) application -
'Performance Advisor for SPM' built on Thingworx platform (Java edge micro- server, extensions/services, mash-ups, etc.)
- Design, develop and enhance features for inventory optimization features in Servigistics Service Part Management (SPM)
- Migration to Java 11
- Performance engineering to enhance scalability and service level agreements.
- Architected automated performance test pipeline to track the performance of key application areas.
Software Architect Principal Software Engineer Senior Performance Engineer
January 2012 - November 2016 (4 years 11 months)
Greater Minneapolis-St. Paul Area
Designation : Senior Technical Consultant
- Design and develop IoT (Internet of things) applications built on the Thingworx platform (Java & C edge micro-server, extensions/services, mash- ups, etc.)
- Design, development, and maintenance of features for Windchill suite of products.
- R&D consultant for Windchill performance improvements (full stack).
- Architected automated performance test framework that sets up VMs, installs and loads performance data, runs performance tests and collects results. Used Jenkins, VMWare/VSphere, Selenium, and JMeter.
- Analyze software performance issues, identify solution/fix, training/mentoring development, and QA groups and developing performance tools and techniques.
Software Development Manager Principal Software Engineer Page 4 of 7
November 2008 - December 2011 (3 years 2 months)
Pune Area, India
Designation: Section Manager
- Design, development, and maintenance of features for Windchill suite of products.
- In addition, responsibility/ownership of 3 PTC products:
- Windchill Unified Workgroup Manager components key to interoperability between Windchill and ProEngineer.
- Workgroup Manager for CATIA v4 (CAD integration with PDM/PLM). Involved in major enhancements projects specifically targeted for a leading supplier to the automotive industry
- Windchill Gateway for IDEAS TDM adaptors for Windchill and I-DEAS TDM. This product is based on Windchill PLM Connector architecture. Mphasis, an EDS company
Senior Project Manager Solutions Architect
October 2007 - November 2008 (1 year 2 months)
Pune Area, India
- PLM Practice Lead for Teamcenter Enterprise and Windchill
- Lead a team of 15 for ongoing Maintenance of a Teamcenter Enterprise Implementation for a major Automotive Ancillary.
- Lead a team of 4 for Windchill re-hosting project for a US Enterprise.
- Participated in developing proof of concepts, artifacts, setting up PLM COE.
- Trained teams in Teamcenter Enterprise and Windchill. Infosys
Senior Project Manager Solutions Architect
May 2007 - October 2007 (6 months)
Mysore area, India
- Lead a team of 13 for Teamcenter Enterprise Implementation (using CPG & Apparel Accelerator solutions) for an Apparel/Retail Customer. Requirements / Gap Analysis, implementation (code changes) as well as configuration for Conference Room Pilot.
- Installation and configuration of Teamcenter Enterprise and Teamcenter Project environment with single sign-on.
- Gap Analysis for a Retail Customer planning to implement FlexPLM.
- Trained the team in Teamcenter Enterprise.
PTC
Page 5 of 7
Principal Software Engineer
March 2005 - May 2007 (2 years 3 months)
Pune Area, India
- Responsible for development and maintenance of Workgroup Manager for CADDS & CATIA v4 (CAD integration with PDM/PLM). Workgroup Managers are swing based Java applications that are used for 3rd party CAD data management for Windchill.
- Responsible for development and maintenance of Wildfire server-side components key to interoperability between Windchill and Creo using Java, J2EE, Javascript, etc.
Tata Consultancy Services Limited
Solutions Architect Senior Technical Lead
June 1998 - January 2005 (6 years 8 months)
Bengaluru Area, India
- Design, development, and customization of Teamcenter Enterprise 3.2 using C, C++ and Java.
- Development of STEP (ISO 10303) translator for PDM data using AP203 & AP232 application protocols.
- Executed several CAD & PDM customization projects and in different phases like analysis, design, implementation. Ufunc API's (Unigraphics Customization), CATGEO API's (CATIA customization)
- Developed Unix scripts for automating routine development tasks Tata Johnson Controls Automotive Private Limited
Design Engineer
November 1995 - May 1998 (2 years 7 months)
Pune Area, India
Solid and Surface modeling of automotive seating systems using I-DEAS and Unigraphics.
CAD data administration, CAD data translation (I-DEAS format to CATEXP, PARA-SOLID, IGES etc. and vice-versa) etc.
Design of components of automotive seating systems. Automotive seating system is an assembly of ergonomically and aesthetically designed components including recliners, tracks, lumber supports, sheet metal parts, foam pads, etc.
Bench-marking of competitor's automotive seating systems. Built prototypes of automotive seating systems from concept designs. Trained Engineers in I-DEAS solids as well as surface modeling. Page 6 of 7
Education
Gogte Institute of Technology
B.E., Mechanical Engineering · (1991 - 1995)
RLS High School & Junior College
12th, Physics, Chemistry, Mathematics, General Civil Engineering · (1989 - 1991)
Milagris High School
10th, State Board · (1982 - 1989)
Page 7 of 7

"
,
IT Specialist II (Data Development & Reporting),"
JOSIAH BRADY
PROFESSIONAL SUMMARY
Aspiring data scientist with a passion for data visualization, data ingestion/manipulation and machine learning. Experienced in statistical modeling, data analytics, data wrangling and pipelining/ETLing, programming development, and facilitating IT and Business partnerships.
WORK HISTORY
IT Specialist II, 10/2017 - Current
ENTERGY SERVICES, LLC, THE WOODLANDS, TX
Innovated several C# .NET Framework apps that execute VBA macros to gather and present real-time data; collaborated with business department.
Implemented deep learning hierarchical clustering model in R to find similarities between different power plants, embedded into R Shiny web application Engineered pathway for GIS data using spatial SQL joins to transport attributes from Oracle database into OSISoft PI AF data infrastructure tool via .NET (C#) programming.
Organized 250+ million rows of OSISoft PI time series data with R and condensed to ~5-6 million rows of digestible data.
Columnist/Analytics Research Contributor, 11/2016 - 06/2018
FANSIDED [NYLON CALCULUS]/DEF PEN, NBA, New Orleans, LA
Created interactive R Shiny web applications for O/U totals & 7-game series projections with multinomial logistic regression.
Communicated professionally in articles to interpret data and embedded Tableau diagrams that could be easily understood.
Performed text mining with Twitter analytics for word clouds and sentiment analysis.
PROJECTS
Korean Basketball League Data Analytics Project
Built analytics database for Korean Basketball League (KBL) data in PostgreSQL (w/ psycopg2 library in Python).
Used Selenium w/ Python to extract raw text & tables, feature engineered Korean language data according to modern basketball analytics with Pandas & Numpy, and hosted database with Amazon Web Services EC2 instance.
Wrote R Shiny application with SQLite implementation of database for user interactivity and data visualization: https://steadylosing.shinyapps.io/KBLGameAnalyticsApp
EDUCATION
Bachelor of Science, Systems Engineering Applied Science, 05/2016
Washington University in St Louis - St Louis, MO

Houston, TX 77022

713-***-****

admpeu@r.postjobfree.com
SKILLS
Data Wrangling
Database Development
Data Visualization
Parameterized Reporting
Technical Writing
R (& Shiny/Markdown)
Python
.NET Framework, C#
VBA
PostreSQL
AWS EC2
OSISoft PI System
Oracle SQL
Microsoft SQL
Power BI
Tableau
HTML/CSS Excel/Google Sheets
Languages: Korean – Intermediate
PUBLICATIONS
Medium, Explaining hierarchical clustering deep learning methodology for predicting NBA win%: https://medium.com/@SteadyLosing/revisiting-historic-teams-win-projections-with-interactive-sheet-cf03e2bddf86
Nylon Calculus, Predicting playoff berth success with Principal Components Analysis: Nylon Calculus: Predicting NBA Playoff berths and similar scenarios: https://fansided.com/2017/09/19/nylon-calculus-predict-nba-playoff-berths-similar-scenarios/
Def Pen, Twitter Wordcloud Sentiment Analysis: https://defpen.com/2017-18-nba-season-anticipation/
.

"
,
Data Scientist/Analyst,"
Mohamed Rakha
**** *. ***** **. *** A, Hawthorne, CA, 90250 310-***-**** admpa8@r.postjobfree.com Education
UNIVERSITY OF CALIFORNIA, BERKELEY GRADUATED: MAY 2021
· Major: Data Science (Minor: Energy Engineering) GPA: 3.43
· Notable coursework: Foundations of Data Science, Principles & Techniques of Data Science, Data Structures, Data Analysis of Environmental Systems, Computer Science for Scientists and Engineers, Designing Information Devices and Systems, Bayesian Data Analysis and Machine Learning, Data Mining and Analytics, Data Inferences and Decisions. Experience
IPG MEDIABRANDS (*HALTED DUE TO COVID-19) MAY 2020 – MAY 2020
· Data Science Intern handling large-scale media data to make decisions on advertisements using Python and MS Office.
· Communicated sales and marketing solutions and decisions with company clients such as Trader Joe’s, Amazon, etc. ECODATALAB JAN 2019 – DEC 2019
· Data science/research intern for climate non-profit and member of Mapping Team managing vehicle and census data.
· Utilized data science techniques (Regression Analysis) to scale up climate and sustainability solutions.
· Manipulated and visualized vehicle and energy data in California using Python, Tableau, R and R Studio (Shiny app).
· Communicated with California DMV to release vehicle data type on the city, county, and zip code level, for visualization.
· Promoted to Mapping Team Leader in Summer 2019, taking complete control of vehicle and energy expenditure data.
· Revolutionized data management by storing data of all seven teams in an SQL server for easier access and use.
· Communicated findings with San Francisco and Alameda County for insight on future climate change policy. CLASSROOM MANAGEMENT PROGRAM MAR 2018 – APR 2021
· Managed the classrooms in the main buildings on the UC Berkeley campus through the Office of the Registrar.
· Ensured the safety and functionality of the technology within the buildings while maintaining events with large capacities. Volunteer/Extracurricular Experience
EGYPTIAN STUDENT ASSOCIATION AT BERKELEY APR 2019 - PRESENT
· Internal Vice President and founder of the club at Berkeley. Our goals are to allow students of Egyptian background to reconnect with their Egyptian heritage as well as non-Egyptians to experience Egyptian culture through food and fun! ISLAMIC CENTER OF HAWTHORNE JUN 2014 - PRESENT
· Revived the Youth Program at my local Mosque and took on the role of Youth Leader, as well as Youth Board Member.
· Lead and entertain youth boys ages six to twelve by setting up team-building activities during school breaks. Skills & Abilities
COMPUTER PROGRAMMING
· Proficiency in Python, Java, MATLAB, R, R Studio, and Tableau. Experience in SQL, C++, and Web Design (HTML, CSS, Javascript).
· Knowledge of Machine Learning techniques (Pandas, Scikit Learn, and various data-centric and ML modules), Jupyter Notebook environments, Microsoft Office applications, and Adobe Creative Cloud (Premiere Pro, Illustrator, Photoshop, etc. ABILITIES/LANGUAGES
· Large scale event organization/programming and project management. Fluent in English, Arabic and basic Spanish. Projects
SOUND, GO AWAY DEC 2019 – FEB 2020
· Performed A/B Hypothesis testing on experiment where we analyzed collected data from our study that tried to prove the relationship between reduced background noise and reaction time in children with Autism (Hyperacusis).
· Utilized Python and Jupyter notebooks to single-handedly develop the A/B algorithm to test our three experiments. MENA WATER NEEDS PREDICTOR SEP 2019 – DEC 2019
· Using data from different sources, my team and I created a predictor that takes in features such as water resource allocation and a state’s population to predict a county’s water need, specifically for countries in the MENA (Middle East, North Africa) region. We use Python 3 data science modules including Pandas, GeoPandas, SciKit-Learn, Numpy, etc. iCLIMATE MAR 2018 – MAY 2018
· Developed an app on MATLAB appdesigner that allows users to test carbon emissions up to the 2060s based on desired climate change mitigation strategies (wedges) based on S. Pacala and R. Socolow’s paper on stabilization strategies.

"
,
Scientist Data,"
DATA SCIENTIST
VICTORIA DEBEBE admoim@r.postjobfree.com
484-***-****

Schwenksville, PA
linkedin.com/in/victoriadebebe/
VPDeb

EDUCATION
Lambda School Aug. 2020 to Mar. 2021
Endorsement Data Science 2021
DATA SCIENCE: Python, SQL, Tesseract, Machine Learning, Web, Database Storage, Database Management, APIs, App Building, Textract BUSINESS MANAGEMENT: Business Development, Property Management, Contract Negotiation, Staff Management, Marketing and Advertising, Problem Resolution, Project Organization, Relationship Development
SKILLS
EMPLOYMENT
COALITION
Data Scientist Mar. 2021 to Current
•Creating Machine Learning model to input forms and extract necessary data fields for automating quotes for brokers KELLER WILLIAMS Blue Bell
Realtor Aug. 2017 to Current
•Reviewed property listings, interviewed potential clients, accompanied clients to properties and effectively communicated condition of sales
•Negotiated contracts with buyers and sellers to maximize customer savings
•Reviewed and executed confidential documents, contracts and disclosures
•Compared recently sold area properties to determine competitive market prices SDA SALES AND SERVICE Pottstown
Co-Owner Mar. 2008 to Current
•Performed system analysis, documentation, implementation and user support for platform transitions
•Coordinated statistical data analysis, design, and information flow
•Corrected any data entry error to prevent later issues such as duplication or data degradation
•Trained employees software for improved data management, monitoring effectiveness and suggesting improvements
•Handled online Advertising and website management COMMERCE BANK King of Prussia
Customer Service Professional Jan. 2004 to Nov. 2007
•Coordinated daily cash reconciliation in high-volume location
•Monitored customer behaviors and upheld strict protocols to prevent theft of assets
•Assisted customers with setting up or closing accounts, completing loan applications and signing up for new services
•Processed quarterly vault and ATM audits with zero error rate
•Continued education on current banking products and services through Commerce University program PROJECTS
COALITION Mar. 2021 to Mar. 2021
•Creating a ML model to take in forms, transcribe necessary information to give automated quotes to brokers who utilize paper forms and send in
•Working with Google's Textract, Document AI, AWS S3 Bucket and Snowflake SQL
•Created a parseable insurance application to automate their quoting process for manually submitted applications and presented to a panel of leadership STORY SQUAD Apr. 2021 to Current
• Learned Tesseract and how to train fonts into other languages.
•Created a 'Lightning Talk' to explain the process for those learning Tesseract
•Continue to give support to new members taking on additional tasks in Tesseract
•Creating algorithms for use in upcoming apps such as Classroom Rumble and Story Squad monster app PREDICTIFY Dec. 2020 to Current
•API Engineering, as well as front end and back end engineering GENDER PREDICT Nov. 2020 to Current
•Created App •Used data from Kaggle •Used Feature Engineering to adjust to my projects outcome • Worked with Python HTML/CSS JavaScript
•Continuing to adjust apps output and process.
CANNABIS CONSULT Jan. 2020 to Current
•Contributed to K-Nearest Neighbor model for App.
VOLUNTEERING
GIRLS ON THE RUN · Coach Jan. 2017 to Current
Schwenksville Pa
Coaching a set of Girls to feel empowered as females through a series of lessons and training for a 5K.

"
,
Data Analyst Hadoop,"
Summary
Around * years of experience in Data Scientist/Data Analyst with R, Python, Tableau and Machine Learning and Expertise in analyzing data and building predictive models to help provide intelligent solutions domain.
Good knowledge of Software Development Life Cycle (SDLC) including Requirements Analysis, Design Specification and Testing as per Cycle in both Agile and Waterfall methodologies.
Working knowledge of Google Analytics, Text Analytics and developing different Statistical Machine Learning, Data Mining solutions to various business problems and generating data visualizations using R, Python.
Working in Python data manipulation for loading and extraction as well as with python and R libraries such as ggplot2, Scikit learn, NumPy, Pandas, Matplotlib, Seaborn, SciPy, Keras, TensorFlow, NLTK, NLP, PySpark for data analysis and numerical computations and developing various machine learning algorithms.
Good understanding of AWS (Amazon Web Services), S3, Amazon RDS, Apache Spark, RDD, process and concepts.
Good knowledge of building and optimizing big Data pipelines, architectures, and Data sets Hadoop, Hive, HBase and Python.
Proficient in writing SQL queries for various RDBMS such as MySQL, Oracle and NoSQL databases such as MongoDB and HBase to handle unstructured data.
Good understanding of implementing best practices for Data Visualization and adept in utilizing tools such as Tableau, Power BI, MS Excel for creating appealing and interactive dashboards.
Proficient in in using source code change management and version control tool such as Git and experience in JIRA software for Plan, Track, Report and Release management.

Education

Master’s in Business Analytics Aug 2019 - Dec 2020
Kent State University, Kent, OH
Bachelor’s in Computer Engineering Aug 2013 - Jun 2017
Savitribai Phule Pune University, India

Skills

Methodology:
SDLC, Agile, Waterfall
Languages:
Python, R, SQL
Statistics & Machine Learning:
Classification, Regression, Decision Trees, Random forest, Naïve Bayes, Time Series, NLP
Packages:

ggplot2, Scikit learn, NumPy, Pandas, Matplotlib, Seaborn, SciPy, Keras, TensorFlow, NLTK, PySpark
Visualizations Tools:
Tableau, Power BI, Google Analytics
Cloud & Big Data Technologies:
AWS, Hadoop, Hive, HBase
Databases:
Oracle, MySQL, MongoDB
Version Control & Other Tools:
MS Office, Git, JIRA
Certification & Achievements
Google Analytics Certification (Dec 2020)
Data Science with Python – Data Camp Certification (Dec 2020)
Lifelong member of Beta Gamma Sigma Business Honor Society
Experience
Liberty Mutual Insurance, NJ May 2020 - Current
Data Scientist
Successfully managed projects using Agile development methodology.
Used Machine Learning to build various algorithms (Classification, Regression, Decision Trees, Random forest, Naïve Bayes, Time Series) models.
Used Python's multiple data science packages like Pandas, NumPy, matplotlib, SciPy, Keras, Scikit learn, TensorFlow and NLTK.
Ensemble Deep learning model, CRF model and NLP techniques to improve the model results.
Utilized Google analytics to understand the user traffic on the Target Website and prepared reports.
Used Hierarchical clustering to understand customer behaviors.
Implemented and tested the model on AWS S3, Redshift, Sage Maker with the best algorithm and parameters.
Installed and configured MongoDB multiple nodes Sharding, integrated with Hadoop cluster system.
Used R programming language to graphically analyses the data and perform data mining.
Used the version control tools like Git and Jira for ticketing and confluence for documentation.
Cognizant Technology Solutions, India Nov 2017- Jul 2019
Programmer Analyst
Developed, analyzed, examined Secondary authentication applications for the leading US Finance industry.
Researched to control and eliminate fraudulent transactions coordinating with 20 different teams globally. Collaborate cross-functionally with business analysts, developers and testers to explain new process transformations.
Queried SQL Server 2008, Oracle 11g / 12c, MySQL 5.0, MySQL 2012 structuring detailed visualization reports to convince stakeholders for better decision making. Presented 2 automation proposals to management, resulting in decreasing 27% of everyday manual tasks.
Automated manual works using Python 3.x and generates 2 business reports monthly, integrating 4 automation Scripts in the “Autosys tool” over Hadoop, making Job’s run automatically, leading to contributing to saving 23.71% team’s revenue utilization in 2018.
Formulated and evaluated biweekly ‘Threat-Metrics’ rules to minimize fraudulent hits on authentication databases. Assisted A/B testing for XML scripts and server migrations every 6 months. Experience with Data Analytics, Ad-hoc Reporting, Graphs, PivotTables and OLAP.
Designed 3 visually impactful Splunk Dashboards that extracts encrypted transaction logs applications and transforms raw data into meaningful, actionable information. Integrated 5 application reports in SharePoint for better profiling of technical requests and troubleshooting issues and tracked using JIRA. This prevented 17% of server failures, accomplishing risk management.
Mentored 4 newly hired employees, prepared for project tasks, and trained on unfamiliar systems.
Designed and developed new reports and maintained existing reports using Microsoft SQL Reporting Services (SSRS) and Microsoft Excel to support the firm's strategy and management and calculate yearly revenue to build KPI.
Oversaw team activities single-handedly as a critical resource for 4 months in times of crisis. Organized security awareness sessions for 5 teams to ensure data confidentiality as organizational activity.
Sigma Tech Solution, India Aug 2016- Sep 2017
Data Analyst
Worked under Waterfall methodology to deliver projects on a timely manner.
Optimized codes in Python to solve a variety of purposes in data mining and machine learning in Python.
Explored and analyzed the customer specific features by using Python (Matplotlib) and R (ggplot2).
Used Python based data manipulation and visualization tools such as Pandas, Matplotlib, Seaborn to clean corrupted data before generating business requested reports.
Developed dashboards using Power BI as per requirements.
Planned monthly and quarterly business monitoring reports by creating Excel Pivot Table summary reports to include System Calendars.
Used Power BI power query to extract data from external sources and modify data to a certain format as required for consumption in the dashboards.

Git hub: https://github.com/spadade

Srushti Padade
Data Scientist
Location: NJ Mobile: 701-***-**** admlut@r.postjobfree.com

"
,
Data Scientist,"
Ryan Wheat
admlpy@r.postjobfree.com 412-***-**** Palm Bay, FL www.linkedin.com/in/rwheat/
EDUCATION
University of Pittsburgh, Pittsburgh PA June 2018 – April 2021 Master of Science in Chemistry
GPA: 3.75/4.00
Florida Institute of Technology, Melbourne FL August 2014 – May 2018 Bachelor of Science in Chemistry
GPA: 3.94/4.00
SKILLS INVENTORY
Tools and Software: Linux (RedHat, Manjaro, Arch), Windows, Microsoft Office (Word, PowerPoint, Excel, Teams), Programming Languages (Python, R, Java, C++, MATLAB), PySpark, PyTorch, TensorFlow, Scikit-Learn, Microsoft Azure, Microsoft Power BI, Tableau, AWS, SQL, DBT (Data Build Tool), Git, GitHub, Oracle, Shell, Bash. Instrumentation: Nuclear Magnetic Resonance (NMR), High-Performance Liquid Chromatography (HPLC), UV/Vis Spectroscopy, Mass Spectrometry, Scanning Electron Microscopy (SEM), Fourier-Transform Infrared Spectroscopy. Industry Knowledge: Data Analysis & Visualization, Pattern Recognition, Regression Analysis, Machine Learning, Algorithms, Computer Vision, Data Modeling, Nanotechnology, CI/CD, Laboratory Techniques, Laboratory Safety. WORK EXPERIENCE
Data Scientist – Computational / Theoretical Chemist June 2018 – April 2021 University of Pittsburgh, Pittsburgh PA
o Developed parsing algorithms alongside linear regression statistics models to approximate bond order, multipole moments, and thermodynamic properties for nanotechnology systems. o Explored machine learning via TensorFlow, PyTorch, and Scikit-Learn as a suitable method for accelerating computational chemistry assessments applicable to materials science and engineering. o Investigated the utility of various database & visualization software, modeling applications, and cloud computing strategies to improve property prediction algorithms to accelerate materials discovery. Mathematical Chemistry Teaching Fellow August 2020 – December 2020 University of Pittsburgh, Pittsburgh PA
o Incorporated weekly recitation sections aimed at reinforcing lecture content as well as introducing students to data analysis concepts such as graphical interpolation, Fourier transforms analysis, and computer vision. o Presented relevant advanced mathematical concepts from calculus, differential equations, and linear algebra. American Chemical Society – Student Chapter President August 2016 – May 2018 Florida Institute of Technology, Melbourne FL
o Organized the 2018 Symposium for Materials Chemistry, an event geared toward uniting central Florida ACS student sections toward improving leadership skills and reviewing recent advances in materials science. o Implemented social events for students, faculty, and the local community to think critically about chemistry. AWARDS AND HONORS
Dietrich School of Arts & Sciences (A&S) Graduate Fellowship June 2018 University of Pittsburgh, Pittsburgh PA
ACS Organic Division Undergraduate Award May 2018
Florida Institute of Technology, Melbourne FL

"
,
Solar Quality Assurance,"
LINKS
SUMMARY
TECHNICAL SKILLS
MAJOR ACADEMIC PROJECTS
ZAMAN
ALI
Chase Ave Des Plaines, Illinois 600**-***-*** 5630 admj2u@r.postjobfree.com Portfolio: https://zamanali23.github.io/zaman-ali.github.io/ Linkedin: https://www.linkedin.com/in/zaman-ali-237810ab/ GitHub: https://github.com/zamanali23/zaman-ali.github.io Data Scientist Proficient in predictive modeling, data processing, data mining algorithms, and scripting languages, including Python and Java. Skilled at creating, developing, testing, and deploying highly adaptive diverse services to translate business and functional qualifications into substantial deliverables. Programming Languages: Python, Php, C, C++, Assembly Language, VHDL, AutoCAD, Matlab
IDE: Google Colab, Jupiter Notebook, Intellij, Visual Studio. Libraries: Numpy, Pandas, Matplotlib, Seaborn, sci-kit-learn, Spicy, Tensorflow, Keras. Data Science Skills: Data Visualization, Data Cleaning, Exploratory Data Analysis, Probabilities & Statistics, Machine Learning, Predictive Modeling, Model Optimization, Deep Learning, NLP, Model Deployment, Neural Networks, Algorithms. Database: MySQL, SQL Server.
Other Skills: Git/Github, Apache Maven, Unix-Shell script, Digital Systems, Embedded Microprocessor, Signal Processing, Circuit Theory, and Analysis, PCB Designing, Microcontroller.
Operating System: Windows, macOS, Linux.
Recognizing Handwritten numbers using Neural Networks Learned how to augment images for classification Implemented a CNN model. Trained a CNN model which achieved an accuracy of 99.97 % in identification. Created a pipeline to improve it model implementation. Deployed Bagging and Boosting models to classify app-based transactions as fraudulent or not
WORK EXPERIENCE
Achieved an accuracy of 89% using XGB classifier and 87% using the Bagging Classifier
Explored the dataset for anomalies and missing values. Used Pandas to derive new features in the dataset. Applied XGBoostClassifier with default parameters. Calculated AUC/ROC score with default hyperparameters. Computed feature importance score and name the top 5 features/columns Applyied BaggingClassifier with base_estimator.
LogisticRegression and compute AUC/ROC score.
On the basis of AUC/ROC score compared BaggingClassifier and XGBoostClassifier. SEPTEMBER 2016-DECEMBER 2019
Junior Engineer Inducto Powers Manufacturing Gujranwala, Punjab Monitored installation and operations to ensure customer satisfaction. Applied principles of electrical theory to advance and improve product development and efficiency.
Utilized AutoCAD technical drawings and electrical systems specifications that exceeded company standards.
Studied circuit board troubleshooting, soldering, etc., from senior engineers Prepared and submitted cost allocation reports.
JUNE 2015-SEPTEMBER 2016
Junior Engineer Wapda School Muzaffarghar Gujranwala, PB
● Assisted in designing PV systems (Converts sunlight into electricity.)
● Equipped selected PV systems with uninterruptible power supply (UPS) capability, allowing customers to operate selected circuits for hours (or even days) during a utility power outage.
● Assisted in distributing system voltage performance analysis for high-level penetration photovoltaics, contributing to research study that will be used to optimize future PV systems.
● Conducted efficiency and price/watt comparisons on other solar power systems on the market to gather intelligence used for marketing campaigns.
● Created content and maintained company blog, educated the public on the advantages of solar electric power.
● Consulted and planned equipment layout to resolve problems in system operation or maintenance.
● Maintained equipment logs, recorded performance problems, repairs, calibrations, and tests.
JUNE 2014-OCTOBER 2014
EDUCATION
Student Intern Descon Engineering Limited Gujranwala, PB Tested Dosimetry and mapped the findings
Gathered data for Energy Saving Opportunities
Planned and revised - Emergency Preparedness.
Presented findings, post-research, on free air-cooling. JUNE 2013-SEPTEMBER 2013
Volunteer - Quality Assurance Analyst Control Nestle Gujranwala, PB Resolved quality-related issues adhering to deadlines Provided training to quality assurance
Created efficient design protocol that can be used across all domains Prepared documentation of inspection process with detailed reports and performance
Recommended improvement measures to production process to ensure quality control standards are met
2020
Bootcamp: Data Science
TECH I.S., Santa Clara
Completed an accelerated program with an immersive hands-on curriculum. Areas of focus were Exploratory Data Analysis, Probabilities & Statistics, Machine Learning, Predictive Modelling, Model Optimizations, Deep Learning, NLP, Model Deployment, Data Visualisation.
AUGUST 2017
Master of Arts: Masters of Project Management
Keller Graduate School of Management, Illinois
JUNE 2015
Bachelor of Engineering: Electrical Engineering
The University of Lahore, Lahore, Pakistan

"
,
Sql Developer Data Analyst,"
FRED OPPONG SARPONG
773-***-**** **** Amsterdam Circle, Montgomery, Illinois, 60538 admiqy@r.postjobfree.com
Tableau and SQL Developer
SUMMARY

Highly motivated, self-starter, detail-oriented Business Intelligence Developer with over 7 years involvement in Business Intelligence, Visualization, Data Warehousing, Data Modelling and Data Analysis.
A Data Analyst and a Database Developer skilled in T-SQL programs, SQL Server Platforms in addition to Complex Query writing from both the Financial and Health Care data sets.
Skillful in tabular, matrix, parameterized SSRS reporting, ETL, SSIS Package Deployment and Creation with different transformation tools including Lookup, Merge join, Multicast, Conditional Split, Data Conversion Transformation, Slow Changing Dimension etc.
Experience in Data modeling for Dimensional and Star schema designs in Data Warehousing
Experienced in providing demonstrations of the use of Tableau functionalities for doing Trend and Forecast Analysis for Executive and Operational Management to enhance strategic decisions.
Excellent communication and leadership skills, rapidly learns and adapts to new technologies, a team player and a productive independent worker who meets deadlines on projects.

TECHNICAL SKILLS

Operating Systems: Microsoft Window XP Professional, Windows 10, Linux and UNIX.
Databases/BI Tools: Tableau – 10.1 - 2020.4 (Desktop, Server, Prep, Public & Online), SQL Server/Management Studio (2008, 2012, 2016), T-SQL, Snowflake, Postgresql, SQL Query Analyzer, Oracle, ETL, Visual Studio (SSIS and SSRS), Agile (Kanban and JIRA), Confluence, Microsoft Office tools (Power Point, Excel and Word).
Alteryx, Python and R Integration into Tableau for predictive analysis.

WORK EXPERIENCE

Reynolds Consumer Products (Judge Group) 07/2019 – Now
Data Visualization Analyst
Extract and transform raw data from various data sources (Spreadsheets, SAP, SQL, Postgresql etc) into useful information to enable more effective strategic, tactical, and operational insights.
Transform business requirements from internal Partners into dashboards that are functional and easily accessible.
Collaborate with the Data Scientist on end-to-end process to ensure data can be easily refreshed as needed.
Develop best in class solutions for visualizations across the organization and recommend ways to improve data reliability, efficiency and quality.
Build dashboard and reporting proofs of concept as needed; develop reporting and analysis templates and tools.
Manage the team’s content on Tableau Server – Users, groups, workbooks, data sources, permissions, coordination etc.
Lead the effort of migrating proof of concept Tableau dashboards to production.

Optum Rx (Insight Global), Chicago, Illinois 05/2018 – 06/2019
Data Analyst/ Tableau Developer
Built out OptumRx Operational Metrics, Clinical Program Metrics, and Book of Business (Medicaid, Medicare, Commercial etc.) metrics into Tableau through the creation of new dashboards from SQL, SAS and Excel data sources.
Developed Business Intelligence Scorecard for PBM, Clinical and Insurance Claims data sets for tracking remediation metrics.
Built and maintained SQL scripts and complex queries for extracting data for analysis ensuring optimized data sources and automated data flow.
Prepared and Validated data sets using SQL, excel, Alteryx and Tableau functionalities (LODs, Table Calculations, Calculated Fields, Joins and Data Blending) to ensure optimization and data integrity.
Built out Self-Service reports and visualizations in Tableau for internal users and upper management reducing reporting time by 60%.
Created insightful and easily understandable Insurance/Financial Claims Reports and Outcomes dashboards with Medicare and Medicaid data sets in Tableau using Sets, Bins, Hierarchies, Groups, Actions, Parameters and complex Formatting.
Created ad hoc Tableau dashboards in a quick turnaround time for upper management reporting from large sets of PBM and clinical data sources for which I received 3 “Bravo awards”

Citadel (Open Systems), Chicago, Illinois 01/2018 – 04/2018
HR Data Analyst
Carried out a Data Source Replacement Project on Tableau dashboards which had static data sources in excel with dynamic data sources from a SQL Server and partly maintained the semantic layer to ensure data integrity and automated updates.
Single handedly created a Data Dictionary from 32 Data Sources to enhance quick and easy access to the different data sources, some with the same records but different Headers and vice versa.
Executed a Tableau dashboard maintenance project overhauling and formatting some visualizations/views to ensure optimized and meaningful business reporting
Carried out a data visualization project on Employee Turnover Rate in Voluntary/Involuntary Terminations per Business Units, Functions and Departments for upper management
Used Excel Functions like Vlookup, Pivot Tables, Index and Matching to generate tables and create reports for end users.
Created Dashboards, stories and reports using calculated fields, parameters, sets, hierarchies and groups in Tableau

Vein Clinics of America (Medix), Downers Grove, Illinois 06/2017 - 12/2017
BI/ BW Developer
Translated technical requirements and specifications in data modelling into Business needs.
Formed and deployed SSIS packages with the different transformation tools such as Data Conversion Transformation, Merge Join, Lookup, Fuzzy Lookup, Conditional Split, Multicast, Slowly Changing Dimension, Aggregate and Derived Column.
Assisted in creating SQL database objects like tables, triggers, views, indexes, stored procedures, and functions using T-SQL to provide definition, structure and to maintain data efficiently.
Worked with teams and clients to integrate resource systems to build and deploy BI solutions.
Created visualizations, dashboards and reports for requested projects with Tableau and SSRS.

WebMD, Manhattan, NY 06/2015–05/2017 Tableau & ETL Developer
Consulted with the business clients and vendors of the organization to enable the creation of customized worksheets, dashboards and story points.
Created interactive dashboards and customized worksheets and visualizations using Calculated Fields, Filters (global, context and quick), Parameters and Table Calculations.
Established and developed best practices guide for Tableau implementation in Tableau (Desktop, Server and Online).
Built and supported the transformation of various data inputs into the Enterprise Data Warehouse (EDW).
Performed ETL (SSIS) functions, process documentation, & helped implement a mass import routine to bring groups of people into Active Directory.
Generated queries to retrieve data from SQL server and created T-SQL statements which loads data into required databases.
attitude
Citigroup Smith Barney, New York 03/2013-06/2015 Tableau Developer/Data Analyst.
Generated Dashboards with the different models of charts in showing the performance of the different critical KPIs simplifying strategic planning by management.
Occasionally carried out Tableau administration duties in adding users and group, data connections, managing licenses and task scheduling in Tableau.
Performed the Optimization of data connections and designing, background task scheduling and incremental refresh for daily, weekly and monthly reports in the tableau server.
Used Calculated Fields, Table Calculations, Data Blending and the different types of joins to bring data from different data sources to one platform to enhance and enrich data analysis and visualization.
Created dashboards where a variety of views were connected by Actions and provided an interactive reporting setting of the Revenue Impact Analysis, Pricing Opportunity Analysis, Markets Overview and Inventory Control.

EDUCATION

MBA - Logistics and Supply Chain Management
Kwame Nkrumah University of Science & Technology (KNUST), Ghana, West Africa
Bachelor of Science – Agriculture Economics
Kwame Nkrumah University of Science & Technology (KNUST), Ghana, West Africa
Expended

"
,
Data Analyst Supply Chain,"
ASHI CHATURVEDI
+1-617-***-**** linkedin.com/in/ashichaturvedi adme74@r.postjobfree.com Boston, MA EDUCATION
Northeastern University, Boston, MA Expected Jul 2021 College of Engineering, Master of Science in Data Analytics Engineering GPA: 3.76 Relevant Courses: Data Warehousing and Business Intelligence, Data management and Database design, Probability and Statistics, Data Mining, Computation and Visualization, Machine Learning, Operation Research IES IPS Academy, Indore, India Aug 2015-Jul 2019
Bachelor of Technology in Electronics and Communication Founder of Literary Club, Student of the year 2019, President of Lab Services PROFESSIONAL EXPERIENCE
Takeda, Boston, MA Aug 2020-Jan 2021
Data Scientist Co-op
• Led analyses to demarcate scope of 2 top prioritized programs by building data systems, pipelines and architecture
• Established methodology for data integration, analyses, and structure on large-scale unstructured data across teams
• Developed models and interpreted data by visualization and predictive analyses and generated Process Flow Diagrams using R studio, SQL, Tableau and PowerBI and used Salesforce for CRM and JIRA to update system analyses
• Applied computational and statistical methods to build algorithms, models and prototypes to derive insights
• Identified key information to inform complex decisions using multivariate analyses with PCA and statistical modeling; discovered clusters and statistical significance between groups for various combinations
• Built the data and reporting infrastructure from ground up using Tableau and SQL to provide real time insights into the product
• Strengthened development of strategy and alignment across sites (Japan, Boston) by analyzing the gaps and mending them
• Communicated model results to cross-functional teams over 100 members through presentations, and technical reports WebCraft IT, Indore, India
Assistant Data Analyst Jan 2018-May 2019
• Implemented ETL strategies for processing Customer Data on various Warehousing Projects
• Constructed workflow charts and gathered customer insights to study business functions and system capabilities
• Influenced stakeholders to support business projects and identified business opportunities using Salesforce
• Designed ETL data pipeline templates for populating data and constructed scripts to generate direct load and SSIS mappings
• Synthesized current business intelligence data to produce reports and presentations using tools like JIRA and Confluence
• Troubleshooted, fixed and reported bugs developed during data migration to Azure cloud environment TECHNICAL SKILLS
Programming Languages: SQL, R programming, Python, SAS Database Management: Oracle SQL, MySQL, PostgreSQL and MS SQL Server Data Integration: Talend, SSIS, Alteryx, ER/Studio Data Architect, Informatica Data Science: Machine Learning, Deep Learning, Neural Networks, Keras, Tenserflow, PyTorch, Dplyr, Time Series, Statistical Methods, Scikit-learn, Numpy, Pandas, Matplotlib, Rshiny, Jupyter, Hadoop, Spark, Azure, R Studio
AWS: Redshift, Quicksight, Athena, S3, Lamda, EC2
Data Visualization: Google Analytics, Power BI, Tableau, Qlik Sense, QlikView, Datawrapper, Flourish ACADEMIC PROJECTS
Stock Price Prediction using Machine Learning Northeastern University Aug 2020-Dec 2020
• Built a model to analyze company’s future profitability on the basis of current business environment and financial performance
• Predicted future stock price by using simple algorithms like averaging and linear regression and advanced technique like LSTM
• Enforced structural time series model using TensorFlow Probability for forecasting everyday closing stock price
• Computed point forecasts and predictive uncertainties in stock price; actual prices fell within 95% interval with 85% accuracy Data Integration and visualization for Student’s behavior Northeastern University Jan 2020-Apr 2020
• Devised a data warehouse from 5 different school districts data sources using Talend and SSIS
• Headed streamlining of data pipelines that automate flow and transformation of data between key systems
• Automated ETL processes, making it easier to wrangle data and reducing time by as much as 40%
• Developed several Tableau and Power BI dashboards following OLAP and Data Modelling concepts to prompt integrated dynamic data visualizations for student behavior analysis and finding Key Performance Indicators Supply Chain Prediction Database Northeastern University Aug 2019-Dec 2019
• Built ERD, Business rules and relationships between 10 entities with minimum 2 attributes to map end to end supply chain
• Created a database to maintain an inventory using stored procedures, triggers, functions, indexes, views and SQL scripts
• Enforced business rules applying constraints, predicted number of products to be produced and visualized results in Power BI

"
,
Engineer Process,"
Michael T. Brady

admd9w@r.postjobfree.com 303-***-**** Loveland, CO https://www.linkedin.com/in/michaeltbrady/

Profile

Lead engineer for a high productivity Data Sci. and Eng. Team. A results-oriented Development Engineer with proven abilities in: Software Engineering, Data Engineering, Machine Learning Engineer and Data Science. Qualities include technical excellence, leadership, innovation, detail oriented, fostering great teamwork, transparency, accountability and communication.

Demonstrated Skills

Data Eng. – designing high reliability, fault tolerant data pipelines and systems.
Data Science – cleaning, transforming, Feature Eng. in order to create ML models.
Software – Use Agile, Unit Testing and CI/CD to consistently deliver high quality software.
Innovation – Ability to solve complex problems where no prior solution exists.
Leadership – Mentoring small high productivity teams of engineers.
Innovation – 14 US patents (10 issued and 4 new applications) creating valuable IP.
Communication – Clearly communicate complex ideas to team members and management.
Leadership – accountable and transparent on projects, deliver on-time.
Innovation – Continuously learn and teach best practices.
Process Eng. – design Mfg. systems and optimization for world class manufacturing.
Electrical Eng. – hardware, circuit design, and embedded firmware experience.

Technical Skills

Languages (years used / Number of lines of code written): Fluent: Python (15 yrs./150k), C (20+ yrs./50k), Unix Shell Exposure to: C++ (2 yrs./8k), Go (1 yr.), Java (1 yr./ 5k), Matlab (3 yrs./3-5k), Perl (1 yr., < 2k), C# (1 yr/ <1k), FORTRAN 77 (1 yr./ <1k), Motorola 68HC11 Assembly (1 yr./ <1k)
Software Engineering Skills: Architecture, Unit Testing, Distributed Compute, Concurrency, Parallel processing, CI/CD, Jenkins, TDD, Agile, Scrum, database schema design, DevOPS, OO Design Patterns, algorithm development and optimization, Middleware, Embedded Firmware, RTOS and TCP/IP.
Data Engineering: real-time & batch, structured and unstructured
Data Science: prepare features, create ML models, Stat. Modeling, Data Visualization, Predictive Modeling
Scientific Method: use it to solve problems.
Source Revision Control: git, SVN, Perforce
OS & other: Linux(Debian and RHEL), UNIX, Docker, Windows
Databases: MySQL, SQLite, ElasticSearch, SQL Server, Data Warehouse, Big Data
NoSQL: Redis, RethinkDB, MongoDB
Big Data: (limited exposure) to Hadoop, Spark
Process Eng.: Setting limits, DOEs, GR&R, 6 Sigma brown belt, & statistical analysis.

People Skills

Leading: small teams resp. for day-to-day and larger projects. Drive larger cross-functional teams.
Mentoring: junior engineers to be their best.
Project: Break-down large projects for self & others, clearly communicate status, find solutions.

Engineering Experience

Micron Technology, Inc. 4/13 – present
Principal Engineer
Data Sci. & Eng. leading team of 3 (including myself) in:
Data Engineer: Architect, design, and implement real-time, high reliability, fault tolerant data pipelines.
Data Scientist: Analyze SSD Mfg. data, use ML to solve problems.
Architect/design mfg. sys. including: distributed compute, firmware, & hardware.

Spectra Logic, Boulder, Colorado 11/11 – 4/13
Sr. Software Engineer
Write real-time robotics software to support a variety of storage products.

Seagate Technology, Staff Software/Firmware Engineer, Longmont, Colorado 6/05 – 11/11
Lead Software Developer, code integrator, and system owner for critical mfg. sys.
New feature development, system maintenance, failure analysis, and defect resolution.
Process Engineer
Saved millions of dollars using statistics to optimize manufacturing parameters.

CORNELL UNIVERSITY, Teaching / Research Assistant, Ithaca, New York 8/02 - 9/03
IBM, Boulder, Colorado Software Engineer, Research Asst., Logic Design Eng. 10/99 - 7/01

Education

Life-long learner in Computing, Math, Electrical Engineering, and Physics.
COURSERA Certificate in Applied Data Science with Python 2/19

CORNELL UNIVERSITY, Ithaca, New York 02-03
Pursued Ph.D. in Applied Physics, spent 1 year studying graduate level Quantum Mechanics

PURDUE UNIVERSITY, West Lafayette, Indiana 95-99
Bachelor of Science in Electrical Engineering, with Highest Academic Distinction GPA: 4.0

Honors, Patents and Publications

Patent and paper details are available at https://www.linkedin.com/in/michaeltbrady/
Graduated from Purdue with Highest Academic Distinction; Received a fellowship while at Cornell and a scholarship at Purdue; Spoke at Electronic Imaging 2002 in San Jose, CA
References available upon request

"
,
Data Analyst/ Data Scientist,"
Nhut Dang University City, ***** • 470-***-****
admdlk@r.postjobfree.com • linkedin.com/in/nhut-dang-30238213b
EDUCATION
Randolph College, Lynchburg, VA 05/2019
Bachelor of Science Mathematics
Bachelor of Science Engineering Physics
Bachelor of Art Global Studies (International Economics specialization)
Washington University in St. Louis, St. Louis, MO 05/2021
Bachelor of Science Systems Engineering
Masters of Science Engineering Management (Applied Analytics specialization)
WORK EXPERIENCE
Innovating for Defense Team Member / Washington University in St. Louis, St. Louis, MO 01/2021 - 05/2021
Worked with Department of Defense clientele on unclassified data processing problem.
Conducted customer discovery and proposed solutions which improved analysts’ workflow and collaboration.
Market and Data Analyst Intern / Cassandra IQ, St. Louis, MO 05/2020 - 08/2020
Created survey, database, and algorithm which auto-generated questions for educational related subjects using SQL and Python.
Analyzed different financial data of educators in Missouri.
Stock Trading Algorithm Project Member / Washington University in St. Louis, St. Louis, MO 08/2019 - 12/2019
Implemented the algorithm from “Trend Following Algorithms for Technical Trading in Stock Market” by Simon Fong, et. al. on Nasdaq using Matlab to test out its performance for Senior Capstone.
Optimized the algorithm and created a machine learning protocol with statistic to optimize the algorithm performance.
Applied Operations Research Member / Washington University in St. Louis, St. Louis, MO 01/2018 - 05/2018
Worked with emergency response clientele on work scheduling problem for Applied Operation Research Project.
Created the work scheduling algorithm that optimized the company profit while satisfying different demands for full-time workers, part-time workers, temporary workers using Python with Google OR package.
Math Modeling Competition COMAP Team Member / Randolph College, Lynchburg, VA 01/2017 - 05/2017
Represented Randolph College and wrote a paper on problem B: Merge After Toll.
Researched on factors which have the most impact on the performance of tollbooth.
Drafted an innovative design for the tollbooth which has less area used, less merging pattern, and improved service.
Summer Research Participant / Randolph College, Lynchburg, VA 06/2016 - 08/2016
Conducted research on the Elliptic Curve cryptosystem and its variation. Solved math problems to prove the equivalence in strength of different cryptosystems.
Received $2000 grant (RISE grant) for research.
Established a protocol based on one of the variations using Java.
Program Coordinator Intern / Randolph College Science Festival, Lynchburg, VA 01/2015 - 05/2015
Planned and helped with Science Festival’s events.
Organized and accommodated 18 drop-in activities.
SOCIETIES AND EXTRACURRICULARS
President Dollar and Sense / Randolph College Business and Economics club, Lynchburg, VA 01/2016 - 05/2017
Senator / Randolph College Student Government, Lynchburg, VA 08/2016 - 05/2017
SKILLS
Python, Java, Matlab, Github, Microsoft Office, OpenCV, Keras, Data Mining, Machine Learning, Math Modeling, Programming, Regression Analysis, Entrepreneurship, Statistical Analysis, Innovative Thinking, Data Analysis, Natural Language Processing.

"
,
Software Engineer,"
Jun Miyamoto, Ph.D.

ac8ykn@r.postjobfree.com • LinkedIn
Automation Software Test and Development Engineer
Multifaceted technical career with 20-year track record in software engineering
Technically seasoned electrical and nuclear engineering professional with extensive experience in software testing, development, and implementation through product development, modifications and issue resolution. Broad knowledge in executing and creating test protocols, validating equipment, and identifying issues and defects to be reviewed. Skilled in managing relationships and creating new relationships through collaborative communication with client, stakeholders, and project team. Talented in data acquisition, data analysis/logging and reports including Labivew, R, Matlab and Python programming. Possess experience in software engineering, metrology, and am interested in laboratory science work in addition to executing research to ensure efficient product uptake.

Technical Proficiencies
Device Fabrication:
Class 100 clean room, CMOS facility, silicon based photolithography, process and device simulation by TCAD, MAXWELL, COULOMB, Kapton flex substrate based micro-patterned devices, semi-conductive detectors for photons and charged particles
Computer Skills:

Communication Protocols:
Electronics Modules:

Labivew and its built-in tools (e.g. NIDAQ, VISA), Monte Carlo Simulations, Bayestian/Weibull statistical analysis, Visual Basic, VisualStudio, Microsoft Excel Macro, Matlab, R, ROOT, Fortran, Ruby, Python, C/C++, C#, SQLite, National Instrument propriety TDMS database, Access database management, Numerical Analysis, Fourier analysis and Digital Signal Processing and signal conditioning, Statistical Analysis, GNU make/Cmake, Math Libraries (GSL, LaPack, Intel Math Kernel, Numerical Recipes), GitHub Version control, Linux (Ubuntu, Mint, RedHat) and scripting in PowerShells, AutoCAD, Spice simulation. Debugging in visual studio code.
GPIB, RS-232, PCI/PCIe, TCP/IP, CAMAC, VME, VXI, CAN-BUS, USB-Serial, Bluetooth, Wifi (802.11a/b/g)
Computer Controlled DMM and Power Supplies, ESD Discharge/Arcing study in vacuum chambers, Electronic noise analysis, Waveform Digitizers (Flash ADC), Keithley current meters, Tektronics/LecRoy Digital Oscilloscopes and Mixed Signal Scope, Tektronics wave-form generators, Analog pulse-shaping amplifiers, Omega remotely controlled transducers for slow control/monitoring, Soldering and Packaging, Printed Circuit Board design, Text-Speech Audio and Speech recognition.
Chemistry/
Chemical Techniques:
Vacuum systems, turbo pumps, gate valve pressure gauges, gas circulation system design using ultra-clean materials, gas purification, impurity analysis, Residual Gas Analyzer, Gas Chromatography, Mass Spectroscopy
Professional Experience
Toyota Motor North America Headquarters Campus, Plano, TX
Automation Software Development Engineer in Connected Technology, Jan 2021 – Present
Test activities on infotainment devices and connected devices (iOS/Android). Planning, tool development, execution and software debugging. Technical support for Toyota’s global locations in the automation test activities.
Key Projects
Software development: Development and testing in Agile environment in a small team
Hardware development: Designing, building and improving hardware equipment for testing
Customer and tech support: Interacting with global customers in troubleshooting and getting feedback to improve software and hardware

Tokyo Boeki North America, INC, Florence, KY
Metrologist, Software/hardware Engineer, ISO 17025 Manager, May 2015 – Dec, 2020
Interface with diverse range of clients to complete identification of product and software bugs before deployment and providing feedback to development team. Oversee field technicians and company-wide quality management team through buying and maintaining metrological equipment for customers leading to ability to attract new clients and consistent relationships with existing clients. Supply clients with creative solutions for management of their software and hardware needs.

Key Projects
Software test Engineer: Streamline the uptake of metrological software by training customers and collaborating with customers on obtaining measurement task that leads to macro/wizard automation and efficient manufacturing and increased revenue for the company.

Interact directly with front-end software programmers during development and test
alpha and beta versions of new software. Write reports on unusual behaviors and
suggest strategies for improvement until official deployment.

Identify high-priority customers who want to customize software for their use and
send detailed requests to the development team. Follow up on the development
until the customers are satisfied with the results.

Hardware test engineer: Proto-type testing during R&D phase. Send test results to the R&D team and update FPGA firmware. Calibrate the measurement equipment before installation and also doing trouble shooting. To tune electro-mechanical optical encoders, I use Labview and Tektronics digital scope to set the trigger point right and it helps our repair technicians minimize downtime of the machine. Diagnosis for malfunctioned ciruit boards and resolder components.
Dismantle faulty measurement machines and clean and reassembly followed by rigorous stress tests.

ISO 17025 Quality and Technical Manager: Coordinate customer purchasing and maintenance of Metrological equipment while ensuring company passes external audits and assessments for certifying agency for on-site inspections and assessments for Metrological equipment. Supervise field technicians to comply with ISO regulations and give them suggestions for improvement.

Metrologist: Assist clients in accomplishing their measurement task needs for set standards for accuracy through developing lasting solutions leading to consistent customer satisfaction and loyalty for company software and hardware.

Southern University ROTC Program, Baton Rouge, LA
Physics and Calculus Tutor, Jan 2015 – April 2015
Applied a solid understanding in the field of physics and calculus and a passion for transforming students minds to reach their full potential. Guided students through acquiring knowledge by adapting tutoring practices to fit different learning styles. Provided ROTC department with session reports on student progress and tutoring session outcomes for each student. Followed up with students outside of tutoring sessions about enrollment progress and concerns related to course topics.
Key Achievements:
Lead the successful matriculation of ROTC program students through their STEM field related coursework by ensuring they were well prepared to learn and pass test for their physics and calculus courses leading to successful retention of students in ROTC and STEM program.
Brookhaveen National Laboratory, Upton, NY
Summer visiting researcher, July-Aug, 2014
Key Projects:
Built a Keithley GPIB-based data acquisition in synchrotron photon facility.
Built Labview based automated DAQ with a digital scope
Ran software to analyze the data
Troubleshooting in case of malfunctioning and repair/tuning of instruments.
Louisiana State University, Baton Rouge, LA
Senior Researcher in Neutrino detection, March 2010 – July 2013
Directed all aspects of research development and implementation with a team of undergraduate students. Led the data collection and analysis process leading to the ability to publish findings for presentation at various conferences. Apply experience and knowledge with a variety of nuclear science and radiological principles to encourage fellow researchers to develop innovative technology and mentor current research students on opportunities after university depending on the career path of their choosing,
Key Achievement:
Designed, developed, built and implemented a detector system based on CdZnTe room temperature gamma ray detector with associated analog electronics and digital data acquisition system as a result of understanding the need for the acquisitions of cosmic rays and standard gamma rays.
Managed, lead and mentored a small research group of student researchers.
Used National Instrument products to control digital oscilloscopes, wave-form generators, current meters, DC power supplies, flash Analog-to-Digital converters to acquire signals from particle detectors.
Built slow-control systems to monitor environments like pressure, temperature sensors using Labivew-based I/O interface.
Programmed in Labview to make band-width limited amplifier to process noisy wave forms using Labview’s DSP express tools.
Programmed in C++ and Python to extract data from TDMS data base.
Built a control system using Labview and xyz electro-mechanical moving stages to automate the movement of the detector with respect to the radiation beam. The automation helped me acquire huge amount of data with minimal supervision, sometimes running over night.

Prior Experience
Physicist (May 2006-Feb 2010) for Weizmann Institute of Science, Rehovot, Israel
Research Associate (Sept 2004 to April 2006) for Carleton University, Ottawa, Canada
Senior Research Scientist (May 1997 to Aug 2004) for Purdue University, West Lafayette,IN

Education and Credentials
Doctor of Philosophy in Nuclear Engineering and Radiological Science
University of Michigan, Ann Arbor, MI
Masters of Science, Nuclear Engineering
University of California, Berkeley, CA
Bachelor of Science in Nuclear Engineering
Tohoku University, Sendai, Japan

"
,
Data Analyst Hadoop,"
Rachel (Chengjie Ren)
https://sites.google.com/view/rachel-ren
adl94j@r.postjobfree.com San Diego, CA, 92122 858-***-**** Highly self-motivated data scientist and data analyst with 3+ years of experience Robust professional and technical skills applying data mining to deliver reports and actionable insights Refined bilingual communicator with strong critical thinking and problem-solving skills SKILL
Tools: MySQL SQL Server Python Excel AWS R SPSS Tableau Power BI Tableau Google Analytics Spark Business Analysis: Business Strategy Digital Marketing Ecommerce (Funnel model) Lifetime Value Cohort Analysis Data Analysis: Machine Learning Time Series NLP Web Scraping Optimization Salesforce CRM Statistics (Hypothesis Testing, ANOVA) A/B Testing Hadoop Real time analysis Contest: UCLA Extension Hackathon: 1st place (Sentiment Analysis) UCLA Extension 3rd place (Predictive Analysis) Certificate: Advanced SQL (2020) Business Analysis (2019) Advanced NLP (present) WORK EXPERIENCE
Data Analyst Intern San Diego, CA
Skinny Gene Project Healthcare Nov 2020 - present Leading 20k+ demographic and healthcare data to automate data manipulation process, increase data visibility, proactively track participants health condition, and segment participants to improve participation rate in our healthcare program
• Data Integration and Visualization: Collaborating with different insurances companies, integrating 20k+ health data to generate 10+ interactive leads/outcome reports using Excel (Pivot Table) and Tableau (Detail-oriented), summarizing and demonstrating results based on statistical indicators to the manager
• Pipelines Optimization: Conducting research and data management on ZOHO CRM, including data quality and validation check, data integration automation, data tracking, and improving data process efficiency by 10%
• Modeling and KPI Analysis: Performing hypothesis testing and Machine Learning (Python/R) models (Random Forest/XGBT) based on 20+ healthcare indicators to detect data patterns and deliver most important factors on people’s participation
Data Analyst Intern Remote
EFORTLES Inc. Accounting Startup Nov 2020 - present Applying quantitative analysis, data mining and data modeling to develop potential clients based on company’s marketing campaign and cooperating with cross-functional teams to support business growth
• Data Collection Automation: Automating data collection pipelines from Youtube, Twitter by leveraging web scraping techniques and API to collect 60k+ potential client data and increasing efficiency by 70% (Python)
• Data Mining and Documentation: Developing deep analytical insights around product usability, business logic, and the end-to-end experiences across 60k+ data and documenting user guidelines (payment, sales, MySQL database architecture) on Notion to support platform upgrade by collaborating with cross-functional teams (Design/Software engineers)
• Data Modeling: Implementing Text mining, data extraction and ad-hoc analysis to develop and segment clients (Logistic Regression/K-means++) for better pinpointing small business owners to deliver company’s accounting services Analyst Assistant and Teaching San Diego, CA
AUPAIR IN AMERICA full-time Oct 2018 - May 2019
Quantified and identified student performance data to solve ad-hoc problems to improve teaching process
• Framework Design: Customized service using Excel based on students’ behavior data and achieved a 96% satisfaction rate
• Actionable solutions delivery: Engaged in the task-oriented team and delivered data-driven insights to optimize curricula EDUCATION
University of California, Los Angeles Los Angeles, CA Data Science Certificate GPA: 4.0/4.0 Mar 2020 - Mar 2021 National Kaohsiung University Kaohsiung
Exchange Program: Financial Management GPA: 3.96/4.0 July 2016 -Jan 2017 Qingdao University Qingdao, China
Bachelor’s degree: International Economics GPA: 3.91/4.0 Sept 2014 - June 2018

"
,
Social Media Marine,"
Daniel L. Kennett
DATA SCIENTIST
+1-253-***-****
adl8kp@r.postjobfree.com
GitHub.com/daniellkennett
LinkedIn.com/in/daniel-kennett
PROJECTS
NLP Earnings Call Predictor
github.com/daniellkennett/Earnings_Call_NLP_Analysis Developed a Random Forest Classifier to predict quarterly ticker growth as compared to respective indices using company Earning Call Transcripts.
● Developed a data collecting process consisting of Financial Modeling Prep and yFinance APIs to retrieve Earnings Call Transcripts and relevant stock data.
● Trained a Random Forest Classifier model using TF-IDF, sentimental analysis, and lexicon complexity to predict index outperformance.
● Simulated trades suggested by model recommendations on small-midsized technology companies to outperform the Vanguard Information Technology Index by .97% over each quarter. BERT QA on Financial Documents
https://github.com/daniellkennett/Earnings_with_BERT Can we take advantage of Google’s highly trained Bert model to refer to company documentation and answer user inputted questions?
● Leveraged Transfer Learning, Tensorflow, and Google Cloud to train a “chatbot” that has the ability to reference earning call transcripts and answer questions for easier understanding.
● Created a pipeline to preprocess the SQuAD dataset and fine-tune the Recurrent Neural Network.
WORK EXPERIENCE
Personal Finance Advisor
United States Marine Corps 2019 - 2021
Taught quarterly financial literacy courses to Marines in a department with up to 55 personnel that enhanced their fiscal health.
● Oversaw recovery and allocation of more than $650K in personal investing funds and advised up to 50 personnel, across many different specialties, on financial health, including payroll, life insurance, investments, and mortgages.
● Increased awareness in the workplace on how to analyze financial knowledge and health, mentored individually as well as up to 20-person group classes on debt reduction plans, budgeting, and investment portfolio management.
Public Affairs Manager
United States Marine Corps 2019 - 2021
Led a 5-person marketing team, coordinated social media programs, and increased community outreach to up to 400K viewers.
● Managed the public face of the organization, incorporated data analytics to monitor social media marketing outreach, timing, and effectiveness, and grew average online engagement by 10%, from 15K to 16.5K viewers.
● Engaged with external media representatives on more than 10 large events, coordinating all marketing efforts and organizing events with up to 2K attendees.
● Maintained a robust community and interagency relations program with over 18K social media viewers; assessed needs, created statistical analysis, identified opportunities to expand and grow reach by 6% within the first 30 days.
SKILLS
Python
Pandas
NumPy
SciPy
Sci-Kit Learn
Data Science
Machine Learning
NLP
Topic Modeling
Data Cleaning
Feature Engineering
A/B Testing
Regression Models
Classification Models
Big Data/Database
SQL
PostgreSQL
MongoDB
Apache Spark
AWS S3
AWS EC2
Data Collection
Web Scraping
BeautifulSoup
Workflow Environment
Docker
VSCode
Jupyter Notebook
Google Collab
Data Visualization
Matplotlib
Plotly
EDUCATION
Data Science Immersive
Galvanize
2020
MS - Financial
Management
Boston University
2019– 2020
BA - Music Performance
2011 - 2015
AWARDS
Navy and Marine Corps
Achievement Medal
2021
Navy and Marine Corps
Achievement Medal
2019

"
,
Data Scientist,"
RITVIK RAMESH PALVANKAR
adl7qq@r.postjobfree.com 352-***-**** LinkedIn GitHub
EDUCATION
UNIVERSITY OF FLORIDA August 2019 – May 2021
Masters of Science, Department of Electrical and Computer engineering GPA: 3.1 Coursework: Machine Learning, Image Processing and Computer Vision, Neural Networks and Deep Learning, Signal Processing. MUMBAI UNIVERSITY August 2014 – March 2018
Bachelor of Engineering, Department of Electronics GPA: 3.75 Coursework: Applied Mathematics, Wireless Networks, Object Oriented Programming methodology
TECHNICAL SKILLS
MACHINE LEARNING, DEEP LEARNING TECHNIQUES AND DATA SCIENCE
• Supervised, Unsupervised Learning, Transfer learning.
• Regression, Classification, Decision tree, Random Forest, SVM, Naive Bayes, K-Means.
• Probabilistic models, Dimensionality reduction, Reinforcement learning, KNN.
• Auto-encoders, MLP, CNN, GAN, LSTM, RNN, Time series analysis, Natural Language Processing.
• Data pre-processing, Data visualization, Data Mining, Data Cleaning SOFTWARES AND TOOLS
• Jupyter Notebook, Tableau, MATLAB, Spyder.
• MS-Excel, MS-Word, MS-PowerPoint, AMDOCS CRM.
• Languages: Python, SQL, MATLAB.
CLOUD COMPUTING
• AWS (EC2, EMR, S3, Lambda, Glue, Redshift, SageMaker), Microsoft Azure (Databricks, Machine Learning)
• Good familiarity with Apache Spark, Pyspark, MLlib for Machine Learning. ACADEMIC PROJECTS
Title: Facemask Detection during Covid-19 (Nov 2020 – Dec 2020)
• Made a classifier to detect the facial features and accounted for multiple objects
/faces in a video/frame. Performed the algorithm using Viola Jones detector and trained the model using Haar Cascade xml files and OpenCV. Title: Stacked auto-encoders using MLP (Mar 2020 – Apr 2020)
• Implemented a stacked auto encoder with 5 hidden layers on fashion MNIST dataset. Trained the features with mean squared error as the loss function and compared the results with CNN classifier.
• Used 500-200-10 as the architectural for the bottleneck layers where 10 denotes dimensionality and trained the encoder output with quadratic mutual information as the loss between MLP output and the labels. Title: Continual Learning in Neural Networks (Sep 2020 – Oct 2020)
• Designed a twin convolutional neural network to avoid catastrophic forgetting in neural networks. The model was built using Keras and TensorFlow in Python using the Omniglot dataset. The model was trained with 2 images from every language present in the dataset and assigning 1 or 0 to similar or non-similar values respectively.
• The model was trained with different hyperparameters and for a fixed number of epochs it was observed that every first iteration gave a maximum accuracy of >94%. Title: Bicycle Sharing System prediction (Jul 2020 – Aug 2020)
• Designed MAP and MLE estimators to predict the number of rides sharing bikes used in a 1-hour period on a given day. Performed data cleaning and accounted for missing data values using Pandas.
• Used seaborn, matplotlib and a heatmap for visualization of every feature against the target variable. R-Square and Q-Q plot were used as a metric for evaluation. Maximum Posteriori gave a better output due to its R- squared value (0.966).

"
,
Hadoop Engineer,"
Frisco, Texas, USA
adl5hs@r.postjobfree.com
+1-469-***-****
LinkedIN
AWS Certified
RAGHU KOTHAPALLI
Graduate Student
I am a Data Science, Cloud enthusiast. I want to explore the potential of various Machine Learning algorithms, cloud computing, statistical tools to face and solve challenging problems, trustworthy predictive analysis techniques that solve business problems.
I
Education
Master’s in Data Science
University of North Texas Aug 2019 -
Texas, USA Apr 2021
Current GPA – 4.0
B. Tech in Computer Science
JNTUH Hyderabad, India
Skills
Programming Languages
Python
Java
SQL
C++
C#
Areas of Interest
Data Science/Engineer/Analytics
Cloud Operations
Artificial Intelligence/Machine Learning
Software Developing/Maintaining
Familiar with
ML, DL Algorithms, NLP
TensorFlow, Keras
Worked with AWS, Azure, GCP
EDA, Feature Engineering
Mathematics related to Data Science
Web scraping
Advanced Excel
SQL
Tableau
GitHub
Bigdata Analytics (Hadoop, Spark)
Projects & Work Experience
Machine Learning Engineer Intern
IntelleXt, Inc. January 2021 – present
Extracting details and insights needed from the legal contracts and also based on chat, audio and video conversations. This involves web scrapping of available contracts, building/training a model using google BERT, word2vec, GloVe, sentiment analysis, text classification and other NLP concepts.
Spam Text filtering with scikit learn
Filtering text using NLP concepts.
Multi Label Text Classification with Keras
Used NLP using ML and Deep Learning algorithms on social media data to classify based on certain traits. GitHub
ML/Cloud Operations
Used ML and Deep learning through NumPy, pandas, tensor flow, keras, NN, torch etc., ML modeling and deploying in cloud (AWS Sage Maker). ML DL DS (Modeled and deployed in AWS, Azure, GCP) Deep Learning - Chatbot
Simple chatbot based on DL and python. GitHub
Bigdata Project – Twitter Sentiment Analysis
Rudimentary sentiment analysis using Hadoop and Spark. Project1 Data analysis using Sqoop and SparkSQl. Project2
Data Visualizations - Tableau
COVID19 analysis visualizations. Tableau GitHub
Data Scientist
Mohan Spintex Jan 2018 – June 2019
Python Excel Data Handling ML Algorithms AWS Managing, processing and analyzing huge data. Sales and Market predictions.
OpenGL Projects – Using C++
2D and 3D shapes generator. Link: GitHub
Unity Projects – Using C#, Unity Engine
Developed few games, AR and VR projects. Profile Log

"
,
Data Scientist,"
BRAD SMITH
Eden Prairie, MN
adl3wj@r.postjobfree.com
952-***-****
There simply is not an adequate word that can describe Brad. He is experience and skilled, but his drive is what truly sets him apart. He is the embodiment of the infamous job description phrase, “Other duties as assigned.” Right out of college his career started in international business where he oversaw all the marketing, strategy, and sales for his company’s retail products. He taught himself to code on the long flights to and from China and found he had a natural gift for analytics. Ever since he’s been bringing advanced analytics and data science solutions to his company through machine learning, prescriptive and predictive analytics, and business intelligence. Business Intelligence Manager
Agropur - Eden Prairie, MN
February 2014 to Present
• Responsible for the development and implementation of market research and data gathering strategy & plans.
• Design, code, and manage developers to meet specifications for physical, logical, end user data, dashboards, and tools.
• Develop advanced analytics models using R, Python, SQL, and business intelligence tools to analyze company data to support decision making process.
• Define, analyze, and report on analytics and financials for a variety of projects.
• Implement, integrate, and extract data from data warehouses across the organization.
• Leverages analytics reporting tools, such as Magento, Hubspot, and Google Analytics, to analyze and report on customer activities and market conditions to identify areas for opportunity and improvement.
• Measure search behavior, conversion rates, referrals, purchase patterns to optimize and improve marketing strategies.
• Provides integrated marketing analytical solutions across customers and channels, contributing to in-market action plans.
• Drive deep market understanding and ensure all marketing and product management projects are designed with the consumer at the heart.
International Relations Manager
Agropur - Eden Prairie, MN
September 2014 to August 2015
• Monitors competition by designing and conducting in depth market research analyses using traditional and advanced methods to gather current marketplace information on pricing, products, new products, delivery schedules, merchandising techniques, etc.
• Presents the results of the in-depth market analysis, competitive intelligence, and business solutions in a comprehensive report to Director of Sales and sales staff.
• Assists with international trademark monitoring by conducting, ordering, reviewing and analyzing trademark search reports with in-house attorneys.
• Counsels in-house attorneys on matters relating to international trademarks; from receiving trademark dispute documents through the resolution of the dispute.
• Coordinates all trade shows activities; including, but not limited to, planning, preparation and follow through in relation to all tradeshow activities.
WORK EXPERIENCE
International Business Manager
Agropur - Eden Prairie, MN
September 2011 to August 2014
• Screens potential business deals by analyzing market strategies, deal requirements, potential, and financials; evaluating options; resolving internal priorities; recommending strategic actions.
• Maintain overall responsibility and accountability for ensuring target market (China, EU, Canada, Southeast Asia) project outcomes are achieved and reported.
• Establish & grow BiProUSA’s sales internationally through analyzing market trends & creating detailed marketing strategies.
• Leverage internal and external resources during sales cycle to successfully close business, drive revenue/profit, and build relationships.
BA in International Business
Bethel University - Saint Paul, MN
2010 to 2012
BA in Human Resource Management
Bethel University - Saint Paul, MN
2010 to 2012
International Business, Talent Management, Business Development, Market Analysis, Market Research, Project Management, International Food Labeling (5 years) R (7 years)
Business Intelligence (8 years)
Competitive Intelligence (8 years)
Data Visualization
Tableau (3 years)
Analytics (8 years)
Machine Learning (6 years)
Data Warehouse (6 years)
Power BI (8 years)
AWS (3 years)
Python (6 years)
Microsoft SQL Server (4 years)
Business Analysis (8 years)
EDUCATION
SKILLS
•
•
•
•
•
•
•
•
•
•
•
•
•
•
Harvard: Statistics and R
2016 to Present
Columbia University: Data Science and Analytics in Context 2016 to Present
Columbia: Enabling Technologies for Data Science and Analytics - The Internet of Things 2016 to Present
Columbia: Machine Learning for Data Science and Analytics 2016 to Present
Columbia: Statistical Thinking for Data Science and Analytics 2016 to Present
Microsoft: Programming in R for Data Science
2016 to Present
Microsoft: Data Analysis - Visualization and Dashboard Design 2016 to Present
Microsoft: Analyzing and Visualizing Data with Power BI 2016 to Present
Intermediate R
2016 to Present
Harvard: Introduction to Linear Models and Matrix Algebra 2016 to Present
Harvard: Statistical Inference and Modeling for High-throughput Experiments 2016 to Present
Harvard: High-Dimensional Data Analysis
2016 to Present
Harvard: Data Analysis for Life Sciences
2016 to Present
Eagle Scout - March 2008
CERTIFICATIONS AND LICENSES
ADDITIONAL INFORMATION

"
,
Data Analyst,"
Logan Crawford
541-***-**** adl2qf@r.postjobfree.com logantcrawford.com github.com/logantcrawford linkedin.com/in/logantcrawford
Data Analyst & Web Developer Willing to relocate Nationwide Target Position: Data Scientist An innovative change agent and cross-functional leader leveraging cutting-edge knowledge. With experience in data science, software engineering, full-stack web development, and data analytics. Offering a robust and transferable skillset within a forward-thinking enterprise while bridging gaps between technical and non-technical departments with knowledge of both the business processes and underlying technology for improved solutions. Play an essential role in a commitment to continuous improvements, develop neural networks, manage large volumes of data, visualize and generate reports, and work in a team environment.
-Education-
Master of Applied Data Science University of Michigan - Ann Arbor, 2020 – Present Dual Bachelor of Science, Spatial Data Science & Technology and Geography with Concentration in Geographic Information Sciences University of Oregon, 2015 – 2019
-Career Overview-
Blue Pencil, Salem, OR
Data Analyst and Full Stack Developer 2019 - Present Exercise entrepreneurial leadership in launching a start-up enterprise dedicated to developing comprehensive data analysis software, visualizations, machine learning solutions, and developing optimized, engaging, profitable websites. Provide a competitive edge by implementing interdisciplinary skills in design, social sciences, and computer programming for ethical analyses, inspiring innovations, and beautiful designs in any medium.
- Created an original and informational web application in support of Young Sons Records’ promotional campaign for signed artists and production services. Elevated brand awareness, customer engagement, and consumer demand with custom UX and an interactive gallery.
- Exercise strategic planning in building new websites, including a political campaign site to collect donations and provide information for the incumbent Sheriff.
- Steer brand development with dynamic storytelling and awareness of trends to enhance marketability, brand recognition, and market penetration.
- Demonstrated superior technical proficiency with Python in developing and managing payroll initiatives. Achieved significant cost savings by slashing 20 hours of virtual assistant fees monthly. HD Open House, Salem, OR
Senior Editor, Lead Videographer, and Photographer 2014 - Present Contribute to revenue growth by executing multimedia initiatives to elevate lead generation for residential and commercial properties. Employ interdisciplinary insight in the production, editing, and publication of marketing materials for this small developing company.
- Photographed and edited 300+ homes per year. Accelerated video production from 127 videos to 223 in one year. Ensured MLS compliance and top-tier quality with unique personalization and branding.
-Skills and Strengths-
Adobe Creative Cloud Anomaly Detection API architecture Big Data C# CSS Database Development
Database Querying Data Analysis Data Extraction Data Manipulation Data Mining Data Pipelines Data Visualization Data Wrangling Deep Learning GAN modeling GIT Google Cloud Platform (GCP) Hadoop HTML 5 Javascript Machine Learning Algorithms Machine Learning Pipelines Neural Network Modeling PostgreSQL MySQL Python PySpark PyTorch R React.js Spark Software Development
SQL Statistics Tableau TensorFlow
-References-
Brain Corcoran (Superviser) - 503-***-**** - adl2qf@r.postjobfree.com Noel Bandey - 541-***-**** - adl2qf@r.postjobfree.com
Jacob Morgan - 904-***-**** - adl2qf@r.postjobfree.com

"
,
Data Enginner/Scientist,"
Kolin Boorom
Miami, FL 954-***-**** adl1yd@r.postjobfree.com
EDUCATION
University of Miami Coral Gables, FL
MS Business Analytics; BS Accounting; BS Business Analytics May 2021 ACT: 33
Relevant Coursework: Stochastic Modeling and Time Series Analysis, Python, R, SQL, Linear & Non-Linear Regression, C++, Database Architecture, Machine Learning I & II, Advanced Accounting Honors: Accelerated MAcc and MSBA Program, Cane’s Presidential Scholarship, Accounting Honors Society WORK EXPERIENCE
Xendoo ($2M/Yr Fintech Startup) Ft. Lauderdale, FL Strategy Analytics/Data Scientist May 2019 – Present (FT Aug 2019)
● Worked with COO and CTO to design a system of core operational metrics and build the company’s data architecture. The project included the creation of custom data pipelines, a new ETL stack (DBT, Fivetran, Snowflake, S3), the creation of a CD/CI process, and a custom report delivery system. My responsibilities as project lead included gathering functional requirements from COO, managing a team of 3 consultants, hosting bi-weekly standups, and meeting with key stakeholders. I later became the lead engineer and was in charge of managing the creation and documentation of all report and feature requests. The project gave the COO the ability to monitor every area of the customer journey and resulted in a ~50% decrease in report request delivery times
● I actively use Analytical SQL (Snowflake & MySQL), Python, DBT, Google Apps Script, and Git from the command line in a live CD environment. I’ve used Trello, Monday, and Jira to create and plan tickets for agile sprints, and use Confluence and Github to keep an active repository of the codebase and documentation.
● Led project to redesign customer onboarding experience. Deliverables included a fit-gap analysis, outlining the existing process, and the implementation of communication monitoring KPI’s to identify bottlenecks. The project resulted in a ~13% increase in maximum onboardings per day and a ~25% decrease in average onboarding time
● Analyzed 3 years of customer data to assist CMO and CEO develop an ideal customer profile. The project included data cleansing in Pandas and visualization in Tableau to inform marketing tests and adjust the pricing of our products. This resulted in a 52% increase in the blended price average of our products and a 0.6% decrease in monthly churn rates.
● Worked alongside CEO to co-author investment briefs, pitch materials, and answered live questions from prospective VC investors to assist in closing the company’s seed round Medical Air Services Association ($100M/Yr Supplemental Insurance Startup) Ft. Lauderdale, FL Accounting & Data Analytics Intern May 2018 – August 2018
● Designed an Excel dashboard to congregate data from the company’s financial statements utilizing VBA, macros, advanced lookup functions, and logical statements. Presented final project to the CFO, Senior Comptroller, and Senior Financial Analyst, leading to its company-wide implementation
● Worked with Senior Financial Analyst to draft FY 19 budget by auditing expense reports and making projections through excel models
SKILLS, ACTIVITIES & INTERESTS
Languages: Advanced SQL, Python (Pandas, Numpy, Seaborn, TensorFlow, Keras), R (dplyr, shiny), HTML, CSS, Intro to C++, Intro to JS/ Node, Conversational Spanish Technical Skills: AWS, Tableau, Mode, DBT, Fivetran, Stitch, Airflow, Quickbooks, Xero, Microsoft Office, Excel (VBA, Macros, Power Pivot etc.)

"
,
"R, Python, Tableau, MySQL, C, C++, Java, MS Word, MS Excel, MS Access","
AISHWARYA DEV
*** ******** ******, ***. *, Harrison, NJ 07029
Tel: +1-480-***-****
Email: adl0z3@r.postjobfree.com
https://www.linkedin.com/in/aishwarya-dev-
59235412b/
https://github.com/aishwarya-dev-1
CAREER OBJECTIVE
Graduate student looking for full-time job opportunities for a Business Analyst position with almost 2 years of industrial experience as a Salesforce developer.
EDUCATION
RUTGERS, THE STATE UNIVERSITY OF NEW JERSEY – RUTGERS BUSINESS SCHOOL Master’s in Information Technology and Analytics, GPA 3.56 Graduate Coursework: Analytics of Business Intelligence, Business Data Management, Data Science, Data Analysis and Visualization, Project Management, Business Analytics Programming, Business Forecasting, Marketing Management.
Aug 2019 – Dec 2020
VISVESVARAYA TECHNOLOGICAL UNIVERSITY
Bachelor of Engineering in Computer Science, GPA 3.4 Undergraduate Coursework: Programming with C, C++, UNIX, Database Management Systems, Computer Graphics and Visualization, Operations Research, Java/J2EE, Artificial Intelligence. Aug 2013 – Jun 2017
TECHNICAL SKILLS
Programming Languages include C, C++, Java/J2EE, R, Python, HTML/CSS, JavaScript, Apex. Database Skills include MySQL, Oracle SQL, MS Access. Business Intelligence Tools include Tableau.
Other Skills include MS Excel, Microsoft Office.
PROFESSIONAL EXPERIENCE
ACCENTURE SOLUTIONS Nov 2017 – July 2019
Application Development Associate
• Worked for development, enhancement and configuration of a Salesforce based Hospitality domain.
• SFDC development using Apex programming on the backend and Lightning component on the User Interface.
• Wrote user stories/epics on Jira, kept a record of it on excel and assigned it to developers.
• Worked as a QA tester where issues were identified and debugged to ensure all standards and guidelines are met. EFFONE TECHNOLOGIES July 2017 – Nov 2017
Intern
• As an intern at Effone Technologies, I worked on the KECT project based on Digital Forensics.
• The proprietary Incident Response tool is designed to assist in detection of malicious threats, without the need for file signatures.
• The tool scans the enterprise for malicious threats and indicators and collects data. Through analysis, malicious indicators across various platforms are detected.
• I was involved with the coding, GUI design, creation of MSI packages and unit testing. INDEPENDENT PROJECTS
COVID-19 cases in the world
• Predicted the future coronavirus cases and deaths worldwide by focusing on the current time series analysis.
• Different models like Naïve Bayes, Mean and Arima model accuracies are compared to determine the best model.
• Visualizations were done on Tableau.
Customer Reviews of Amazon Products
• The aim of this project was to analyze Amazon’s most successful consumer electronics product launches and discover insights into consumer reviews.
• Provided Data Visualization for the same on Tableau. Data Science job market in the U.S
• Provided market insights to the job scenario in the U.S. for Data Scientists and successfully concluded that Data Scientist role was the highest in demand across any company and location.
• Naïve Bayes Classification was used to achieve the same. Credit card fraud detection
• The aim of this project is to use this data to help minimize the risk of losing money while still lending money to its customers based on the types of customer and their spending behaviour.
• Achieved by exploratory data analysis which gives an understanding of risk analytics involved in the business.
• Visualizations are done in python using matplotlib, plotly and on Tableau. EXTRACURRICULAR ACTIVITIES
• Let's Be The Change is a non-profitable organization where as a volunteer I helped educate people on the importance of waste segregation. My service also involved cleaning up garbage littered public areas and transforming it into a cleaner environment.
• Post Project Activity (PPA) is an event organizing group where I was the person responsible to take upon the 'Corporate Social Responsibility' (CSR) which involved visiting NGO institutes, schools, old-age homes and helping them out in every way possible.
• Kalpa Raksha Educational and Charitable Trust Often visited this NGO to serve food and taught subjects like Mathematics and English to all the children. Helped the children out by providing useful supplies like stationaries, medicines and groceries.

"
,
Data Science,"
Professional Summary
Professional in the field of Data Science with 8+ years of experience in statistical analysis, data analytics, data modeling, and creation of custom algorithms. Application to the disciplines of machine learning and neural networks using a variety of systems and methods in training algorithms with different could platform. Industry experience includes predictive analytics in finance, marketing, advertising, geospatial and Internet of Things (IoT). Use of NLP and Computer Vision technologies.

Experience with a variety of NLP methods for information extraction, topic modeling, parsing, and relationship extraction in python.
Familiarity with developing, deploying, and maintaining production NLP models with scalability in mind
Worked on Natural Language Processing with NLTK, SpaCy and other module for application development for automated customer response
Wrote automation processes using Python and the AWS Lambda service
Utilized Docker to handle deployment on heterogeneous platforms such as Linux, Windows, OSX, and AWS
Reviewed the use of MongoDB, node.js, and Hadoop to automate the data ingestion and initial analysis processes
Reviewed and deployed the infrastructure on AWS to minimize cost while providing the required functionality
Scale analytics solutions to Big Data with Hadoop, Spark/PySpark, and other Big Data tools
Experience with Public Cloud (Google Cloud, Amazon AWS and/or Microsoft Azure)
Experience working with big data infrastructure with tools such as Hive, Spark and h2o, sparkling water
Implementing solutions with common NLP frameworks and libraries in Python (NLTK, spaCy, gensim) or Java (Stanford CoreNLP, NLP4J)
Experience with knowledge databases and language ontologies
Quantitative training in probability, statistics and machine learning
Experience in the application of Neural Network, Support Vector Machines (SVM), and Random Forest.
Creative thinking and propose innovative ways to look at problems by using data mining approaches on the set of information available.
Identifies/creates the appropriate algorithm to discover patterns, validate their findings using an experimental and iterative approach.
Applies advanced statistical and predictive modeling techniques to build, maintain, and improve on multiple, real-time decision systems. Closely works with product managers, Service development managers, and product development team in productizing the algorithms developed.
Experience in designing star schema, Snow flake schema for Data Warehouse, ODS architecture.
Experience in designing stunning visualizations using Tableau software and publishing and presenting dashboards, Storyline on web and desktop platforms.
Experience in working with relational databases (Teradata, Oracle) with advanced SQL programming skills.
In-depth knowledge of statistical procedures that are applied in Supervised / Unsupervised problems
Basic-Intermediate level proficiency in SAS (Base SAS, Enterprise Guide, Enterprise Miner) & in UNIX
Track record of applying machine learning techniques to marketing and merchandizing ideas.
Experience in Big Data platforms like Hadoop platforms (Map-R, Hortonworks & others), Aster and Graph Databases.
Experience in operations research / optimization will be good to have. Experienced in working with advanced analytical teams to design, build, validate and refresh data models that enable the next generation of sophisticated solutions for global clients.
Excellent communication skills (verbal and written) to communicate with clients and team, prepare + deliver effective presentations.
Strong experience in Software Development Life Cycle (SDLC) including Requirements Analysis, Design

Technical Skills

Data Science Specialties: Natural Language Processing, Machine Learning, Internet of Things (IoT) analytics, Social Analytics, Predictive Maintenance, Stochastic Analytics
Analytic Skills: Bayesian Analysis, Inference, Models, Regression Analysis, Linear models, Multivariate analysis, Stochastic Gradient Descent, Sampling methods, Forecasting, Segmentation, Clustering, Naïve Bayes Classification, Sentiment Analysis, Predictive Analytics
Analytic Tools: Classification and Regression Trees (CART), H2O, Docker, Support Vector Machine, Random Forest, Gradient Boosting Machine (GBM), TensorFlow, PCA, RNN, Linear and non-Linear Regression
Analytic Languages and Scripts: R, Python, HiveQL, Spark, Spark MLlib, Spark SQL, Hadoop, Scala, Impala, MapReduce
Languages: Java, Python, R, Command Line, C++/C, JavaScript, SQL, SAS
Python Packages: Numpy, Pandas, Scikit-learn, Tensorflow, SciPy, Matplotlib, Seaborn, Plotly, NLTK, Scrapy, Gensim
Version Control: GitHub, Git, SVN
IDE: Jupyter Notebook, VS Code, Intellij IDEA, Spyder, Eclipse
Data Query: Azure, Google Bigquery, Amazon RedShift, Kinesis, EMR; HDFS, RDBMS, SQL, MangoDB, HBase, Cassandra and NoSQL, data warehouse, data lake and various SQL and NoSQL databases and data warehouses.
Deep Learning: Machine Perception, Data Mining, Machine Learning algorithms, Neural Networks, TensorFlow, Keras
Soft Skills: Experienced with delivering presentations and technical reports; collaboration with stakeholders and cross-functional teams, and advisement on how to leverage analytical insights. Developed analytical reports which directly address strategic goals.


Professional Experience

March 2020 – Present
Machine Learning Engineer
Mobile Apps Co. Atlanta, GA

Project Summary:
We built a pipeline for automatic data entry of scanned government forms, using AWS as storage for our database. The pipeline accepts two images (the image for information extraction and the reference form) and outputs information such as the names, id, and other tax information. The information extraction steps were implemented using Google tesseract and AWS text Extract as well as Convolutional Neural Networks. We performed OCR on the image to extract wanted information and store them in our AWS database. In the end, we reduced the data entry time exponentially by a power of 3.
This project includes using machine learning techniques and python libraries to derive relevant analysis and metrics, including building proof of concepts to determine value of implementing in future projects. As part of a team focused on understanding the customer experience across the organization, I worked with business partners to understand the business objectives, explore data sources and build NLP/ML solutions. I was able to think outside the box to uncover new ways to analyze unstructured data leveraging our Open Source Data Science Platform (OSDS) and other analytical tools to solve complex business objectives.

Project Points:
Applied business analytics skills, integrated and prepared large, varied datasets and communicated results.
Worked with specialized database architecture and computing environments.
Developed analytic approaches to strategic business decisions.
Performed analysis using predictive modeling, data/text mining, and statistical tools.
Built predictive modeling using Machine Learning algorithms such as Random Forests, Naive Bayes, Neural Networks, MaxEnt, SVM, Topic Modeling/LDA, Ensemble Modeling, GB, etc.
Used common NLP techniques, such as pre-processing (tokenization, part-of-speech tagging, parsing, stemming)
Performed semantic analysis (named entity recognition, sentiment analysis), modeling and word representations (RNN / ConvNets, TF-IDF, LDA, word2vec, doc2vec)
Worked with big data infrastructure and tools such as Hive and Spark
Collaborated cross-functionally with team to develop actionable insights.
Synthesized analytic results with business input to drive measurable change.
Performed data visualization and developed presentation material using Tableau.
Responsible for defining the key business problems to be solved while developing, maintaining relationships with stakeholders, SMEs, and cross-functional teams.
Provided knowledge and understanding of current best practices and emerging trends within the analytics industry.
Participated in product redesigns and enhancements to know how the changes will be tracked and to suggest product direction based on data patterns.
Applied statistics and organizing large datasets of both structured and unstructured data.
Worked with applied statistics and applied mathematics tools for performance optimization.
Facilitated data collection to analyze document data processes, scenarios, and information flow.
Determined data structures and their relations in supporting business objectives and provided useful data in reports.
Assisted in continual improvement of AWS data lake environment.
Used Agile approaches, including Extreme Programming, Test-Driven Development, and Agile Scrum.
Promoted enterprise-wide business intelligence by enabling report access in SAS BI Portal and Tableau Server.



June 2018 – Feb 2020
Data Scientist
Freeport McMorran Phoenix, AZ

Project Summary:
FM is a large mining operation with major interest in Copper and other metals. Operations span the globe with main operations in the Continental United States and Peru. The Data Science/Machine Learning interest is in creating analysis tools to optimize the deliver and production of process ore as well as preventative maintenance of mining/crushing equipment.
I worked as a computer vision researcher to find state-of-the-art solutions in classifying crushable and uncrushable materials for FM’s production pipeline. We built a full pipeline to make inference in real time. Before sending the material for further processing our model had to decide whether it is a crushable one or not. We tested various computer vision architectures such as VGG16, Resnet, Inception, EfficientNet, etc. In the end, Resnet gave the best performance for a recall of 0.85 and precision 0.7 and fast enough inference time (~3s). We aim to maximize recall in this problem because it is more cost-effective to filter out as many uncrushable materials than to keep as many crushable ones.
This project built “digital twins” — computer models replicating mining equipment behavior. Input from sensor readings was applied to specific field issues such as ore truck loading and conveyor system optimization.

Project Points:
Worked in Git development environment
Expert Level Python Development
Tensoflow, Keras Python deep neural network and analytical techniques
Expertise in transforming business requirements into analytical models, designing algorithms, building models, developing data mining and reporting solutions that scales across massive volume of structured and unstructured data
Professional competency in Statistical NLP / Machine Learning, especially Supervised Learning- Document classification, information extraction, and named entity recognition in-context
Worked with Proof of Concepts (POC's) and gap analysis and gathered necessary data for analysis from different sources, prepared data for data exploration using data wrangling
Designed Physical Data Architecture of New system engines
Hands on experience in implementing neural network skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, neural networks, Principle Component Analysis and good knowledge on Recommender Systems
Strong experience in Software Development Life Cycle (SDLC) including Requirements Analysis, Design Specification and Testing as per cycle in both Waterfall and Agile methodologies
Strong SQL Server and Python programming skills with experience in working with functions
Efficient in developing Logical and Physical Data model and organizing data as per the business requirements using Sybase Power Designer, ER Studio in both Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) applications
Experience in designing star schema, Snow flake schema for Data Warehouse, Operational Data Store (ODS) architecture.
Experience and technical proficiency in Designing, Data Modeling Online Applications, Solution Lead for Architecting Data Warehouse/Business Intelligence Applications
Worked with languages like Python and Scala and software packages such as Stata, SAS and SPSS to develop neural network and cluster analysis
Designed visualizations using Tableau software and publishing and presenting dashboards, Storyline on web and desktop platforms
Developed Logical Data Architecture with adherence to Enterprise Architecture
Used pandas in Python for performing Exploratory data analysis
Experience working with data modeling tools like Power Designer and ER Studio
Well experienced in Normalization & De-Normalization techniques for optimum performance in relational and dimensional database environments
Skilled in System Analysis, E-R/Dimensional Data Modeling, Database Design and implementing RDBMS specific features
Experienced in Data Integration Validation and Data Quality controls for ETL process and Data Warehousing using MS Visual Studio, SSIS, SSAS, SSRS
Responsible for Data Analytics, Data Reporting, Ad-hoc Reporting, Graphs, Scales, PivotTables and Online Analytical Processing (OLAP) reporting
Interacted with data from Hadoop for basic analysis and extraction of data in the infrastructure to provide data summarization
Created visualization tools and dashboards with Tableau, ggplot2 and d3.js
Worked with and extracted data from various database sources like Oracle, SQL Server, and DB2



February 2016 – July 2018
Computer Vision Specialist
ETSY New York,NY

Project Summary:
Here I helped build various features to increase user’s experiences such as building a similarity search model to reduce identical search results, classification model to categorize real prices against exaggerated prices of an item. In the end, my team proposed a novel approach to improve their recommendation system with computer vision. We utilized the novel siamese-network to create a recommendation model that matches items complementing each other in terms of functionality, styles, etc. We leveraged object detection with Fast RCNN using pytorch in order to generate our datasets from Etsy’s large database. In the end, our model outperformed the baseline model using just normal CNN architectures and was featured during the wrap up of the program..

Project Points:
Developed processes and tools to monitor and analyze performance and data accuracy.
Enhanced data collection procedures to include relevant information to build and continuously optimize the analytics systems.
Collaborated with I.T. to continuously optimize business performance.
Processed, cleansed and verified the integrity of data from various sources used for analysis and reporting.
Leveraged the latest data visualization tools and techniques to present and communicate analysis to the leadership team utilizing data management, analytics modeling, and business analysis.
Used predictive modeling to increase and optimize customer experiences, revenue generation, ad targeting and other business outcomes.
Advised leadership team and stakeholders with data-driven solutions and recommended strategies that address business challenges.



May 2013 – Dec 2015
Data Analyst (Remote)
Axiom Tech Group Chicago, Illinois

Project Summary:
Axiom provides enterprise advisory solutions and risk management. The firm relies on big data analytics to provide strategic insights and reporting to outside clients. I worked on analytics projects for various clients; cleaning data and performing analysis and reporting.

Project Points:
Business Data Analytics - Involved in ETL/BI requirement gathering and conversion into useful functional requirements.
Source to target data Mapping document preparation.
Developed report wireframes along with SQL schema data element definitions.
Worked with Data Warehouse architecture and wrote SQL queries.
Applied dimension modelling to identify dimension & fact tables and associated data elements.
Familiar with wealth and asset management concepts as well as trading life cycles.
Conducted in-depth data analysis on the reports/dashboards to identify gaps.
Involved in data governance to find authoritative sources for the critical data elements used in the governance reports.
Data profiling to validate data quality issues for the critical data elements.
Participated in user acceptance testing to ensure software satisfied all requirements before it was deployed to production.
Knowledge in BFSI domain and financial markets.
Well versed with Agile process, scrum and sprint concepts.
Knowledge on creation of user stories in JIRA
Worked as an ETL/Reporting Tester.
Familiar with Test plan & strategy document preparation and Test case preparation based on the requirements.
End to end testing in DWH projects.
Validation of ETL jobs against requirements by running through Control-M scheduler.
Validating target tables structure, constraints against ETL requirements.
Validating target data against source data based on ETL requirements.
Involved in test data preparation.
Report & Dashboard testing against target tables using SQL queries.
Worked with module testing including defect capturing in ALM.
Experienced with complete software development life cycle (SDLC) and software testing life cycle (STLC) life cycles
Worked extensively with on-time delivery, process improvement, regular interaction with client and mentoring the team.

Education
MS Statistics
Texas Tech University, Lubbock TX

BS Mathematics
Illinois Wesleyan University, Bloomington, IL

"
,
Data Science,"
Professional Profile

Technical Skills Profile

Analytic Development: Python, Javascript, Matlab, SAS, Spark, SQL VBA, C++, C
Python Packages: Numpy, Pandas, scikit-learn, TensorFlow, SciPy, Matplotlib, Seaborn, Numba, SpaCy, NLTK, LightGBM, XGBOOST, CatBoost, Dask, Gensim
IDE: Jupyter, Spyder, MatLab, Visual Studio
Version Control: GitHub, Git,
Machine Learning: Time Series Prediction, Natural Language Processing & Understanding, Machine Intelligence, Generalized Linear Models, Machine Learning algorithms
Data Query: Azure, Google Cloud, Amazon RedShift, Kinesis, EMR; RDBMS, Snowflake, SQL and data warehouse, data lake and various SQL and NoSQL databases.
Deep Learning: Machine perception, Machine Learning algorithms, Neural Networks, TensorFlow, Keras.
Artificial Intelligence: text understanding, NLP, Computer Vision, customer behavior predictive modeling, classification, pattern recognition, targeting systems, ranking systems.
Analysis Methods: Advanced Data Modeling, Time Series Analysis, Forecasting, Predictive, Statistical, Sentiment Analysis, Exploratory, Stochastic Calculus, Bayesian Analysis, Inference, Models, Regression Analysis, Linear models, Multivariate analysis, Sampling methods, Forecasting, Segmentation, Clustering, Predictive Analytics, Big Data and Queries Interpretation, Design and Analysis of Experiments, Association Analysis
Analysis Techniques: Classification and Regression Trees (CART), Support Vector Machine, Random Forest, Gradient Boosting Machine (GBM), TensorFlow, Principle Component Analysis, Recurrent Neural Networks, Regression, Naïve Bayes
Data Modeling: Bayesian Analysis, Statistical Inference, Predictive Modeling, Stochastic Modeling, Linear Modeling, Behavioral Modeling, Probabilistic Modeling, Time-Series Analysis, Survival Analysis
Applied Data Science: Natural Language Processing, Machine Learning, Social Analytics, Predictive modeling
Soft Skills: Excellent communication and presentation skills; ability to work well with stakeholders to discern needs accurately, leadership, mentoring, coaching

Professional Experience

Roza Contractors Chicago IL
Principal Data Scientist
October, 2019 - Present

At Roza Contractors purchase order processing was automated using Optical Character Recognition (OCR) on scanned documents. Prior to my work, they were creating handwritten purchase orders and filing them in a filing cabinet. This took additional time and manpower when a record needed to be referenced. This system was automated so that scanned documents could be submitted to an API and the results of the OCR were stored in a SQL database for easy referencing.

Lead the development team and implemented the complete solution.
Wrote the code in Python and SQL
Implemented various NLP techniques to identify text fragments.
Used Tensorflow and Keras to implement word embeddings.
Used Google Tesseract and AWS Text Extract.
Used NLP technics to sort and classify documents.
Used AWS Redshift Data Warehouse and Boto 3 to access AWS Resources from Python.
Work with product development team for optimal product design, pricing and marketing strategy.
Developing hazard models for credit loss projection by applying Cox Proportional hazard model and logistic probability model with customer cohorts, and explore a Gradient Boosted Machine and Deep Neural Network
Oversee/Manage daily asset liability modeling production on AWS with Oracle database and other data formats



ROUTE Chicago IL
Machine Learning Engineer
April 2017 – September 2019

Route is a SaaS product for commercial cleaning companies. To supplement their application, an Object Detection model was created that could identify the type and number of “fixtures” at a location based on a camera phone photograph. In the cleaning space a “fixture” is any item in a space that needs to be cleaned. These items could be chairs, couches, tables, computer systems, desks, etc.. The model was deployed on edge devices using Tensorflow-Lite.

Developed a custom dataset for fine-tuning a deep neural network.
Fine-tuned a variety of image models with object detection heads.
Used both Single Shot Detection (SSD) and You Only Look Once (YOLO) object detection models.
Deployed finished model on edge devices using Tensorflow-Lite.
Build various statistical models Statistical algorithms involving Time Series analysis, Survival Analysis, Multivariate Regression, Linear Regression, Logistic Regression and PCA in financial projection
Lead the development of the expected profit projection engine by applying machine learning with financial engineering, actuarial science.
Build various statistical models Statistical algorithms involving Time Series analysis, Survival Analysis, Multivariate Regression, Linear Regression, Logistic Regression and PCA in financial projection
Perform inforce management including survival analysis, churn/retention analysis, risk identification
Used pre-trained models to visualize the feature maps in the intermediate layers and performed transfer learning
Used pre-trained models (VGG16, ResNets, Inceptions, DenseNet, U-Net, etc.) for transfer learning on small datasets
Design and implement the enterprise Financial Value-at-Risk model
Lead various cross-department projects and worked closely with internal stakeholders such as business teams, product managers, engineering teams
Worked on customer segmentation using an unsupervised learning technique clustering.

Rozalado Services South Bend, IN
Lead Data Scientist March 2015 – April 2017

Rozalado Services is a commercial cleaning company that takes a data driven approach to commercial cleaning. During my time there, I developed an NLP model that would read incoming emails and determine if a call needed to be scheduled. If so, the script would send an automated reply with a calendar that contained the open call slots of the salesperson.

Performed fund analysis for Investment Management, improving methodologies for modeling and hedging fund driven exposure, providing monthly fund dashboard, quantifying the risk characteristics of funds
Lead the development of customer key risk indicator using natural language processing (NLP) technical and LSTM to process text records.
Implemented ML and NLP solutions in Python

Developed portfolio replicating process for asset-liability portfolio using plain vanilla instruments for Asset-Liability/Economical Capital management
Used NLP techniques such tokenization, stop words, normalization, regularization, bag-of-words, and tf-idf for classification of emails
Performed EDA and build statistical visualizations for language used in emails
Developed RESTful API to pull emails from and placed into model pipeline

Hollywood Video South Bend, IN
Lead Data Scientist October 2013 – January 2015

Hollywood Video pivoted from a video rental chain to a website that partnered with Amazon to provide movie reviews and purchase suggestions. At that time I was responsible for business analytics, using statistical techniques to categorize new movies using Logistic Regression, to provide business insights with Tableau, and to help on their ‘social scoring’ algorithm, an early recommender system.
Performed Collaborative Filtering for ‘social scoring’ algorithm to match users with movies they may like.
Performed Content Based Filtering to match users with movies through ‘social scoring’ algorithm.
Maintained a database of users preferences and movie reviews to match users with movies.

Performed fund analysis for Investment Management, improving methodologies for modeling and hedging fund driven exposure, providing monthly fund dashboard, quantifying the risk characteristics of funds
Lead the development of customer key risk indicator using natural language processing (NLP) technical and LSTM to process text records.

Developed portfolio replicating process for asset-liability portfolio using plain vanilla instruments for Asset-Liability/Economical Capital management

Jacobs-Cathey Co. Waco, TX
Staff Data Science Associate
March 2011 – August 2013

Jacobs-Cathey Company is a heating and air (HVAC) company. I was in charge of load calculation for their residential clients and during my time there I digitized their customer data and created an algorithm that forecasted summer sales based on this data. In TX, HVAC is a seasonal business where most of their customers' problems would happen in the summer when temperatures often rise above the mechanical limits of air conditioning units. Based on their last maintenance time, the maintenance problem code, and other factors the model predicted a general sales forecast. This forecast was used to buy copper in the winter when the prices were low that would last through the summer when copper prices would rise exponentially.

Developed custom regression model for prediction of heat load in residential systems
Developed time-series analysis to forecast summer sales.
Used time-series analysis to predict amount of copper piping needed for summer sales.
Worked with key mathematicians and statisticians to build financial models and perform statistical analysis.
Developed models and documented algorithms for production Scenario Generator

EDUCATION

Baylor University Waco, TX
B.A. in Philosophy, December 2008
Specialization: Computational Finance, Numerical Analysis and Mathematical Programming

Northwestern University Chicago, IL
M.A in Philosophy, December 2011
Specialization: Formal Syntantics, Cognitive Sx and Semantics, Cognitive Sntics, Cognitive Science, Mathematical Modeling

Northwestern University Chicago, IL
PhD in Mathematical Logic, June 2018
Specialization: Church’s Typed Lambda Calculus, Computational Linguistics, Semantics

CERTIFICATES

Cognitive Science, Northwestern University
Full Stack Engineering, Northwestern University
Python Programming for Data Science, Promotable at 1871
Data Analytics for Business, Promotable at 1871

312-***-****
adlzpj@r.postjobfree.com

Experience
10 years of experience
Data Science and Machine Learning Engineer

Competencies
Quantitative Modeling

Machine Learning

Predictive Modeling and Analytics

Market Risk Modeling

Natural Language Processing

Fraud Prevention
10 years of quantitative modeling, data science, Machine Learning and Python programmer
Extensive experience of cross-department project management
Experience in the application of Naïve Bayes, Regression Analysis, Neural Networks/Deep Neural Networks, Support Vector Machines (SVM), and Random Forest machine learning techniques.
Experience in machine learning models, statistical models on big data sets using cloud/cluster computing assets with AWS and Azure.
Extensive experience on time series analysis and survival analysis
Work with product development department for optimal product design, pricing and marketing strategy
Hands on experience in credit risk modeling from hazard models, severity models and exposure at risk models
Extensive quantitative Natural Language Processing (NLP)
Convolutional Neural Networks, Computer Vision
Customer behavior predictive modeling on lapse/churn, withdraw
Fraud detection and prevention with financial transactions
Extensive model validation experience in data science, and quantitative modeling
Excellent communication skills (verbal and written), able to communicate with clients/stakeholders and team members.

"
